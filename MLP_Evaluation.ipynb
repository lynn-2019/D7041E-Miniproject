{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a172eaaf-68dd-443a-b268-d054fe5f26f6",
   "metadata": {},
   "source": [
    "## MLP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c17fb2c-ee77-4d5b-90d9-3ee2ed4d2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Load ground truth\n",
    "Ytrain = np.load('TrainGT_preprocessed.npy')\n",
    "Ytest = np.load('TestGT_preprocessed.npy')\n",
    "\n",
    "# Load data\n",
    "Xtrain = np.load('Trainpics_preprocessed.npy')\n",
    "Xtest = np.load('Testpics_preprocessed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f1f1a3-f1de-4064-bbbf-6a33eec90877",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = np.where((Ytrain == 1) | (Ytrain == 2))[0]\n",
    "\n",
    "# Reduziere Xtrain und Ytrain auf die ausgewählten Indizes\n",
    "Xtrain = Xtrain[selected_indices, :]\n",
    "Ytrain = Ytrain[selected_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b78350-8db8-4170-bec3-105232cc3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = np.where((Ytest == 1) | (Ytest == 2))[0]\n",
    "\n",
    "# Reduziere Xtest und Ytest auf die ausgewählten Indizes\n",
    "Xtest = Xtest[selected_indices, :]\n",
    "Ytest = Ytest[selected_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14209dc4-a850-4c08-94c1-d66dd7717efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71191562\n",
      "Iteration 2, loss = 0.77608976\n",
      "Iteration 3, loss = 0.82061891\n",
      "Iteration 4, loss = 0.89818227\n",
      "Iteration 5, loss = 0.93593090\n",
      "Iteration 6, loss = 0.95111442\n",
      "Iteration 7, loss = 0.94497012\n",
      "Iteration 8, loss = 0.94469087\n",
      "Iteration 9, loss = 0.93836179\n",
      "Iteration 10, loss = 0.93742821\n",
      "Iteration 11, loss = 0.93048230\n",
      "Iteration 12, loss = 0.92878732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71275864\n",
      "Iteration 2, loss = 0.77155026\n",
      "Iteration 3, loss = 0.81698714\n",
      "Iteration 4, loss = 0.89198074\n",
      "Iteration 5, loss = 0.94030597\n",
      "Iteration 6, loss = 0.94962671\n",
      "Iteration 7, loss = 0.95299176\n",
      "Iteration 8, loss = 0.94436397\n",
      "Iteration 9, loss = 0.94584752\n",
      "Iteration 10, loss = 0.93745129\n",
      "Iteration 11, loss = 0.93796597\n",
      "Iteration 12, loss = 0.92914456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71376250\n",
      "Iteration 2, loss = 0.77532158\n",
      "Iteration 3, loss = 0.82022773\n",
      "Iteration 4, loss = 0.89636821\n",
      "Iteration 5, loss = 0.93918265\n",
      "Iteration 6, loss = 0.95158820\n",
      "Iteration 7, loss = 0.95000068\n",
      "Iteration 8, loss = 0.94579062\n",
      "Iteration 9, loss = 0.94317505\n",
      "Iteration 10, loss = 0.93870911\n",
      "Iteration 11, loss = 0.93532187\n",
      "Iteration 12, loss = 0.93026038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71191562\n",
      "Iteration 2, loss = 9.75387276\n",
      "Iteration 3, loss = 3.84686969\n",
      "Iteration 4, loss = 3.86983236\n",
      "Iteration 5, loss = 5.69581798\n",
      "Iteration 6, loss = 4.30777175\n",
      "Iteration 7, loss = 1.06743724\n",
      "Iteration 8, loss = 3.38910330\n",
      "Iteration 9, loss = 5.00166119\n",
      "Iteration 10, loss = 4.60514900\n",
      "Iteration 11, loss = 2.67230373\n",
      "Iteration 12, loss = 0.79079102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71275864\n",
      "Iteration 2, loss = 9.87815440\n",
      "Iteration 3, loss = 3.80202732\n",
      "Iteration 4, loss = 3.97173881\n",
      "Iteration 5, loss = 5.90281056\n",
      "Iteration 6, loss = 4.66401104\n",
      "Iteration 7, loss = 1.49338868\n",
      "Iteration 8, loss = 3.03670603\n",
      "Iteration 9, loss = 4.80019425\n",
      "Iteration 10, loss = 4.51733142\n",
      "Iteration 11, loss = 2.66676592\n",
      "Iteration 12, loss = 0.76737254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71376250\n",
      "Iteration 2, loss = 9.81459650\n",
      "Iteration 3, loss = 3.82690990\n",
      "Iteration 4, loss = 3.92034566\n",
      "Iteration 5, loss = 5.79723872\n",
      "Iteration 6, loss = 4.48127355\n",
      "Iteration 7, loss = 1.26356481\n",
      "Iteration 8, loss = 3.25226121\n",
      "Iteration 9, loss = 4.96471398\n",
      "Iteration 10, loss = 4.64826355\n",
      "Iteration 11, loss = 2.77752525\n",
      "Iteration 12, loss = 0.73574862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76254741\n",
      "Iteration 2, loss = 1.10924768\n",
      "Iteration 3, loss = 1.30514989\n",
      "Iteration 4, loss = 1.31688076\n",
      "Iteration 5, loss = 1.23476464\n",
      "Iteration 6, loss = 1.28989670\n",
      "Iteration 7, loss = 1.25933472\n",
      "Iteration 8, loss = 1.25902652\n",
      "Iteration 9, loss = 1.24758935\n",
      "Iteration 10, loss = 1.24826280\n",
      "Iteration 11, loss = 1.22704775\n",
      "Iteration 12, loss = 1.23172667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75807209\n",
      "Iteration 2, loss = 1.08537625\n",
      "Iteration 3, loss = 1.32114071\n",
      "Iteration 4, loss = 1.29189509\n",
      "Iteration 5, loss = 1.26235213\n",
      "Iteration 6, loss = 1.27250528\n",
      "Iteration 7, loss = 1.27796834\n",
      "Iteration 8, loss = 1.24300741\n",
      "Iteration 9, loss = 1.26557602\n",
      "Iteration 10, loss = 1.23242674\n",
      "Iteration 11, loss = 1.24518660\n",
      "Iteration 12, loss = 1.21721879\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75609056\n",
      "Iteration 2, loss = 1.09479480\n",
      "Iteration 3, loss = 1.30884825\n",
      "Iteration 4, loss = 1.29997459\n",
      "Iteration 5, loss = 1.24397603\n",
      "Iteration 6, loss = 1.27745097\n",
      "Iteration 7, loss = 1.26386566\n",
      "Iteration 8, loss = 1.24714581\n",
      "Iteration 9, loss = 1.25215123\n",
      "Iteration 10, loss = 1.23651205\n",
      "Iteration 11, loss = 1.23181244\n",
      "Iteration 12, loss = 1.22082018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76254741\n",
      "Iteration 2, loss = 10.08680480\n",
      "Iteration 3, loss = 5.21089189\n",
      "Iteration 4, loss = 2.11501977\n",
      "Iteration 5, loss = 3.70318745\n",
      "Iteration 6, loss = 2.04746181\n",
      "Iteration 7, loss = 1.64072012\n",
      "Iteration 8, loss = 2.54299278\n",
      "Iteration 9, loss = 1.43555554\n",
      "Iteration 10, loss = 1.28425232\n",
      "Iteration 11, loss = 1.97183029\n",
      "Iteration 12, loss = 1.13028730\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75807209\n",
      "Iteration 2, loss = 10.21742244\n",
      "Iteration 3, loss = 5.12929065\n",
      "Iteration 4, loss = 2.31966936\n",
      "Iteration 5, loss = 3.99620294\n",
      "Iteration 6, loss = 2.47107386\n",
      "Iteration 7, loss = 1.18557530\n",
      "Iteration 8, loss = 2.25858517\n",
      "Iteration 9, loss = 1.30684533\n",
      "Iteration 10, loss = 1.28010134\n",
      "Iteration 11, loss = 1.82623065\n",
      "Iteration 12, loss = 0.95490939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75609056\n",
      "Iteration 2, loss = 10.14374541\n",
      "Iteration 3, loss = 5.17328140\n",
      "Iteration 4, loss = 2.19946199\n",
      "Iteration 5, loss = 3.82764988\n",
      "Iteration 6, loss = 2.23244396\n",
      "Iteration 7, loss = 1.42664092\n",
      "Iteration 8, loss = 2.37884278\n",
      "Iteration 9, loss = 1.32332419\n",
      "Iteration 10, loss = 1.32391186\n",
      "Iteration 11, loss = 1.92434144\n",
      "Iteration 12, loss = 1.04185425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73088966\n",
      "Iteration 2, loss = 0.69464007\n",
      "Iteration 3, loss = 0.70187155\n",
      "Iteration 4, loss = 0.69753543\n",
      "Iteration 5, loss = 0.69404063\n",
      "Iteration 6, loss = 0.69385807\n",
      "Iteration 7, loss = 0.69376568\n",
      "Iteration 8, loss = 0.69346418\n",
      "Iteration 9, loss = 0.69325651\n",
      "Iteration 10, loss = 0.69308140\n",
      "Iteration 11, loss = 0.69288873\n",
      "Iteration 12, loss = 0.69268727\n",
      "Iteration 13, loss = 0.69248071\n",
      "Iteration 14, loss = 0.69226744\n",
      "Iteration 15, loss = 0.69204779\n",
      "Iteration 16, loss = 0.69182265\n",
      "Iteration 17, loss = 0.69159256\n",
      "Iteration 18, loss = 0.69135800\n",
      "Iteration 19, loss = 0.69111943\n",
      "Iteration 20, loss = 0.69087728\n",
      "Iteration 21, loss = 0.69063192\n",
      "Iteration 22, loss = 0.69038368\n",
      "Iteration 23, loss = 0.69013289\n",
      "Iteration 24, loss = 0.68987980\n",
      "Iteration 25, loss = 0.68962466\n",
      "Iteration 26, loss = 0.68936770\n",
      "Iteration 27, loss = 0.68910911\n",
      "Iteration 28, loss = 0.68884907\n",
      "Iteration 29, loss = 0.68858775\n",
      "Iteration 30, loss = 0.68832528\n",
      "Iteration 31, loss = 0.68806180\n",
      "Iteration 32, loss = 0.68779741\n",
      "Iteration 33, loss = 0.68753223\n",
      "Iteration 34, loss = 0.68726635\n",
      "Iteration 35, loss = 0.68699985\n",
      "Iteration 36, loss = 0.68673281\n",
      "Iteration 37, loss = 0.68646528\n",
      "Iteration 38, loss = 0.68619734\n",
      "Iteration 39, loss = 0.68592904\n",
      "Iteration 40, loss = 0.68566042\n",
      "Iteration 41, loss = 0.68539152\n",
      "Iteration 42, loss = 0.68512239\n",
      "Iteration 43, loss = 0.68485306\n",
      "Iteration 44, loss = 0.68458356\n",
      "Iteration 45, loss = 0.68431392\n",
      "Iteration 46, loss = 0.68404415\n",
      "Iteration 47, loss = 0.68377429\n",
      "Iteration 48, loss = 0.68350434\n",
      "Iteration 49, loss = 0.68323433\n",
      "Iteration 50, loss = 0.68296427\n",
      "Iteration 51, loss = 0.68269418\n",
      "Iteration 52, loss = 0.68242405\n",
      "Iteration 53, loss = 0.68215392\n",
      "Iteration 54, loss = 0.68188377\n",
      "Iteration 55, loss = 0.68161362\n",
      "Iteration 56, loss = 0.68134348\n",
      "Iteration 57, loss = 0.68107335\n",
      "Iteration 58, loss = 0.68080324\n",
      "Iteration 59, loss = 0.68053314\n",
      "Iteration 60, loss = 0.68026307\n",
      "Iteration 61, loss = 0.67999302\n",
      "Iteration 62, loss = 0.67972300\n",
      "Iteration 63, loss = 0.67945300\n",
      "Iteration 64, loss = 0.67918304\n",
      "Iteration 65, loss = 0.67891310\n",
      "Iteration 66, loss = 0.67864319\n",
      "Iteration 67, loss = 0.67837330\n",
      "Iteration 68, loss = 0.67810345\n",
      "Iteration 69, loss = 0.67783362\n",
      "Iteration 70, loss = 0.67756382\n",
      "Iteration 71, loss = 0.67729405\n",
      "Iteration 72, loss = 0.67702429\n",
      "Iteration 73, loss = 0.67675456\n",
      "Iteration 74, loss = 0.67648486\n",
      "Iteration 75, loss = 0.67621516\n",
      "Iteration 76, loss = 0.67594549\n",
      "Iteration 77, loss = 0.67567583\n",
      "Iteration 78, loss = 0.67540618\n",
      "Iteration 79, loss = 0.67513654\n",
      "Iteration 80, loss = 0.67486691\n",
      "Iteration 81, loss = 0.67459729\n",
      "Iteration 82, loss = 0.67432767\n",
      "Iteration 83, loss = 0.67405805\n",
      "Iteration 84, loss = 0.67378842\n",
      "Iteration 85, loss = 0.67351880\n",
      "Iteration 86, loss = 0.67324916\n",
      "Iteration 87, loss = 0.67297952\n",
      "Iteration 88, loss = 0.67270986\n",
      "Iteration 89, loss = 0.67244019\n",
      "Iteration 90, loss = 0.67217050\n",
      "Iteration 91, loss = 0.67190079\n",
      "Iteration 92, loss = 0.67163106\n",
      "Iteration 93, loss = 0.67136130\n",
      "Iteration 94, loss = 0.67109151\n",
      "Iteration 95, loss = 0.67082170\n",
      "Iteration 96, loss = 0.67055184\n",
      "Iteration 97, loss = 0.67028195\n",
      "Iteration 98, loss = 0.67001203\n",
      "Iteration 99, loss = 0.66974206\n",
      "Iteration 100, loss = 0.66947204\n",
      "Iteration 101, loss = 0.66920198\n",
      "Iteration 102, loss = 0.66893186\n",
      "Iteration 103, loss = 0.66866169\n",
      "Iteration 104, loss = 0.66839147\n",
      "Iteration 105, loss = 0.66812119\n",
      "Iteration 106, loss = 0.66785084\n",
      "Iteration 107, loss = 0.66758044\n",
      "Iteration 108, loss = 0.66730996\n",
      "Iteration 109, loss = 0.66703942\n",
      "Iteration 110, loss = 0.66676880\n",
      "Iteration 111, loss = 0.66649811\n",
      "Iteration 112, loss = 0.66622734\n",
      "Iteration 113, loss = 0.66595649\n",
      "Iteration 114, loss = 0.66568556\n",
      "Iteration 115, loss = 0.66541454\n",
      "Iteration 116, loss = 0.66514344\n",
      "Iteration 117, loss = 0.66487224\n",
      "Iteration 118, loss = 0.66460095\n",
      "Iteration 119, loss = 0.66432956\n",
      "Iteration 120, loss = 0.66405807\n",
      "Iteration 121, loss = 0.66378649\n",
      "Iteration 122, loss = 0.66351479\n",
      "Iteration 123, loss = 0.66324300\n",
      "Iteration 124, loss = 0.66297109\n",
      "Iteration 125, loss = 0.66269907\n",
      "Iteration 126, loss = 0.66242693\n",
      "Iteration 127, loss = 0.66215468\n",
      "Iteration 128, loss = 0.66188231\n",
      "Iteration 129, loss = 0.66160982\n",
      "Iteration 130, loss = 0.66133720\n",
      "Iteration 131, loss = 0.66106446\n",
      "Iteration 132, loss = 0.66079159\n",
      "Iteration 133, loss = 0.66051858\n",
      "Iteration 134, loss = 0.66024544\n",
      "Iteration 135, loss = 0.65997216\n",
      "Iteration 136, loss = 0.65969875\n",
      "Iteration 137, loss = 0.65942519\n",
      "Iteration 138, loss = 0.65915148\n",
      "Iteration 139, loss = 0.65887763\n",
      "Iteration 140, loss = 0.65860363\n",
      "Iteration 141, loss = 0.65832948\n",
      "Iteration 142, loss = 0.65805518\n",
      "Iteration 143, loss = 0.65778071\n",
      "Iteration 144, loss = 0.65750609\n",
      "Iteration 145, loss = 0.65723131\n",
      "Iteration 146, loss = 0.65695636\n",
      "Iteration 147, loss = 0.65668125\n",
      "Iteration 148, loss = 0.65640597\n",
      "Iteration 149, loss = 0.65613052\n",
      "Iteration 150, loss = 0.65585489\n",
      "Iteration 151, loss = 0.65557909\n",
      "Iteration 152, loss = 0.65530312\n",
      "Iteration 153, loss = 0.65502696\n",
      "Iteration 154, loss = 0.65475062\n",
      "Iteration 155, loss = 0.65447409\n",
      "Iteration 156, loss = 0.65419738\n",
      "Iteration 157, loss = 0.65392047\n",
      "Iteration 158, loss = 0.65364338\n",
      "Iteration 159, loss = 0.65336609\n",
      "Iteration 160, loss = 0.65308861\n",
      "Iteration 161, loss = 0.65281093\n",
      "Iteration 162, loss = 0.65253304\n",
      "Iteration 163, loss = 0.65225496\n",
      "Iteration 164, loss = 0.65197667\n",
      "Iteration 165, loss = 0.65169817\n",
      "Iteration 166, loss = 0.65141946\n",
      "Iteration 167, loss = 0.65114055\n",
      "Iteration 168, loss = 0.65086141\n",
      "Iteration 169, loss = 0.65058206\n",
      "Iteration 170, loss = 0.65030250\n",
      "Iteration 171, loss = 0.65002271\n",
      "Iteration 172, loss = 0.64974270\n",
      "Iteration 173, loss = 0.64946247\n",
      "Iteration 174, loss = 0.64918201\n",
      "Iteration 175, loss = 0.64890132\n",
      "Iteration 176, loss = 0.64862040\n",
      "Iteration 177, loss = 0.64833925\n",
      "Iteration 178, loss = 0.64805786\n",
      "Iteration 179, loss = 0.64777623\n",
      "Iteration 180, loss = 0.64749437\n",
      "Iteration 181, loss = 0.64721226\n",
      "Iteration 182, loss = 0.64692991\n",
      "Iteration 183, loss = 0.64664732\n",
      "Iteration 184, loss = 0.64636448\n",
      "Iteration 185, loss = 0.64608139\n",
      "Iteration 186, loss = 0.64579805\n",
      "Iteration 187, loss = 0.64551445\n",
      "Iteration 188, loss = 0.64523060\n",
      "Iteration 189, loss = 0.64494649\n",
      "Iteration 190, loss = 0.64466212\n",
      "Iteration 191, loss = 0.64437749\n",
      "Iteration 192, loss = 0.64409260\n",
      "Iteration 193, loss = 0.64380744\n",
      "Iteration 194, loss = 0.64352202\n",
      "Iteration 195, loss = 0.64323633\n",
      "Iteration 196, loss = 0.64295036\n",
      "Iteration 197, loss = 0.64266412\n",
      "Iteration 198, loss = 0.64237761\n",
      "Iteration 199, loss = 0.64209082\n",
      "Iteration 200, loss = 0.64180375\n",
      "Iteration 201, loss = 0.64151641\n",
      "Iteration 202, loss = 0.64122877\n",
      "Iteration 203, loss = 0.64094086\n",
      "Iteration 204, loss = 0.64065266\n",
      "Iteration 205, loss = 0.64036417\n",
      "Iteration 206, loss = 0.64007539\n",
      "Iteration 207, loss = 0.63978632\n",
      "Iteration 208, loss = 0.63949696\n",
      "Iteration 209, loss = 0.63920730\n",
      "Iteration 210, loss = 0.63891734\n",
      "Iteration 211, loss = 0.63862709\n",
      "Iteration 212, loss = 0.63833653\n",
      "Iteration 213, loss = 0.63804567\n",
      "Iteration 214, loss = 0.63775451\n",
      "Iteration 215, loss = 0.63746304\n",
      "Iteration 216, loss = 0.63717127\n",
      "Iteration 217, loss = 0.63687918\n",
      "Iteration 218, loss = 0.63658679\n",
      "Iteration 219, loss = 0.63629408\n",
      "Iteration 220, loss = 0.63600105\n",
      "Iteration 221, loss = 0.63570772\n",
      "Iteration 222, loss = 0.63541406\n",
      "Iteration 223, loss = 0.63512008\n",
      "Iteration 224, loss = 0.63482578\n",
      "Iteration 225, loss = 0.63453116\n",
      "Iteration 226, loss = 0.63423622\n",
      "Iteration 227, loss = 0.63394095\n",
      "Iteration 228, loss = 0.63364535\n",
      "Iteration 229, loss = 0.63334942\n",
      "Iteration 230, loss = 0.63305316\n",
      "Iteration 231, loss = 0.63275657\n",
      "Iteration 232, loss = 0.63245964\n",
      "Iteration 233, loss = 0.63216238\n",
      "Iteration 234, loss = 0.63186478\n",
      "Iteration 235, loss = 0.63156684\n",
      "Iteration 236, loss = 0.63126856\n",
      "Iteration 237, loss = 0.63096994\n",
      "Iteration 238, loss = 0.63067098\n",
      "Iteration 239, loss = 0.63037167\n",
      "Iteration 240, loss = 0.63007201\n",
      "Iteration 241, loss = 0.62977201\n",
      "Iteration 242, loss = 0.62947165\n",
      "Iteration 243, loss = 0.62917095\n",
      "Iteration 244, loss = 0.62886989\n",
      "Iteration 245, loss = 0.62856848\n",
      "Iteration 246, loss = 0.62826671\n",
      "Iteration 247, loss = 0.62796459\n",
      "Iteration 248, loss = 0.62766210\n",
      "Iteration 249, loss = 0.62735926\n",
      "Iteration 250, loss = 0.62705605\n",
      "Iteration 251, loss = 0.62675249\n",
      "Iteration 252, loss = 0.62644856\n",
      "Iteration 253, loss = 0.62614426\n",
      "Iteration 254, loss = 0.62583959\n",
      "Iteration 255, loss = 0.62553456\n",
      "Iteration 256, loss = 0.62522916\n",
      "Iteration 257, loss = 0.62492339\n",
      "Iteration 258, loss = 0.62461724\n",
      "Iteration 259, loss = 0.62431072\n",
      "Iteration 260, loss = 0.62400383\n",
      "Iteration 261, loss = 0.62369656\n",
      "Iteration 262, loss = 0.62338891\n",
      "Iteration 263, loss = 0.62308088\n",
      "Iteration 264, loss = 0.62277247\n",
      "Iteration 265, loss = 0.62246368\n",
      "Iteration 266, loss = 0.62215451\n",
      "Iteration 267, loss = 0.62184495\n",
      "Iteration 268, loss = 0.62153501\n",
      "Iteration 269, loss = 0.62122468\n",
      "Iteration 270, loss = 0.62091396\n",
      "Iteration 271, loss = 0.62060286\n",
      "Iteration 272, loss = 0.62029136\n",
      "Iteration 273, loss = 0.61997947\n",
      "Iteration 274, loss = 0.61966719\n",
      "Iteration 275, loss = 0.61935452\n",
      "Iteration 276, loss = 0.61904145\n",
      "Iteration 277, loss = 0.61872798\n",
      "Iteration 278, loss = 0.61841412\n",
      "Iteration 279, loss = 0.61809985\n",
      "Iteration 280, loss = 0.61778519\n",
      "Iteration 281, loss = 0.61747013\n",
      "Iteration 282, loss = 0.61715466\n",
      "Iteration 283, loss = 0.61683879\n",
      "Iteration 284, loss = 0.61652252\n",
      "Iteration 285, loss = 0.61620584\n",
      "Iteration 286, loss = 0.61588876\n",
      "Iteration 287, loss = 0.61557127\n",
      "Iteration 288, loss = 0.61525337\n",
      "Iteration 289, loss = 0.61493506\n",
      "Iteration 290, loss = 0.61461634\n",
      "Iteration 291, loss = 0.61429720\n",
      "Iteration 292, loss = 0.61397766\n",
      "Iteration 293, loss = 0.61365770\n",
      "Iteration 294, loss = 0.61333733\n",
      "Iteration 295, loss = 0.61301654\n",
      "Iteration 296, loss = 0.61269533\n",
      "Iteration 297, loss = 0.61237371\n",
      "Iteration 298, loss = 0.61205167\n",
      "Iteration 299, loss = 0.61172920\n",
      "Iteration 300, loss = 0.61140632\n",
      "Iteration 1, loss = 0.72511345\n",
      "Iteration 2, loss = 0.69227392\n",
      "Iteration 3, loss = 0.69884198\n",
      "Iteration 4, loss = 0.69490564\n",
      "Iteration 5, loss = 0.69175557\n",
      "Iteration 6, loss = 0.69159883\n",
      "Iteration 7, loss = 0.69152280\n",
      "Iteration 8, loss = 0.69125944\n",
      "Iteration 9, loss = 0.69108113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.69093262\n",
      "Iteration 11, loss = 0.69076874\n",
      "Iteration 12, loss = 0.69059736\n",
      "Iteration 13, loss = 0.69042175\n",
      "Iteration 14, loss = 0.69024041\n",
      "Iteration 15, loss = 0.69005362\n",
      "Iteration 16, loss = 0.68986214\n",
      "Iteration 17, loss = 0.68966642\n",
      "Iteration 18, loss = 0.68946687\n",
      "Iteration 19, loss = 0.68926388\n",
      "Iteration 20, loss = 0.68905782\n",
      "Iteration 21, loss = 0.68884900\n",
      "Iteration 22, loss = 0.68863772\n",
      "Iteration 23, loss = 0.68842422\n",
      "Iteration 24, loss = 0.68820874\n",
      "Iteration 25, loss = 0.68799149\n",
      "Iteration 26, loss = 0.68777267\n",
      "Iteration 27, loss = 0.68755243\n",
      "Iteration 28, loss = 0.68733093\n",
      "Iteration 29, loss = 0.68710831\n",
      "Iteration 30, loss = 0.68688468\n",
      "Iteration 31, loss = 0.68666017\n",
      "Iteration 32, loss = 0.68643486\n",
      "Iteration 33, loss = 0.68620885\n",
      "Iteration 34, loss = 0.68598221\n",
      "Iteration 35, loss = 0.68575503\n",
      "Iteration 36, loss = 0.68552735\n",
      "Iteration 37, loss = 0.68529924\n",
      "Iteration 38, loss = 0.68507074\n",
      "Iteration 39, loss = 0.68484192\n",
      "Iteration 40, loss = 0.68461280\n",
      "Iteration 41, loss = 0.68438342\n",
      "Iteration 42, loss = 0.68415382\n",
      "Iteration 43, loss = 0.68392402\n",
      "Iteration 44, loss = 0.68369405\n",
      "Iteration 45, loss = 0.68346394\n",
      "Iteration 46, loss = 0.68323371\n",
      "Iteration 47, loss = 0.68300336\n",
      "Iteration 48, loss = 0.68277293\n",
      "Iteration 49, loss = 0.68254242\n",
      "Iteration 50, loss = 0.68231185\n",
      "Iteration 51, loss = 0.68208123\n",
      "Iteration 52, loss = 0.68185056\n",
      "Iteration 53, loss = 0.68161986\n",
      "Iteration 54, loss = 0.68138914\n",
      "Iteration 55, loss = 0.68115839\n",
      "Iteration 56, loss = 0.68092763\n",
      "Iteration 57, loss = 0.68069687\n",
      "Iteration 58, loss = 0.68046610\n",
      "Iteration 59, loss = 0.68023533\n",
      "Iteration 60, loss = 0.68000456\n",
      "Iteration 61, loss = 0.67977379\n",
      "Iteration 62, loss = 0.67954303\n",
      "Iteration 63, loss = 0.67931228\n",
      "Iteration 64, loss = 0.67908154\n",
      "Iteration 65, loss = 0.67885081\n",
      "Iteration 66, loss = 0.67862009\n",
      "Iteration 67, loss = 0.67838938\n",
      "Iteration 68, loss = 0.67815867\n",
      "Iteration 69, loss = 0.67792798\n",
      "Iteration 70, loss = 0.67769730\n",
      "Iteration 71, loss = 0.67746663\n",
      "Iteration 72, loss = 0.67723597\n",
      "Iteration 73, loss = 0.67700531\n",
      "Iteration 74, loss = 0.67677466\n",
      "Iteration 75, loss = 0.67654402\n",
      "Iteration 76, loss = 0.67631338\n",
      "Iteration 77, loss = 0.67608274\n",
      "Iteration 78, loss = 0.67585211\n",
      "Iteration 79, loss = 0.67562147\n",
      "Iteration 80, loss = 0.67539084\n",
      "Iteration 81, loss = 0.67516019\n",
      "Iteration 82, loss = 0.67492955\n",
      "Iteration 83, loss = 0.67469890\n",
      "Iteration 84, loss = 0.67446823\n",
      "Iteration 85, loss = 0.67423756\n",
      "Iteration 86, loss = 0.67400688\n",
      "Iteration 87, loss = 0.67377618\n",
      "Iteration 88, loss = 0.67354547\n",
      "Iteration 89, loss = 0.67331474\n",
      "Iteration 90, loss = 0.67308399\n",
      "Iteration 91, loss = 0.67285321\n",
      "Iteration 92, loss = 0.67262242\n",
      "Iteration 93, loss = 0.67239160\n",
      "Iteration 94, loss = 0.67216075\n",
      "Iteration 95, loss = 0.67192987\n",
      "Iteration 96, loss = 0.67169896\n",
      "Iteration 97, loss = 0.67146802\n",
      "Iteration 98, loss = 0.67123704\n",
      "Iteration 99, loss = 0.67100602\n",
      "Iteration 100, loss = 0.67077497\n",
      "Iteration 101, loss = 0.67054387\n",
      "Iteration 102, loss = 0.67031273\n",
      "Iteration 103, loss = 0.67008155\n",
      "Iteration 104, loss = 0.66985032\n",
      "Iteration 105, loss = 0.66961903\n",
      "Iteration 106, loss = 0.66938770\n",
      "Iteration 107, loss = 0.66915632\n",
      "Iteration 108, loss = 0.66892488\n",
      "Iteration 109, loss = 0.66869338\n",
      "Iteration 110, loss = 0.66846182\n",
      "Iteration 111, loss = 0.66823021\n",
      "Iteration 112, loss = 0.66799853\n",
      "Iteration 113, loss = 0.66776678\n",
      "Iteration 114, loss = 0.66753497\n",
      "Iteration 115, loss = 0.66730309\n",
      "Iteration 116, loss = 0.66707114\n",
      "Iteration 117, loss = 0.66683912\n",
      "Iteration 118, loss = 0.66660702\n",
      "Iteration 119, loss = 0.66637484\n",
      "Iteration 120, loss = 0.66614259\n",
      "Iteration 121, loss = 0.66591026\n",
      "Iteration 122, loss = 0.66567784\n",
      "Iteration 123, loss = 0.66544534\n",
      "Iteration 124, loss = 0.66521276\n",
      "Iteration 125, loss = 0.66498008\n",
      "Iteration 126, loss = 0.66474732\n",
      "Iteration 127, loss = 0.66451447\n",
      "Iteration 128, loss = 0.66428152\n",
      "Iteration 129, loss = 0.66404847\n",
      "Iteration 130, loss = 0.66381533\n",
      "Iteration 131, loss = 0.66358210\n",
      "Iteration 132, loss = 0.66334875\n",
      "Iteration 133, loss = 0.66311531\n",
      "Iteration 134, loss = 0.66288176\n",
      "Iteration 135, loss = 0.66264811\n",
      "Iteration 136, loss = 0.66241435\n",
      "Iteration 137, loss = 0.66218048\n",
      "Iteration 138, loss = 0.66194649\n",
      "Iteration 139, loss = 0.66171240\n",
      "Iteration 140, loss = 0.66147819\n",
      "Iteration 141, loss = 0.66124386\n",
      "Iteration 142, loss = 0.66100941\n",
      "Iteration 143, loss = 0.66077484\n",
      "Iteration 144, loss = 0.66054015\n",
      "Iteration 145, loss = 0.66030533\n",
      "Iteration 146, loss = 0.66007039\n",
      "Iteration 147, loss = 0.65983532\n",
      "Iteration 148, loss = 0.65960013\n",
      "Iteration 149, loss = 0.65936480\n",
      "Iteration 150, loss = 0.65912933\n",
      "Iteration 151, loss = 0.65889374\n",
      "Iteration 152, loss = 0.65865800\n",
      "Iteration 153, loss = 0.65842213\n",
      "Iteration 154, loss = 0.65818612\n",
      "Iteration 155, loss = 0.65794997\n",
      "Iteration 156, loss = 0.65771368\n",
      "Iteration 157, loss = 0.65747724\n",
      "Iteration 158, loss = 0.65724065\n",
      "Iteration 159, loss = 0.65700392\n",
      "Iteration 160, loss = 0.65676703\n",
      "Iteration 161, loss = 0.65653000\n",
      "Iteration 162, loss = 0.65629281\n",
      "Iteration 163, loss = 0.65605547\n",
      "Iteration 164, loss = 0.65581797\n",
      "Iteration 165, loss = 0.65558031\n",
      "Iteration 166, loss = 0.65534249\n",
      "Iteration 167, loss = 0.65510451\n",
      "Iteration 168, loss = 0.65486637\n",
      "Iteration 169, loss = 0.65462807\n",
      "Iteration 170, loss = 0.65438960\n",
      "Iteration 171, loss = 0.65415096\n",
      "Iteration 172, loss = 0.65391215\n",
      "Iteration 173, loss = 0.65367317\n",
      "Iteration 174, loss = 0.65343402\n",
      "Iteration 175, loss = 0.65319469\n",
      "Iteration 176, loss = 0.65295519\n",
      "Iteration 177, loss = 0.65271551\n",
      "Iteration 178, loss = 0.65247565\n",
      "Iteration 179, loss = 0.65223562\n",
      "Iteration 180, loss = 0.65199540\n",
      "Iteration 181, loss = 0.65175500\n",
      "Iteration 182, loss = 0.65151441\n",
      "Iteration 183, loss = 0.65127364\n",
      "Iteration 184, loss = 0.65103268\n",
      "Iteration 185, loss = 0.65079153\n",
      "Iteration 186, loss = 0.65055019\n",
      "Iteration 187, loss = 0.65030865\n",
      "Iteration 188, loss = 0.65006692\n",
      "Iteration 189, loss = 0.64982500\n",
      "Iteration 190, loss = 0.64958288\n",
      "Iteration 191, loss = 0.64934057\n",
      "Iteration 192, loss = 0.64909805\n",
      "Iteration 193, loss = 0.64885533\n",
      "Iteration 194, loss = 0.64861241\n",
      "Iteration 195, loss = 0.64836928\n",
      "Iteration 196, loss = 0.64812595\n",
      "Iteration 197, loss = 0.64788241\n",
      "Iteration 198, loss = 0.64763867\n",
      "Iteration 199, loss = 0.64739471\n",
      "Iteration 200, loss = 0.64715054\n",
      "Iteration 201, loss = 0.64690616\n",
      "Iteration 202, loss = 0.64666157\n",
      "Iteration 203, loss = 0.64641676\n",
      "Iteration 204, loss = 0.64617173\n",
      "Iteration 205, loss = 0.64592649\n",
      "Iteration 206, loss = 0.64568102\n",
      "Iteration 207, loss = 0.64543534\n",
      "Iteration 208, loss = 0.64518943\n",
      "Iteration 209, loss = 0.64494330\n",
      "Iteration 210, loss = 0.64469694\n",
      "Iteration 211, loss = 0.64445036\n",
      "Iteration 212, loss = 0.64420355\n",
      "Iteration 213, loss = 0.64395651\n",
      "Iteration 214, loss = 0.64370924\n",
      "Iteration 215, loss = 0.64346173\n",
      "Iteration 216, loss = 0.64321400\n",
      "Iteration 217, loss = 0.64296603\n",
      "Iteration 218, loss = 0.64271782\n",
      "Iteration 219, loss = 0.64246938\n",
      "Iteration 220, loss = 0.64222070\n",
      "Iteration 221, loss = 0.64197178\n",
      "Iteration 222, loss = 0.64172261\n",
      "Iteration 223, loss = 0.64147321\n",
      "Iteration 224, loss = 0.64122356\n",
      "Iteration 225, loss = 0.64097367\n",
      "Iteration 226, loss = 0.64072353\n",
      "Iteration 227, loss = 0.64047314\n",
      "Iteration 228, loss = 0.64022251\n",
      "Iteration 229, loss = 0.63997162\n",
      "Iteration 230, loss = 0.63972049\n",
      "Iteration 231, loss = 0.63946910\n",
      "Iteration 232, loss = 0.63921746\n",
      "Iteration 233, loss = 0.63896556\n",
      "Iteration 234, loss = 0.63871341\n",
      "Iteration 235, loss = 0.63846100\n",
      "Iteration 236, loss = 0.63820833\n",
      "Iteration 237, loss = 0.63795540\n",
      "Iteration 238, loss = 0.63770222\n",
      "Iteration 239, loss = 0.63744877\n",
      "Iteration 240, loss = 0.63719505\n",
      "Iteration 241, loss = 0.63694108\n",
      "Iteration 242, loss = 0.63668683\n",
      "Iteration 243, loss = 0.63643232\n",
      "Iteration 244, loss = 0.63617755\n",
      "Iteration 245, loss = 0.63592250\n",
      "Iteration 246, loss = 0.63566718\n",
      "Iteration 247, loss = 0.63541160\n",
      "Iteration 248, loss = 0.63515574\n",
      "Iteration 249, loss = 0.63489960\n",
      "Iteration 250, loss = 0.63464319\n",
      "Iteration 251, loss = 0.63438651\n",
      "Iteration 252, loss = 0.63412955\n",
      "Iteration 253, loss = 0.63387231\n",
      "Iteration 254, loss = 0.63361479\n",
      "Iteration 255, loss = 0.63335700\n",
      "Iteration 256, loss = 0.63309892\n",
      "Iteration 257, loss = 0.63284056\n",
      "Iteration 258, loss = 0.63258191\n",
      "Iteration 259, loss = 0.63232299\n",
      "Iteration 260, loss = 0.63206377\n",
      "Iteration 261, loss = 0.63180427\n",
      "Iteration 262, loss = 0.63154448\n",
      "Iteration 263, loss = 0.63128441\n",
      "Iteration 264, loss = 0.63102404\n",
      "Iteration 265, loss = 0.63076338\n",
      "Iteration 266, loss = 0.63050244\n",
      "Iteration 267, loss = 0.63024120\n",
      "Iteration 268, loss = 0.62997966\n",
      "Iteration 269, loss = 0.62971783\n",
      "Iteration 270, loss = 0.62945571\n",
      "Iteration 271, loss = 0.62919329\n",
      "Iteration 272, loss = 0.62893057\n",
      "Iteration 273, loss = 0.62866755\n",
      "Iteration 274, loss = 0.62840423\n",
      "Iteration 275, loss = 0.62814062\n",
      "Iteration 276, loss = 0.62787670\n",
      "Iteration 277, loss = 0.62761248\n",
      "Iteration 278, loss = 0.62734795\n",
      "Iteration 279, loss = 0.62708312\n",
      "Iteration 280, loss = 0.62681799\n",
      "Iteration 281, loss = 0.62655255\n",
      "Iteration 282, loss = 0.62628680\n",
      "Iteration 283, loss = 0.62602075\n",
      "Iteration 284, loss = 0.62575438\n",
      "Iteration 285, loss = 0.62548771\n",
      "Iteration 286, loss = 0.62522073\n",
      "Iteration 287, loss = 0.62495343\n",
      "Iteration 288, loss = 0.62468582\n",
      "Iteration 289, loss = 0.62441790\n",
      "Iteration 290, loss = 0.62414966\n",
      "Iteration 291, loss = 0.62388111\n",
      "Iteration 292, loss = 0.62361225\n",
      "Iteration 293, loss = 0.62334306\n",
      "Iteration 294, loss = 0.62307356\n",
      "Iteration 295, loss = 0.62280374\n",
      "Iteration 296, loss = 0.62253360\n",
      "Iteration 297, loss = 0.62226314\n",
      "Iteration 298, loss = 0.62199237\n",
      "Iteration 299, loss = 0.62172126\n",
      "Iteration 300, loss = 0.62144984\n",
      "Iteration 1, loss = 0.72837800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69405011\n",
      "Iteration 3, loss = 0.70091041\n",
      "Iteration 4, loss = 0.69680094\n",
      "Iteration 5, loss = 0.69350497\n",
      "Iteration 6, loss = 0.69334218\n",
      "Iteration 7, loss = 0.69326446\n",
      "Iteration 8, loss = 0.69299043\n",
      "Iteration 9, loss = 0.69280580\n",
      "Iteration 10, loss = 0.69265258\n",
      "Iteration 11, loss = 0.69248340\n",
      "Iteration 12, loss = 0.69230648\n",
      "Iteration 13, loss = 0.69212525\n",
      "Iteration 14, loss = 0.69193813\n",
      "Iteration 15, loss = 0.69174540\n",
      "Iteration 16, loss = 0.69154783\n",
      "Iteration 17, loss = 0.69134591\n",
      "Iteration 18, loss = 0.69114004\n",
      "Iteration 19, loss = 0.69093065\n",
      "Iteration 20, loss = 0.69071809\n",
      "Iteration 21, loss = 0.69050271\n",
      "Iteration 22, loss = 0.69028478\n",
      "Iteration 23, loss = 0.69006459\n",
      "Iteration 24, loss = 0.68984237\n",
      "Iteration 25, loss = 0.68961834\n",
      "Iteration 26, loss = 0.68939269\n",
      "Iteration 27, loss = 0.68916559\n",
      "Iteration 28, loss = 0.68893721\n",
      "Iteration 29, loss = 0.68870768\n",
      "Iteration 30, loss = 0.68847713\n",
      "Iteration 31, loss = 0.68824567\n",
      "Iteration 32, loss = 0.68801340\n",
      "Iteration 33, loss = 0.68778042\n",
      "Iteration 34, loss = 0.68754680\n",
      "Iteration 35, loss = 0.68731263\n",
      "Iteration 36, loss = 0.68707796\n",
      "Iteration 37, loss = 0.68684286\n",
      "Iteration 38, loss = 0.68660738\n",
      "Iteration 39, loss = 0.68637156\n",
      "Iteration 40, loss = 0.68613545\n",
      "Iteration 41, loss = 0.68589909\n",
      "Iteration 42, loss = 0.68566250\n",
      "Iteration 43, loss = 0.68542573\n",
      "Iteration 44, loss = 0.68518879\n",
      "Iteration 45, loss = 0.68495171\n",
      "Iteration 46, loss = 0.68471451\n",
      "Iteration 47, loss = 0.68447721\n",
      "Iteration 48, loss = 0.68423983\n",
      "Iteration 49, loss = 0.68400238\n",
      "Iteration 50, loss = 0.68376488\n",
      "Iteration 51, loss = 0.68352733\n",
      "Iteration 52, loss = 0.68328975\n",
      "Iteration 53, loss = 0.68305214\n",
      "Iteration 54, loss = 0.68281452\n",
      "Iteration 55, loss = 0.68257688\n",
      "Iteration 56, loss = 0.68233924\n",
      "Iteration 57, loss = 0.68210160\n",
      "Iteration 58, loss = 0.68186397\n",
      "Iteration 59, loss = 0.68162634\n",
      "Iteration 60, loss = 0.68138872\n",
      "Iteration 61, loss = 0.68115112\n",
      "Iteration 62, loss = 0.68091353\n",
      "Iteration 63, loss = 0.68067596\n",
      "Iteration 64, loss = 0.68043840\n",
      "Iteration 65, loss = 0.68020086\n",
      "Iteration 66, loss = 0.67996335\n",
      "Iteration 67, loss = 0.67972585\n",
      "Iteration 68, loss = 0.67948836\n",
      "Iteration 69, loss = 0.67925090\n",
      "Iteration 70, loss = 0.67901346\n",
      "Iteration 71, loss = 0.67877603\n",
      "Iteration 72, loss = 0.67853861\n",
      "Iteration 73, loss = 0.67830122\n",
      "Iteration 74, loss = 0.67806383\n",
      "Iteration 75, loss = 0.67782646\n",
      "Iteration 76, loss = 0.67758910\n",
      "Iteration 77, loss = 0.67735175\n",
      "Iteration 78, loss = 0.67711440\n",
      "Iteration 79, loss = 0.67687707\n",
      "Iteration 80, loss = 0.67663973\n",
      "Iteration 81, loss = 0.67640240\n",
      "Iteration 82, loss = 0.67616507\n",
      "Iteration 83, loss = 0.67592774\n",
      "Iteration 84, loss = 0.67569041\n",
      "Iteration 85, loss = 0.67545307\n",
      "Iteration 86, loss = 0.67521572\n",
      "Iteration 87, loss = 0.67497837\n",
      "Iteration 88, loss = 0.67474100\n",
      "Iteration 89, loss = 0.67450363\n",
      "Iteration 90, loss = 0.67426623\n",
      "Iteration 91, loss = 0.67402882\n",
      "Iteration 92, loss = 0.67379139\n",
      "Iteration 93, loss = 0.67355394\n",
      "Iteration 94, loss = 0.67331646\n",
      "Iteration 95, loss = 0.67307896\n",
      "Iteration 96, loss = 0.67284143\n",
      "Iteration 97, loss = 0.67260388\n",
      "Iteration 98, loss = 0.67236629\n",
      "Iteration 99, loss = 0.67212866\n",
      "Iteration 100, loss = 0.67189100\n",
      "Iteration 101, loss = 0.67165330\n",
      "Iteration 102, loss = 0.67141556\n",
      "Iteration 103, loss = 0.67117777\n",
      "Iteration 104, loss = 0.67093994\n",
      "Iteration 105, loss = 0.67070207\n",
      "Iteration 106, loss = 0.67046414\n",
      "Iteration 107, loss = 0.67022616\n",
      "Iteration 108, loss = 0.66998813\n",
      "Iteration 109, loss = 0.66975004\n",
      "Iteration 110, loss = 0.66951190\n",
      "Iteration 111, loss = 0.66927369\n",
      "Iteration 112, loss = 0.66903543\n",
      "Iteration 113, loss = 0.66879709\n",
      "Iteration 114, loss = 0.66855869\n",
      "Iteration 115, loss = 0.66832023\n",
      "Iteration 116, loss = 0.66808169\n",
      "Iteration 117, loss = 0.66784308\n",
      "Iteration 118, loss = 0.66760439\n",
      "Iteration 119, loss = 0.66736563\n",
      "Iteration 120, loss = 0.66712679\n",
      "Iteration 121, loss = 0.66688787\n",
      "Iteration 122, loss = 0.66664886\n",
      "Iteration 123, loss = 0.66640977\n",
      "Iteration 124, loss = 0.66617059\n",
      "Iteration 125, loss = 0.66593132\n",
      "Iteration 126, loss = 0.66569196\n",
      "Iteration 127, loss = 0.66545251\n",
      "Iteration 128, loss = 0.66521296\n",
      "Iteration 129, loss = 0.66497332\n",
      "Iteration 130, loss = 0.66473357\n",
      "Iteration 131, loss = 0.66449372\n",
      "Iteration 132, loss = 0.66425377\n",
      "Iteration 133, loss = 0.66401372\n",
      "Iteration 134, loss = 0.66377355\n",
      "Iteration 135, loss = 0.66353328\n",
      "Iteration 136, loss = 0.66329289\n",
      "Iteration 137, loss = 0.66305239\n",
      "Iteration 138, loss = 0.66281178\n",
      "Iteration 139, loss = 0.66257105\n",
      "Iteration 140, loss = 0.66233020\n",
      "Iteration 141, loss = 0.66208923\n",
      "Iteration 142, loss = 0.66184813\n",
      "Iteration 143, loss = 0.66160691\n",
      "Iteration 144, loss = 0.66136556\n",
      "Iteration 145, loss = 0.66112408\n",
      "Iteration 146, loss = 0.66088247\n",
      "Iteration 147, loss = 0.66064073\n",
      "Iteration 148, loss = 0.66039886\n",
      "Iteration 149, loss = 0.66015684\n",
      "Iteration 150, loss = 0.65991469\n",
      "Iteration 151, loss = 0.65967240\n",
      "Iteration 152, loss = 0.65942997\n",
      "Iteration 153, loss = 0.65918739\n",
      "Iteration 154, loss = 0.65894466\n",
      "Iteration 155, loss = 0.65870179\n",
      "Iteration 156, loss = 0.65845877\n",
      "Iteration 157, loss = 0.65821559\n",
      "Iteration 158, loss = 0.65797227\n",
      "Iteration 159, loss = 0.65772878\n",
      "Iteration 160, loss = 0.65748514\n",
      "Iteration 161, loss = 0.65724134\n",
      "Iteration 162, loss = 0.65699738\n",
      "Iteration 163, loss = 0.65675326\n",
      "Iteration 164, loss = 0.65650897\n",
      "Iteration 165, loss = 0.65626452\n",
      "Iteration 166, loss = 0.65601990\n",
      "Iteration 167, loss = 0.65577511\n",
      "Iteration 168, loss = 0.65553015\n",
      "Iteration 169, loss = 0.65528501\n",
      "Iteration 170, loss = 0.65503970\n",
      "Iteration 171, loss = 0.65479421\n",
      "Iteration 172, loss = 0.65454854\n",
      "Iteration 173, loss = 0.65430270\n",
      "Iteration 174, loss = 0.65405667\n",
      "Iteration 175, loss = 0.65381046\n",
      "Iteration 176, loss = 0.65356406\n",
      "Iteration 177, loss = 0.65331747\n",
      "Iteration 178, loss = 0.65307070\n",
      "Iteration 179, loss = 0.65282373\n",
      "Iteration 180, loss = 0.65257658\n",
      "Iteration 181, loss = 0.65232922\n",
      "Iteration 182, loss = 0.65208168\n",
      "Iteration 183, loss = 0.65183393\n",
      "Iteration 184, loss = 0.65158599\n",
      "Iteration 185, loss = 0.65133784\n",
      "Iteration 186, loss = 0.65108950\n",
      "Iteration 187, loss = 0.65084095\n",
      "Iteration 188, loss = 0.65059219\n",
      "Iteration 189, loss = 0.65034323\n",
      "Iteration 190, loss = 0.65009405\n",
      "Iteration 191, loss = 0.64984467\n",
      "Iteration 192, loss = 0.64959507\n",
      "Iteration 193, loss = 0.64934526\n",
      "Iteration 194, loss = 0.64909524\n",
      "Iteration 195, loss = 0.64884499\n",
      "Iteration 196, loss = 0.64859453\n",
      "Iteration 197, loss = 0.64834385\n",
      "Iteration 198, loss = 0.64809295\n",
      "Iteration 199, loss = 0.64784182\n",
      "Iteration 200, loss = 0.64759047\n",
      "Iteration 201, loss = 0.64733889\n",
      "Iteration 202, loss = 0.64708708\n",
      "Iteration 203, loss = 0.64683505\n",
      "Iteration 204, loss = 0.64658278\n",
      "Iteration 205, loss = 0.64633028\n",
      "Iteration 206, loss = 0.64607754\n",
      "Iteration 207, loss = 0.64582457\n",
      "Iteration 208, loss = 0.64557136\n",
      "Iteration 209, loss = 0.64531792\n",
      "Iteration 210, loss = 0.64506423\n",
      "Iteration 211, loss = 0.64481030\n",
      "Iteration 212, loss = 0.64455613\n",
      "Iteration 213, loss = 0.64430171\n",
      "Iteration 214, loss = 0.64404704\n",
      "Iteration 215, loss = 0.64379213\n",
      "Iteration 216, loss = 0.64353697\n",
      "Iteration 217, loss = 0.64328156\n",
      "Iteration 218, loss = 0.64302589\n",
      "Iteration 219, loss = 0.64276997\n",
      "Iteration 220, loss = 0.64251380\n",
      "Iteration 221, loss = 0.64225737\n",
      "Iteration 222, loss = 0.64200068\n",
      "Iteration 223, loss = 0.64174373\n",
      "Iteration 224, loss = 0.64148652\n",
      "Iteration 225, loss = 0.64122905\n",
      "Iteration 226, loss = 0.64097132\n",
      "Iteration 227, loss = 0.64071332\n",
      "Iteration 228, loss = 0.64045505\n",
      "Iteration 229, loss = 0.64019651\n",
      "Iteration 230, loss = 0.63993771\n",
      "Iteration 231, loss = 0.63967863\n",
      "Iteration 232, loss = 0.63941929\n",
      "Iteration 233, loss = 0.63915966\n",
      "Iteration 234, loss = 0.63889977\n",
      "Iteration 235, loss = 0.63863960\n",
      "Iteration 236, loss = 0.63837915\n",
      "Iteration 237, loss = 0.63811842\n",
      "Iteration 238, loss = 0.63785741\n",
      "Iteration 239, loss = 0.63759612\n",
      "Iteration 240, loss = 0.63733455\n",
      "Iteration 241, loss = 0.63707269\n",
      "Iteration 242, loss = 0.63681055\n",
      "Iteration 243, loss = 0.63654812\n",
      "Iteration 244, loss = 0.63628540\n",
      "Iteration 245, loss = 0.63602239\n",
      "Iteration 246, loss = 0.63575909\n",
      "Iteration 247, loss = 0.63549550\n",
      "Iteration 248, loss = 0.63523162\n",
      "Iteration 249, loss = 0.63496744\n",
      "Iteration 250, loss = 0.63470297\n",
      "Iteration 251, loss = 0.63443820\n",
      "Iteration 252, loss = 0.63417313\n",
      "Iteration 253, loss = 0.63390776\n",
      "Iteration 254, loss = 0.63364209\n",
      "Iteration 255, loss = 0.63337612\n",
      "Iteration 256, loss = 0.63310984\n",
      "Iteration 257, loss = 0.63284327\n",
      "Iteration 258, loss = 0.63257638\n",
      "Iteration 259, loss = 0.63230919\n",
      "Iteration 260, loss = 0.63204169\n",
      "Iteration 261, loss = 0.63177388\n",
      "Iteration 262, loss = 0.63150576\n",
      "Iteration 263, loss = 0.63123733\n",
      "Iteration 264, loss = 0.63096859\n",
      "Iteration 265, loss = 0.63069953\n",
      "Iteration 266, loss = 0.63043016\n",
      "Iteration 267, loss = 0.63016047\n",
      "Iteration 268, loss = 0.62989046\n",
      "Iteration 269, loss = 0.62962014\n",
      "Iteration 270, loss = 0.62934950\n",
      "Iteration 271, loss = 0.62907853\n",
      "Iteration 272, loss = 0.62880724\n",
      "Iteration 273, loss = 0.62853563\n",
      "Iteration 274, loss = 0.62826370\n",
      "Iteration 275, loss = 0.62799144\n",
      "Iteration 276, loss = 0.62771886\n",
      "Iteration 277, loss = 0.62744594\n",
      "Iteration 278, loss = 0.62717270\n",
      "Iteration 279, loss = 0.62689913\n",
      "Iteration 280, loss = 0.62662523\n",
      "Iteration 281, loss = 0.62635100\n",
      "Iteration 282, loss = 0.62607643\n",
      "Iteration 283, loss = 0.62580153\n",
      "Iteration 284, loss = 0.62552630\n",
      "Iteration 285, loss = 0.62525073\n",
      "Iteration 286, loss = 0.62497482\n",
      "Iteration 287, loss = 0.62469858\n",
      "Iteration 288, loss = 0.62442200\n",
      "Iteration 289, loss = 0.62414507\n",
      "Iteration 290, loss = 0.62386781\n",
      "Iteration 291, loss = 0.62359020\n",
      "Iteration 292, loss = 0.62331226\n",
      "Iteration 293, loss = 0.62303396\n",
      "Iteration 294, loss = 0.62275533\n",
      "Iteration 295, loss = 0.62247634\n",
      "Iteration 296, loss = 0.62219702\n",
      "Iteration 297, loss = 0.62191734\n",
      "Iteration 298, loss = 0.62163731\n",
      "Iteration 299, loss = 0.62135694\n",
      "Iteration 300, loss = 0.62107621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73088966\n",
      "Iteration 2, loss = 6.16721756\n",
      "Iteration 3, loss = 2.65705917\n",
      "Iteration 4, loss = 2.16125698\n",
      "Iteration 5, loss = 3.28043044\n",
      "Iteration 6, loss = 2.35910912\n",
      "Iteration 7, loss = 0.70657717\n",
      "Iteration 8, loss = 1.96835260\n",
      "Iteration 9, loss = 2.43969280\n",
      "Iteration 10, loss = 1.72179558\n",
      "Iteration 11, loss = 0.69310758\n",
      "Iteration 12, loss = 1.51527925\n",
      "Iteration 13, loss = 1.86561719\n",
      "Iteration 14, loss = 1.31692236\n",
      "Iteration 15, loss = 0.69292753\n",
      "Iteration 16, loss = 1.27035575\n",
      "Iteration 17, loss = 1.46216557\n",
      "Iteration 18, loss = 0.99942959\n",
      "Iteration 19, loss = 0.72538258\n",
      "Iteration 20, loss = 1.15126852\n",
      "Iteration 21, loss = 1.15934019\n",
      "Iteration 22, loss = 0.76495288\n",
      "Iteration 23, loss = 0.81054063\n",
      "Iteration 24, loss = 1.06641477\n",
      "Iteration 25, loss = 0.91165065\n",
      "Iteration 26, loss = 0.68644491\n",
      "Iteration 27, loss = 0.88091549\n",
      "Iteration 28, loss = 0.93580136\n",
      "Iteration 29, loss = 0.72770097\n",
      "Iteration 30, loss = 0.74456187\n",
      "Iteration 31, loss = 0.88012252\n",
      "Iteration 32, loss = 0.77518372\n",
      "Iteration 33, loss = 0.68952015\n",
      "Iteration 34, loss = 0.80641573\n",
      "Iteration 35, loss = 0.78670127\n",
      "Iteration 36, loss = 0.68391230\n",
      "Iteration 37, loss = 0.74940369\n",
      "Iteration 38, loss = 0.77494889\n",
      "Iteration 39, loss = 0.69070352\n",
      "Iteration 40, loss = 0.71465743\n",
      "Iteration 41, loss = 0.75481345\n",
      "Iteration 42, loss = 0.69473337\n",
      "Iteration 43, loss = 0.69673605\n",
      "Iteration 44, loss = 0.73525315\n",
      "Iteration 45, loss = 0.69438931\n",
      "Iteration 46, loss = 0.68776165\n",
      "Iteration 47, loss = 0.71952016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72511345\n",
      "Iteration 2, loss = 6.24386005\n",
      "Iteration 3, loss = 2.60687252\n",
      "Iteration 4, loss = 2.25655514\n",
      "Iteration 5, loss = 3.43569181\n",
      "Iteration 6, loss = 2.59937734\n",
      "Iteration 7, loss = 0.80922292\n",
      "Iteration 8, loss = 1.96717808\n",
      "Iteration 9, loss = 2.70829707\n",
      "Iteration 10, loss = 2.18118109\n",
      "Iteration 11, loss = 0.90265063\n",
      "Iteration 12, loss = 1.30680007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72837800\n",
      "Iteration 2, loss = 6.20550341\n",
      "Iteration 3, loss = 2.62968068\n",
      "Iteration 4, loss = 2.21466217\n",
      "Iteration 5, loss = 3.36338962\n",
      "Iteration 6, loss = 2.48298573\n",
      "Iteration 7, loss = 0.74655860\n",
      "Iteration 8, loss = 1.98588091\n",
      "Iteration 9, loss = 2.60300888\n",
      "Iteration 10, loss = 1.98456827\n",
      "Iteration 11, loss = 0.77068701\n",
      "Iteration 12, loss = 1.43819641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71191562\n",
      "Iteration 2, loss = 6.07177183\n",
      "Iteration 3, loss = 18.13962372\n",
      "Iteration 4, loss = 2.29861587\n",
      "Iteration 5, loss = 13.79328943\n",
      "Iteration 6, loss = 5.17255001\n",
      "Iteration 7, loss = 0.72095041\n",
      "Iteration 8, loss = 1.30105801\n",
      "Iteration 9, loss = 0.70069270\n",
      "Iteration 10, loss = 1.81119893\n",
      "Iteration 11, loss = 4.23242265\n",
      "Iteration 12, loss = 5.27536050\n",
      "Iteration 13, loss = 4.33606482\n",
      "Iteration 14, loss = 6.18243625\n",
      "Iteration 15, loss = 2.73129442\n",
      "Iteration 16, loss = 4.49092032\n",
      "Iteration 17, loss = 1.15199982\n",
      "Iteration 18, loss = 1.25320551\n",
      "Iteration 19, loss = 0.88468168\n",
      "Iteration 20, loss = 0.71772976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71275864\n",
      "Iteration 2, loss = 5.85589441\n",
      "Iteration 3, loss = 17.90404430\n",
      "Iteration 4, loss = 3.28846853\n",
      "Iteration 5, loss = 12.58189438\n",
      "Iteration 6, loss = 4.49487313\n",
      "Iteration 7, loss = 0.71338246\n",
      "Iteration 8, loss = 0.96875846\n",
      "Iteration 9, loss = 0.72626779\n",
      "Iteration 10, loss = 0.95003755\n",
      "Iteration 11, loss = 3.67923246\n",
      "Iteration 12, loss = 7.15781249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71376250\n",
      "Iteration 2, loss = 5.97211485\n",
      "Iteration 3, loss = 18.02183396\n",
      "Iteration 4, loss = 2.77537799\n",
      "Iteration 5, loss = 13.26502103\n",
      "Iteration 6, loss = 4.87613710\n",
      "Iteration 7, loss = 0.71518338\n",
      "Iteration 8, loss = 1.17770854\n",
      "Iteration 9, loss = 0.70284539\n",
      "Iteration 10, loss = 1.45158161\n",
      "Iteration 11, loss = 4.58204011\n",
      "Iteration 12, loss = 5.05079714\n",
      "Iteration 13, loss = 5.01220298\n",
      "Iteration 14, loss = 5.77846463\n",
      "Iteration 15, loss = 3.53361279\n",
      "Iteration 16, loss = 4.05870368\n",
      "Iteration 17, loss = 1.62892177\n",
      "Iteration 18, loss = 1.43948025\n",
      "Iteration 19, loss = 0.78802718\n",
      "Iteration 20, loss = 0.71183095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71191562\n",
      "Iteration 2, loss = 17.90404689\n",
      "Iteration 3, loss = 17.90404469\n",
      "Iteration 4, loss = 18.13962402\n",
      "Iteration 5, loss = 18.13962449\n",
      "Iteration 6, loss = 18.13962417\n",
      "Iteration 7, loss = 10.28476593\n",
      "Iteration 8, loss = 17.90404440\n",
      "Iteration 9, loss = 17.90404479\n",
      "Iteration 10, loss = 17.90404480\n",
      "Iteration 11, loss = 17.90404449\n",
      "Iteration 12, loss = 12.11821780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71275864\n",
      "Iteration 2, loss = 18.13962633\n",
      "Iteration 3, loss = 18.13962410\n",
      "Iteration 4, loss = 17.90404461\n",
      "Iteration 5, loss = 17.90404514\n",
      "Iteration 6, loss = 17.90404484\n",
      "Iteration 7, loss = 14.98215516\n",
      "Iteration 8, loss = 18.13962374\n",
      "Iteration 9, loss = 18.13962403\n",
      "Iteration 10, loss = 18.13962402\n",
      "Iteration 11, loss = 18.13962374\n",
      "Iteration 12, loss = 2.90502271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71376250\n",
      "Iteration 2, loss = 18.02183655\n",
      "Iteration 3, loss = 18.02183434\n",
      "Iteration 4, loss = 18.02183427\n",
      "Iteration 5, loss = 18.02183476\n",
      "Iteration 6, loss = 18.02183445\n",
      "Iteration 7, loss = 12.55495788\n",
      "Iteration 8, loss = 18.02183402\n",
      "Iteration 9, loss = 18.02183436\n",
      "Iteration 10, loss = 18.02183436\n",
      "Iteration 11, loss = 18.02183407\n",
      "Iteration 12, loss = 7.73214474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76254741\n",
      "Iteration 2, loss = 13.24673806\n",
      "Iteration 3, loss = 16.25663024\n",
      "Iteration 4, loss = 4.04591067\n",
      "Iteration 5, loss = 10.64510355\n",
      "Iteration 6, loss = 0.86682655\n",
      "Iteration 7, loss = 2.60874132\n",
      "Iteration 8, loss = 0.73640128\n",
      "Iteration 9, loss = 1.09232120\n",
      "Iteration 10, loss = 1.55903773\n",
      "Iteration 11, loss = 0.82781249\n",
      "Iteration 12, loss = 1.48036202\n",
      "Iteration 13, loss = 0.78829120\n",
      "Iteration 14, loss = 1.25911130\n",
      "Iteration 15, loss = 2.69584763\n",
      "Iteration 16, loss = 1.64066362\n",
      "Iteration 17, loss = 1.97547413\n",
      "Iteration 18, loss = 1.37429952\n",
      "Iteration 19, loss = 0.94327245\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75807209\n",
      "Iteration 2, loss = 12.90995618\n",
      "Iteration 3, loss = 17.17417744\n",
      "Iteration 4, loss = 2.64222572\n",
      "Iteration 5, loss = 12.12538905\n",
      "Iteration 6, loss = 1.75672865\n",
      "Iteration 7, loss = 2.94404042\n",
      "Iteration 8, loss = 0.69342970\n",
      "Iteration 9, loss = 2.59653723\n",
      "Iteration 10, loss = 2.79364914\n",
      "Iteration 11, loss = 1.68402407\n",
      "Iteration 12, loss = 0.80837082\n",
      "Iteration 13, loss = 0.71871271\n",
      "Iteration 14, loss = 0.84260291\n",
      "Iteration 15, loss = 0.80341049\n",
      "Iteration 16, loss = 0.70758816\n",
      "Iteration 17, loss = 0.70912396\n",
      "Iteration 18, loss = 0.75107463\n",
      "Iteration 19, loss = 0.71894887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75609056\n",
      "Iteration 2, loss = 13.11405904\n",
      "Iteration 3, loss = 16.64884273\n",
      "Iteration 4, loss = 3.40765398\n",
      "Iteration 5, loss = 11.36882055\n",
      "Iteration 6, loss = 1.26761492\n",
      "Iteration 7, loss = 3.09908228\n",
      "Iteration 8, loss = 0.70813561\n",
      "Iteration 9, loss = 2.25500733\n",
      "Iteration 10, loss = 2.53988410\n",
      "Iteration 11, loss = 1.47475276\n",
      "Iteration 12, loss = 0.70682664\n",
      "Iteration 13, loss = 1.09090885\n",
      "Iteration 14, loss = 0.85566920\n",
      "Iteration 15, loss = 0.70297490\n",
      "Iteration 16, loss = 0.70547049\n",
      "Iteration 17, loss = 0.70522178\n",
      "Iteration 18, loss = 0.70045065\n",
      "Iteration 19, loss = 0.69922155\n",
      "Iteration 20, loss = 0.69924968\n",
      "Iteration 21, loss = 0.69904186\n",
      "Iteration 22, loss = 0.69877194\n",
      "Iteration 23, loss = 0.69857859\n",
      "Iteration 24, loss = 0.69840918\n",
      "Iteration 25, loss = 0.69823009\n",
      "Iteration 26, loss = 0.69804603\n",
      "Iteration 27, loss = 0.69786325\n",
      "Iteration 28, loss = 0.69768169\n",
      "Iteration 29, loss = 0.69750045\n",
      "Iteration 30, loss = 0.69731989\n",
      "Iteration 31, loss = 0.69714068\n",
      "Iteration 32, loss = 0.69696322\n",
      "Iteration 33, loss = 0.69678773\n",
      "Iteration 34, loss = 0.69661442\n",
      "Iteration 35, loss = 0.69644347\n",
      "Iteration 36, loss = 0.69627504\n",
      "Iteration 37, loss = 0.69610922\n",
      "Iteration 38, loss = 0.69594608\n",
      "Iteration 39, loss = 0.69578566\n",
      "Iteration 40, loss = 0.69562797\n",
      "Iteration 41, loss = 0.69547299\n",
      "Iteration 42, loss = 0.69532069\n",
      "Iteration 43, loss = 0.69517103\n",
      "Iteration 44, loss = 0.69502395\n",
      "Iteration 45, loss = 0.69487937\n",
      "Iteration 46, loss = 0.69473722\n",
      "Iteration 47, loss = 0.69459740\n",
      "Iteration 48, loss = 0.69445981\n",
      "Iteration 49, loss = 0.69432435\n",
      "Iteration 50, loss = 0.69419091\n",
      "Iteration 51, loss = 0.69405936\n",
      "Iteration 52, loss = 0.69392961\n",
      "Iteration 53, loss = 0.69380152\n",
      "Iteration 54, loss = 0.69367496\n",
      "Iteration 55, loss = 0.69354982\n",
      "Iteration 56, loss = 0.69342597\n",
      "Iteration 57, loss = 0.69330328\n",
      "Iteration 58, loss = 0.69318162\n",
      "Iteration 59, loss = 0.69306087\n",
      "Iteration 60, loss = 0.69294088\n",
      "Iteration 61, loss = 0.69282154\n",
      "Iteration 62, loss = 0.69270272\n",
      "Iteration 63, loss = 0.69258427\n",
      "Iteration 64, loss = 0.69246608\n",
      "Iteration 65, loss = 0.69234802\n",
      "Iteration 66, loss = 0.69222995\n",
      "Iteration 67, loss = 0.69211175\n",
      "Iteration 68, loss = 0.69199329\n",
      "Iteration 69, loss = 0.69187443\n",
      "Iteration 70, loss = 0.69175505\n",
      "Iteration 71, loss = 0.69163503\n",
      "Iteration 72, loss = 0.69151422\n",
      "Iteration 73, loss = 0.69139251\n",
      "Iteration 74, loss = 0.69126976\n",
      "Iteration 75, loss = 0.69114585\n",
      "Iteration 76, loss = 0.69102063\n",
      "Iteration 77, loss = 0.69089399\n",
      "Iteration 78, loss = 0.69076579\n",
      "Iteration 79, loss = 0.69063590\n",
      "Iteration 80, loss = 0.69050419\n",
      "Iteration 81, loss = 0.69037051\n",
      "Iteration 82, loss = 0.69023475\n",
      "Iteration 83, loss = 0.69009675\n",
      "Iteration 84, loss = 0.68995639\n",
      "Iteration 85, loss = 0.68981352\n",
      "Iteration 86, loss = 0.68966800\n",
      "Iteration 87, loss = 0.68951969\n",
      "Iteration 88, loss = 0.68936845\n",
      "Iteration 89, loss = 0.68921413\n",
      "Iteration 90, loss = 0.68905659\n",
      "Iteration 91, loss = 0.68889567\n",
      "Iteration 92, loss = 0.68873121\n",
      "Iteration 93, loss = 0.68856307\n",
      "Iteration 94, loss = 0.68839109\n",
      "Iteration 95, loss = 0.68821511\n",
      "Iteration 96, loss = 0.68803496\n",
      "Iteration 97, loss = 0.68785048\n",
      "Iteration 98, loss = 0.68766150\n",
      "Iteration 99, loss = 0.68746784\n",
      "Iteration 100, loss = 0.68726934\n",
      "Iteration 101, loss = 0.68706581\n",
      "Iteration 102, loss = 0.68685707\n",
      "Iteration 103, loss = 0.68664293\n",
      "Iteration 104, loss = 0.68642321\n",
      "Iteration 105, loss = 0.68619771\n",
      "Iteration 106, loss = 0.68596623\n",
      "Iteration 107, loss = 0.68572857\n",
      "Iteration 108, loss = 0.68548452\n",
      "Iteration 109, loss = 0.68523388\n",
      "Iteration 110, loss = 0.68497642\n",
      "Iteration 111, loss = 0.68471192\n",
      "Iteration 112, loss = 0.68444017\n",
      "Iteration 113, loss = 0.68416092\n",
      "Iteration 114, loss = 0.68387395\n",
      "Iteration 115, loss = 0.68357901\n",
      "Iteration 116, loss = 0.68327585\n",
      "Iteration 117, loss = 0.68296423\n",
      "Iteration 118, loss = 0.68264388\n",
      "Iteration 119, loss = 0.68231454\n",
      "Iteration 120, loss = 0.68197594\n",
      "Iteration 121, loss = 0.68162781\n",
      "Iteration 122, loss = 0.68126986\n",
      "Iteration 123, loss = 0.68090180\n",
      "Iteration 124, loss = 0.68052334\n",
      "Iteration 125, loss = 0.68013418\n",
      "Iteration 126, loss = 0.67973401\n",
      "Iteration 127, loss = 0.67932250\n",
      "Iteration 128, loss = 0.67889934\n",
      "Iteration 129, loss = 0.67846420\n",
      "Iteration 130, loss = 0.67801674\n",
      "Iteration 131, loss = 0.67755662\n",
      "Iteration 132, loss = 0.67708347\n",
      "Iteration 133, loss = 0.67659695\n",
      "Iteration 134, loss = 0.67609668\n",
      "Iteration 135, loss = 0.67558229\n",
      "Iteration 136, loss = 0.67505339\n",
      "Iteration 137, loss = 0.67450959\n",
      "Iteration 138, loss = 0.67395050\n",
      "Iteration 139, loss = 0.67337569\n",
      "Iteration 140, loss = 0.67278476\n",
      "Iteration 141, loss = 0.67217728\n",
      "Iteration 142, loss = 0.67155281\n",
      "Iteration 143, loss = 0.67091092\n",
      "Iteration 144, loss = 0.67025115\n",
      "Iteration 145, loss = 0.66957305\n",
      "Iteration 146, loss = 0.66887614\n",
      "Iteration 147, loss = 0.66815996\n",
      "Iteration 148, loss = 0.66742400\n",
      "Iteration 149, loss = 0.66666780\n",
      "Iteration 150, loss = 0.66589083\n",
      "Iteration 151, loss = 0.66509259\n",
      "Iteration 152, loss = 0.66427258\n",
      "Iteration 153, loss = 0.66343024\n",
      "Iteration 154, loss = 0.66256507\n",
      "Iteration 155, loss = 0.66167651\n",
      "Iteration 156, loss = 0.66076401\n",
      "Iteration 157, loss = 0.65982702\n",
      "Iteration 158, loss = 0.65886497\n",
      "Iteration 159, loss = 0.65787730\n",
      "Iteration 160, loss = 0.65686343\n",
      "Iteration 161, loss = 0.65582277\n",
      "Iteration 162, loss = 0.65475473\n",
      "Iteration 163, loss = 0.65365872\n",
      "Iteration 164, loss = 0.65253415\n",
      "Iteration 165, loss = 0.65138039\n",
      "Iteration 166, loss = 0.65019686\n",
      "Iteration 167, loss = 0.64898292\n",
      "Iteration 168, loss = 0.64773798\n",
      "Iteration 169, loss = 0.64646140\n",
      "Iteration 170, loss = 0.64515258\n",
      "Iteration 171, loss = 0.64381088\n",
      "Iteration 172, loss = 0.64243569\n",
      "Iteration 173, loss = 0.64102639\n",
      "Iteration 174, loss = 0.63958235\n",
      "Iteration 175, loss = 0.63810296\n",
      "Iteration 176, loss = 0.63658761\n",
      "Iteration 177, loss = 0.63503567\n",
      "Iteration 178, loss = 0.63344656\n",
      "Iteration 179, loss = 0.63181966\n",
      "Iteration 180, loss = 0.63015439\n",
      "Iteration 181, loss = 0.62845015\n",
      "Iteration 182, loss = 0.62670639\n",
      "Iteration 183, loss = 0.62492252\n",
      "Iteration 184, loss = 0.62309801\n",
      "Iteration 185, loss = 0.62123231\n",
      "Iteration 186, loss = 0.61932494\n",
      "Iteration 187, loss = 0.61737611\n",
      "Iteration 188, loss = 0.61540486\n",
      "Iteration 189, loss = 0.61394306\n",
      "Iteration 190, loss = 0.62860230\n",
      "Iteration 191, loss = 1.06618564\n",
      "Iteration 192, loss = 3.91878721\n",
      "Iteration 193, loss = 0.94642307\n",
      "Iteration 194, loss = 1.46377951\n",
      "Iteration 195, loss = 1.43179233\n",
      "Iteration 196, loss = 0.67227240\n",
      "Iteration 197, loss = 0.66101879\n",
      "Iteration 198, loss = 0.66729347\n",
      "Iteration 199, loss = 0.67456200\n",
      "Iteration 200, loss = 0.68201773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76254741\n",
      "Iteration 2, loss = 17.90404703\n",
      "Iteration 3, loss = 17.90404610\n",
      "Iteration 4, loss = 17.51802329\n",
      "Iteration 5, loss = 18.13962588\n",
      "Iteration 6, loss = 15.92962796\n",
      "Iteration 7, loss = 17.68133562\n",
      "Iteration 8, loss = 17.90404486\n",
      "Iteration 9, loss = 14.80562966\n",
      "Iteration 10, loss = 8.60534401\n",
      "Iteration 11, loss = 14.18725786\n",
      "Iteration 12, loss = 5.94742499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75807209\n",
      "Iteration 2, loss = 18.13962646\n",
      "Iteration 3, loss = 18.13962551\n",
      "Iteration 4, loss = 17.90404655\n",
      "Iteration 5, loss = 17.90404652\n",
      "Iteration 6, loss = 17.90404575\n",
      "Iteration 7, loss = 11.69258777\n",
      "Iteration 8, loss = 18.11272031\n",
      "Iteration 9, loss = 6.47227534\n",
      "Iteration 10, loss = 17.89045349\n",
      "Iteration 11, loss = 17.90404460\n",
      "Iteration 12, loss = 17.30912837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75609056\n",
      "Iteration 2, loss = 18.02183668\n",
      "Iteration 3, loss = 18.02183575\n",
      "Iteration 4, loss = 18.02183619\n",
      "Iteration 5, loss = 18.02183613\n",
      "Iteration 6, loss = 18.00569962\n",
      "Iteration 7, loss = 14.96578515\n",
      "Iteration 8, loss = 18.02183449\n",
      "Iteration 9, loss = 10.98113242\n",
      "Iteration 10, loss = 12.95271318\n",
      "Iteration 11, loss = 18.02183418\n",
      "Iteration 12, loss = 11.27445467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73088966\n",
      "Iteration 2, loss = 2.75756282\n",
      "Iteration 3, loss = 6.76006208\n",
      "Iteration 4, loss = 0.69293675\n",
      "Iteration 5, loss = 1.15836303\n",
      "Iteration 6, loss = 1.60144998\n",
      "Iteration 7, loss = 0.87641476\n",
      "Iteration 8, loss = 0.69777841\n",
      "Iteration 9, loss = 0.69280348\n",
      "Iteration 10, loss = 0.69293228\n",
      "Iteration 11, loss = 0.69408206\n",
      "Iteration 12, loss = 0.69652362\n",
      "Iteration 13, loss = 0.69986986\n",
      "Iteration 14, loss = 0.70067586\n",
      "Iteration 15, loss = 0.69715272\n",
      "Iteration 16, loss = 0.69655232\n",
      "Iteration 17, loss = 0.69888715\n",
      "Iteration 18, loss = 0.69741267\n",
      "Iteration 19, loss = 0.69748671\n",
      "Iteration 20, loss = 0.69760107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72511345\n",
      "Iteration 2, loss = 2.64307011\n",
      "Iteration 3, loss = 6.99898225\n",
      "Iteration 4, loss = 0.77833793\n",
      "Iteration 5, loss = 2.61836592\n",
      "Iteration 6, loss = 0.75637727\n",
      "Iteration 7, loss = 0.69554001\n",
      "Iteration 8, loss = 0.69143177\n",
      "Iteration 9, loss = 0.69195457\n",
      "Iteration 10, loss = 0.69297680\n",
      "Iteration 11, loss = 0.69584827\n",
      "Iteration 12, loss = 0.70184980\n",
      "Iteration 13, loss = 0.70815857\n",
      "Iteration 14, loss = 0.70628160\n",
      "Iteration 15, loss = 0.69738103\n",
      "Iteration 16, loss = 0.69795337\n",
      "Iteration 17, loss = 0.70180716\n",
      "Iteration 18, loss = 0.69769548\n",
      "Iteration 19, loss = 0.69747504\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72837800\n",
      "Iteration 2, loss = 2.69379898\n",
      "Iteration 3, loss = 6.89012257\n",
      "Iteration 4, loss = 0.71942008\n",
      "Iteration 5, loss = 1.91705922\n",
      "Iteration 6, loss = 1.21779842\n",
      "Iteration 7, loss = 0.78342314\n",
      "Iteration 8, loss = 0.69730697\n",
      "Iteration 9, loss = 0.69340317\n",
      "Iteration 10, loss = 0.69370780\n",
      "Iteration 11, loss = 0.69539526\n",
      "Iteration 12, loss = 0.69857821\n",
      "Iteration 13, loss = 0.70106083\n",
      "Iteration 14, loss = 0.69856684\n",
      "Iteration 15, loss = 0.69575209\n",
      "Iteration 16, loss = 0.69844169\n",
      "Iteration 17, loss = 0.69739398\n",
      "Iteration 18, loss = 0.69708736\n",
      "Iteration 19, loss = 0.69722843\n",
      "Iteration 20, loss = 0.69736993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73088966\n",
      "Iteration 2, loss = 17.90404638\n",
      "Iteration 3, loss = 17.90404429\n",
      "Iteration 4, loss = 17.48106734\n",
      "Iteration 5, loss = 18.13962374\n",
      "Iteration 6, loss = 18.13962348\n",
      "Iteration 7, loss = 1.61536900\n",
      "Iteration 8, loss = 17.90404407\n",
      "Iteration 9, loss = 17.90404450\n",
      "Iteration 10, loss = 17.90404451\n",
      "Iteration 11, loss = 17.90404418\n",
      "Iteration 12, loss = 9.86639230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72511345\n",
      "Iteration 2, loss = 18.13962581\n",
      "Iteration 3, loss = 18.13962369\n",
      "Iteration 4, loss = 17.90404397\n",
      "Iteration 5, loss = 17.90404439\n",
      "Iteration 6, loss = 17.90404415\n",
      "Iteration 7, loss = 4.87649305\n",
      "Iteration 8, loss = 18.13614294\n",
      "Iteration 9, loss = 18.13962376\n",
      "Iteration 10, loss = 18.13962377\n",
      "Iteration 11, loss = 18.13962347\n",
      "Iteration 12, loss = 6.81251746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72837800\n",
      "Iteration 2, loss = 18.02183603\n",
      "Iteration 3, loss = 18.02183394\n",
      "Iteration 4, loss = 18.01283172\n",
      "Iteration 5, loss = 18.02183402\n",
      "Iteration 6, loss = 18.02183377\n",
      "Iteration 7, loss = 3.27212375\n",
      "Iteration 8, loss = 18.02183369\n",
      "Iteration 9, loss = 18.02183411\n",
      "Iteration 10, loss = 18.02183413\n",
      "Iteration 11, loss = 18.02183382\n",
      "Iteration 12, loss = 9.46698599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71191562\n",
      "Iteration 2, loss = 17.90404507\n",
      "Iteration 3, loss = 17.90406624\n",
      "Iteration 4, loss = 17.90433244\n",
      "Iteration 5, loss = 17.90795609\n",
      "Iteration 6, loss = 17.96381604\n",
      "Iteration 7, loss = 19.32141482\n",
      "Iteration 8, loss = 50.84644076\n",
      "Iteration 9, loss = 701.91036541\n",
      "Iteration 10, loss = 19002.18441079\n",
      "Iteration 11, loss = 395197.33378398\n",
      "Iteration 12, loss = 10936773.26742568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71275864\n",
      "Iteration 2, loss = 18.13962443\n",
      "Iteration 3, loss = 18.13964528\n",
      "Iteration 4, loss = 18.13991329\n",
      "Iteration 5, loss = 18.14352877\n",
      "Iteration 6, loss = 18.19459640\n",
      "Iteration 7, loss = 19.01900809\n",
      "Iteration 8, loss = 36.09148729\n",
      "Iteration 9, loss = 506.79811793\n",
      "Iteration 10, loss = 10531.97545420\n",
      "Iteration 11, loss = 283698.24635984\n",
      "Iteration 12, loss = 6070775.82703881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71376250\n",
      "Iteration 2, loss = 18.02183470\n",
      "Iteration 3, loss = 18.02185558\n",
      "Iteration 4, loss = 18.02212076\n",
      "Iteration 5, loss = 18.02570755\n",
      "Iteration 6, loss = 18.07853350\n",
      "Iteration 7, loss = 19.03729908\n",
      "Iteration 8, loss = 42.98345305\n",
      "Iteration 9, loss = 581.57772262\n",
      "Iteration 10, loss = 14465.79044549\n",
      "Iteration 11, loss = 325810.18763111\n",
      "Iteration 12, loss = 8342289.72297125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71191562\n",
      "Iteration 2, loss = 17.90430877\n",
      "Iteration 3, loss = 17.90414369\n",
      "Iteration 4, loss = 18.13976752\n",
      "Iteration 5, loss = 18.13981239\n",
      "Iteration 6, loss = 18.13985952\n",
      "Iteration 7, loss = 17.90440502\n",
      "Iteration 8, loss = 17.90443035\n",
      "Iteration 9, loss = 17.90435949\n",
      "Iteration 10, loss = 17.90425472\n",
      "Iteration 11, loss = 18.13975551\n",
      "Iteration 12, loss = 18.13973593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71275864\n",
      "Iteration 2, loss = 18.13988820\n",
      "Iteration 3, loss = 18.13971959\n",
      "Iteration 4, loss = 17.90418771\n",
      "Iteration 5, loss = 17.90423992\n",
      "Iteration 6, loss = 17.90429486\n",
      "Iteration 7, loss = 18.14000612\n",
      "Iteration 8, loss = 18.14003289\n",
      "Iteration 9, loss = 18.13996147\n",
      "Iteration 10, loss = 18.13985596\n",
      "Iteration 11, loss = 17.90419808\n",
      "Iteration 12, loss = 17.90417787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71376250\n",
      "Iteration 2, loss = 18.02209672\n",
      "Iteration 3, loss = 18.02193104\n",
      "Iteration 4, loss = 18.02197665\n",
      "Iteration 5, loss = 18.02202468\n",
      "Iteration 6, loss = 18.02207517\n",
      "Iteration 7, loss = 18.02220251\n",
      "Iteration 8, loss = 18.02222831\n",
      "Iteration 9, loss = 18.02215757\n",
      "Iteration 10, loss = 18.02205305\n",
      "Iteration 11, loss = 18.02197490\n",
      "Iteration 12, loss = 18.02195508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76254741\n",
      "Iteration 2, loss = 17.90404753\n",
      "Iteration 3, loss = 17.90411200\n",
      "Iteration 4, loss = 18.14095888\n",
      "Iteration 5, loss = 17.94148241\n",
      "Iteration 6, loss = 18.91155537\n",
      "Iteration 7, loss = 39.48100598\n",
      "Iteration 8, loss = 464.22920457\n",
      "Iteration 9, loss = 12447.95258745\n",
      "Iteration 10, loss = 257817.16415930\n",
      "Iteration 11, loss = 7160933.17404101\n",
      "Iteration 12, loss = 148989497.53222257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75807209\n",
      "Iteration 2, loss = 18.13962674\n",
      "Iteration 3, loss = 18.13968871\n",
      "Iteration 4, loss = 18.14090075\n",
      "Iteration 5, loss = 17.93861395\n",
      "Iteration 6, loss = 18.85113510\n",
      "Iteration 7, loss = 37.83730492\n",
      "Iteration 8, loss = 429.46811057\n",
      "Iteration 9, loss = 11480.61863072\n",
      "Iteration 10, loss = 237758.06573165\n",
      "Iteration 11, loss = 6592297.47098178\n",
      "Iteration 12, loss = 137421336.82470348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75609056\n",
      "Iteration 2, loss = 18.02183709\n",
      "Iteration 3, loss = 18.02190023\n",
      "Iteration 4, loss = 18.02314075\n",
      "Iteration 5, loss = 18.05751143\n",
      "Iteration 6, loss = 18.74979248\n",
      "Iteration 7, loss = 37.75163525\n",
      "Iteration 8, loss = 421.90276197\n",
      "Iteration 9, loss = 10986.34462352\n",
      "Iteration 10, loss = 224105.48532102\n",
      "Iteration 11, loss = 6083078.40012261\n",
      "Iteration 12, loss = 124332114.92517063\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76254741\n",
      "Iteration 2, loss = 17.90430881\n",
      "Iteration 3, loss = 17.90438823\n",
      "Iteration 4, loss = 18.14022421\n",
      "Iteration 5, loss = 18.14018524\n",
      "Iteration 6, loss = 18.14002608\n",
      "Iteration 7, loss = 18.13986759\n",
      "Iteration 8, loss = 17.90421409\n",
      "Iteration 9, loss = 17.90418405\n",
      "Iteration 10, loss = 17.90416041\n",
      "Iteration 11, loss = 17.90415188\n",
      "Iteration 12, loss = 18.13976090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75807209\n",
      "Iteration 2, loss = 18.13988825\n",
      "Iteration 3, loss = 18.13996530\n",
      "Iteration 4, loss = 17.90464557\n",
      "Iteration 5, loss = 17.90461390\n",
      "Iteration 6, loss = 17.90445853\n",
      "Iteration 7, loss = 17.90429998\n",
      "Iteration 8, loss = 18.13980164\n",
      "Iteration 9, loss = 18.13976606\n",
      "Iteration 10, loss = 18.13973615\n",
      "Iteration 11, loss = 18.13972016\n",
      "Iteration 12, loss = 17.90416015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75609056\n",
      "Iteration 2, loss = 18.02209676\n",
      "Iteration 3, loss = 18.02217464\n",
      "Iteration 4, loss = 18.02243087\n",
      "Iteration 5, loss = 18.02239549\n",
      "Iteration 6, loss = 18.02223905\n",
      "Iteration 7, loss = 18.02208143\n",
      "Iteration 8, loss = 18.02200592\n",
      "Iteration 9, loss = 18.02197339\n",
      "Iteration 10, loss = 18.02194694\n",
      "Iteration 11, loss = 18.02193498\n",
      "Iteration 12, loss = 18.02195965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73088966\n",
      "Iteration 2, loss = 17.90404424\n",
      "Iteration 3, loss = 17.90405516\n",
      "Iteration 4, loss = 17.90421944\n",
      "Iteration 5, loss = 18.14309367\n",
      "Iteration 6, loss = 18.00118032\n",
      "Iteration 7, loss = 20.14740760\n",
      "Iteration 8, loss = 73.88023285\n",
      "Iteration 9, loss = 1178.46840965\n",
      "Iteration 10, loss = 32264.38191847\n",
      "Iteration 11, loss = 670619.48182041\n",
      "Iteration 12, loss = 18576946.79436189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72511345\n",
      "Iteration 2, loss = 18.13962363\n",
      "Iteration 3, loss = 18.13963409\n",
      "Iteration 4, loss = 18.13978958\n",
      "Iteration 5, loss = 18.14273597\n",
      "Iteration 6, loss = 17.98371811\n",
      "Iteration 7, loss = 19.87034545\n",
      "Iteration 8, loss = 63.86762370\n",
      "Iteration 9, loss = 1019.65946920\n",
      "Iteration 10, loss = 26468.33037473\n",
      "Iteration 11, loss = 579419.13566138\n",
      "Iteration 12, loss = 15222757.43639572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72837800\n",
      "Iteration 2, loss = 18.02183388\n",
      "Iteration 3, loss = 18.02184445\n",
      "Iteration 4, loss = 18.02200200\n",
      "Iteration 5, loss = 18.02506240\n",
      "Iteration 6, loss = 18.10721688\n",
      "Iteration 7, loss = 19.81617967\n",
      "Iteration 8, loss = 67.45207839\n",
      "Iteration 9, loss = 1055.12966369\n",
      "Iteration 10, loss = 28555.06501382\n",
      "Iteration 11, loss = 599249.67510942\n",
      "Iteration 12, loss = 16475409.20965304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73088966\n",
      "Iteration 2, loss = 17.90430821\n",
      "Iteration 3, loss = 17.90420779\n",
      "Iteration 4, loss = 18.13984487\n",
      "Iteration 5, loss = 18.13988193\n",
      "Iteration 6, loss = 17.90435113\n",
      "Iteration 7, loss = 17.90439343\n",
      "Iteration 8, loss = 17.90440142\n",
      "Iteration 9, loss = 18.13999453\n",
      "Iteration 10, loss = 18.14000465\n",
      "Iteration 11, loss = 18.13999680\n",
      "Iteration 12, loss = 17.90440914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72511345\n",
      "Iteration 2, loss = 18.13988765\n",
      "Iteration 3, loss = 18.13978249\n",
      "Iteration 4, loss = 17.90426495\n",
      "Iteration 5, loss = 17.90431157\n",
      "Iteration 6, loss = 18.13995146\n",
      "Iteration 7, loss = 18.13999965\n",
      "Iteration 8, loss = 18.14001125\n",
      "Iteration 9, loss = 17.90445002\n",
      "Iteration 10, loss = 17.90446319\n",
      "Iteration 11, loss = 17.90445515\n",
      "Iteration 12, loss = 18.14002201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72837800\n",
      "Iteration 2, loss = 18.02209617\n",
      "Iteration 3, loss = 18.02199384\n",
      "Iteration 4, loss = 18.02205336\n",
      "Iteration 5, loss = 18.02209512\n",
      "Iteration 6, loss = 18.02214961\n",
      "Iteration 7, loss = 18.02219456\n",
      "Iteration 8, loss = 18.02220406\n",
      "Iteration 9, loss = 18.02221955\n",
      "Iteration 10, loss = 18.02223094\n",
      "Iteration 11, loss = 18.02222300\n",
      "Iteration 12, loss = 18.02221274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72125764\n",
      "Iteration 2, loss = 0.70761930\n",
      "Iteration 3, loss = 0.70224952\n",
      "Iteration 4, loss = 0.69996770\n",
      "Iteration 5, loss = 0.69983604\n",
      "Iteration 6, loss = 0.69961646\n",
      "Iteration 7, loss = 0.69940379\n",
      "Iteration 8, loss = 0.69917576\n",
      "Iteration 9, loss = 0.69893066\n",
      "Iteration 10, loss = 0.69867048\n",
      "Iteration 11, loss = 0.69839670\n",
      "Iteration 12, loss = 0.69811073\n",
      "Iteration 13, loss = 0.69781386\n",
      "Iteration 14, loss = 0.69750722\n",
      "Iteration 15, loss = 0.69719185\n",
      "Iteration 16, loss = 0.69686868\n",
      "Iteration 17, loss = 0.69653854\n",
      "Iteration 18, loss = 0.69620217\n",
      "Iteration 19, loss = 0.69586026\n",
      "Iteration 20, loss = 0.69551340\n",
      "Iteration 21, loss = 0.69516213\n",
      "Iteration 22, loss = 0.69480695\n",
      "Iteration 23, loss = 0.69444828\n",
      "Iteration 24, loss = 0.69408651\n",
      "Iteration 25, loss = 0.69372200\n",
      "Iteration 26, loss = 0.69335507\n",
      "Iteration 27, loss = 0.69298598\n",
      "Iteration 28, loss = 0.69261499\n",
      "Iteration 29, loss = 0.69224232\n",
      "Iteration 30, loss = 0.69186818\n",
      "Iteration 31, loss = 0.69149274\n",
      "Iteration 32, loss = 0.69111616\n",
      "Iteration 33, loss = 0.69073858\n",
      "Iteration 34, loss = 0.69036013\n",
      "Iteration 35, loss = 0.68998092\n",
      "Iteration 36, loss = 0.68960104\n",
      "Iteration 37, loss = 0.68922058\n",
      "Iteration 38, loss = 0.68883963\n",
      "Iteration 39, loss = 0.68845825\n",
      "Iteration 40, loss = 0.68807649\n",
      "Iteration 41, loss = 0.68769441\n",
      "Iteration 42, loss = 0.68731205\n",
      "Iteration 43, loss = 0.68692946\n",
      "Iteration 44, loss = 0.68654666\n",
      "Iteration 45, loss = 0.68616368\n",
      "Iteration 46, loss = 0.68578056\n",
      "Iteration 47, loss = 0.68539730\n",
      "Iteration 48, loss = 0.68501393\n",
      "Iteration 49, loss = 0.68463046\n",
      "Iteration 50, loss = 0.68424690\n",
      "Iteration 51, loss = 0.68386325\n",
      "Iteration 52, loss = 0.68347953\n",
      "Iteration 53, loss = 0.68309574\n",
      "Iteration 54, loss = 0.68271187\n",
      "Iteration 55, loss = 0.68232794\n",
      "Iteration 56, loss = 0.68194393\n",
      "Iteration 57, loss = 0.68155985\n",
      "Iteration 58, loss = 0.68117569\n",
      "Iteration 59, loss = 0.68079144\n",
      "Iteration 60, loss = 0.68040711\n",
      "Iteration 61, loss = 0.68002268\n",
      "Iteration 62, loss = 0.67963815\n",
      "Iteration 63, loss = 0.67925350\n",
      "Iteration 64, loss = 0.67886874\n",
      "Iteration 65, loss = 0.67848385\n",
      "Iteration 66, loss = 0.67809881\n",
      "Iteration 67, loss = 0.67771363\n",
      "Iteration 68, loss = 0.67732829\n",
      "Iteration 69, loss = 0.67694278\n",
      "Iteration 70, loss = 0.67655709\n",
      "Iteration 71, loss = 0.67617121\n",
      "Iteration 72, loss = 0.67578512\n",
      "Iteration 73, loss = 0.67539881\n",
      "Iteration 74, loss = 0.67501227\n",
      "Iteration 75, loss = 0.67462550\n",
      "Iteration 76, loss = 0.67423847\n",
      "Iteration 77, loss = 0.67385118\n",
      "Iteration 78, loss = 0.67346361\n",
      "Iteration 79, loss = 0.67307575\n",
      "Iteration 80, loss = 0.67268758\n",
      "Iteration 81, loss = 0.67229910\n",
      "Iteration 82, loss = 0.67191029\n",
      "Iteration 83, loss = 0.67152114\n",
      "Iteration 84, loss = 0.67113164\n",
      "Iteration 85, loss = 0.67074177\n",
      "Iteration 86, loss = 0.67035152\n",
      "Iteration 87, loss = 0.66996088\n",
      "Iteration 88, loss = 0.66956983\n",
      "Iteration 89, loss = 0.66917836\n",
      "Iteration 90, loss = 0.66878647\n",
      "Iteration 91, loss = 0.66839413\n",
      "Iteration 92, loss = 0.66800133\n",
      "Iteration 93, loss = 0.66760806\n",
      "Iteration 94, loss = 0.66721431\n",
      "Iteration 95, loss = 0.66682006\n",
      "Iteration 96, loss = 0.66642531\n",
      "Iteration 97, loss = 0.66603004\n",
      "Iteration 98, loss = 0.66563423\n",
      "Iteration 99, loss = 0.66523787\n",
      "Iteration 100, loss = 0.66484096\n",
      "Iteration 101, loss = 0.66444347\n",
      "Iteration 102, loss = 0.66404541\n",
      "Iteration 103, loss = 0.66364674\n",
      "Iteration 104, loss = 0.66324746\n",
      "Iteration 105, loss = 0.66284757\n",
      "Iteration 106, loss = 0.66244704\n",
      "Iteration 107, loss = 0.66204586\n",
      "Iteration 108, loss = 0.66164402\n",
      "Iteration 109, loss = 0.66124151\n",
      "Iteration 110, loss = 0.66083832\n",
      "Iteration 111, loss = 0.66043443\n",
      "Iteration 112, loss = 0.66002983\n",
      "Iteration 113, loss = 0.65962451\n",
      "Iteration 114, loss = 0.65921846\n",
      "Iteration 115, loss = 0.65881166\n",
      "Iteration 116, loss = 0.65840411\n",
      "Iteration 117, loss = 0.65799578\n",
      "Iteration 118, loss = 0.65758668\n",
      "Iteration 119, loss = 0.65717678\n",
      "Iteration 120, loss = 0.65676607\n",
      "Iteration 121, loss = 0.65635455\n",
      "Iteration 122, loss = 0.65594219\n",
      "Iteration 123, loss = 0.65552900\n",
      "Iteration 124, loss = 0.65511495\n",
      "Iteration 125, loss = 0.65470004\n",
      "Iteration 126, loss = 0.65428424\n",
      "Iteration 127, loss = 0.65386756\n",
      "Iteration 128, loss = 0.65344998\n",
      "Iteration 129, loss = 0.65303149\n",
      "Iteration 130, loss = 0.65261207\n",
      "Iteration 131, loss = 0.65219171\n",
      "Iteration 132, loss = 0.65177041\n",
      "Iteration 133, loss = 0.65134814\n",
      "Iteration 134, loss = 0.65092491\n",
      "Iteration 135, loss = 0.65050069\n",
      "Iteration 136, loss = 0.65007548\n",
      "Iteration 137, loss = 0.64964927\n",
      "Iteration 138, loss = 0.64922204\n",
      "Iteration 139, loss = 0.64879377\n",
      "Iteration 140, loss = 0.64836447\n",
      "Iteration 141, loss = 0.64793412\n",
      "Iteration 142, loss = 0.64750270\n",
      "Iteration 143, loss = 0.64707021\n",
      "Iteration 144, loss = 0.64663664\n",
      "Iteration 145, loss = 0.64620197\n",
      "Iteration 146, loss = 0.64576619\n",
      "Iteration 147, loss = 0.64532929\n",
      "Iteration 148, loss = 0.64489126\n",
      "Iteration 149, loss = 0.64445209\n",
      "Iteration 150, loss = 0.64401177\n",
      "Iteration 151, loss = 0.64357029\n",
      "Iteration 152, loss = 0.64312763\n",
      "Iteration 153, loss = 0.64268378\n",
      "Iteration 154, loss = 0.64223874\n",
      "Iteration 155, loss = 0.64179249\n",
      "Iteration 156, loss = 0.64134503\n",
      "Iteration 157, loss = 0.64089633\n",
      "Iteration 158, loss = 0.64044639\n",
      "Iteration 159, loss = 0.63999521\n",
      "Iteration 160, loss = 0.63954276\n",
      "Iteration 161, loss = 0.63908904\n",
      "Iteration 162, loss = 0.63863403\n",
      "Iteration 163, loss = 0.63817774\n",
      "Iteration 164, loss = 0.63772013\n",
      "Iteration 165, loss = 0.63726122\n",
      "Iteration 166, loss = 0.63680097\n",
      "Iteration 167, loss = 0.63633939\n",
      "Iteration 168, loss = 0.63587647\n",
      "Iteration 169, loss = 0.63541218\n",
      "Iteration 170, loss = 0.63494653\n",
      "Iteration 171, loss = 0.63447950\n",
      "Iteration 172, loss = 0.63401109\n",
      "Iteration 173, loss = 0.63354127\n",
      "Iteration 174, loss = 0.63307004\n",
      "Iteration 175, loss = 0.63259740\n",
      "Iteration 176, loss = 0.63212332\n",
      "Iteration 177, loss = 0.63164781\n",
      "Iteration 178, loss = 0.63117084\n",
      "Iteration 179, loss = 0.63069242\n",
      "Iteration 180, loss = 0.63021252\n",
      "Iteration 181, loss = 0.62973114\n",
      "Iteration 182, loss = 0.62924828\n",
      "Iteration 183, loss = 0.62876391\n",
      "Iteration 184, loss = 0.62827803\n",
      "Iteration 185, loss = 0.62779063\n",
      "Iteration 186, loss = 0.62730170\n",
      "Iteration 187, loss = 0.62681123\n",
      "Iteration 188, loss = 0.62631921\n",
      "Iteration 189, loss = 0.62582563\n",
      "Iteration 190, loss = 0.62533048\n",
      "Iteration 191, loss = 0.62483376\n",
      "Iteration 192, loss = 0.62433544\n",
      "Iteration 193, loss = 0.62383552\n",
      "Iteration 194, loss = 0.62333400\n",
      "Iteration 195, loss = 0.62283086\n",
      "Iteration 196, loss = 0.62232609\n",
      "Iteration 197, loss = 0.62181969\n",
      "Iteration 198, loss = 0.62131164\n",
      "Iteration 199, loss = 0.62080193\n",
      "Iteration 200, loss = 0.62029056\n",
      "Iteration 201, loss = 0.61977752\n",
      "Iteration 202, loss = 0.61926279\n",
      "Iteration 203, loss = 0.61874637\n",
      "Iteration 204, loss = 0.61822825\n",
      "Iteration 205, loss = 0.61770842\n",
      "Iteration 206, loss = 0.61718687\n",
      "Iteration 207, loss = 0.61666360\n",
      "Iteration 208, loss = 0.61613858\n",
      "Iteration 209, loss = 0.61561182\n",
      "Iteration 210, loss = 0.61508330\n",
      "Iteration 211, loss = 0.61455302\n",
      "Iteration 212, loss = 0.61402097\n",
      "Iteration 213, loss = 0.61348714\n",
      "Iteration 214, loss = 0.61295152\n",
      "Iteration 215, loss = 0.61241410\n",
      "Iteration 216, loss = 0.61187487\n",
      "Iteration 217, loss = 0.61133383\n",
      "Iteration 218, loss = 0.61079096\n",
      "Iteration 219, loss = 0.61024626\n",
      "Iteration 220, loss = 0.60969972\n",
      "Iteration 221, loss = 0.60915134\n",
      "Iteration 222, loss = 0.60860109\n",
      "Iteration 223, loss = 0.60804899\n",
      "Iteration 224, loss = 0.60749501\n",
      "Iteration 225, loss = 0.60693914\n",
      "Iteration 226, loss = 0.60638140\n",
      "Iteration 227, loss = 0.60582175\n",
      "Iteration 228, loss = 0.60526020\n",
      "Iteration 229, loss = 0.60469674\n",
      "Iteration 230, loss = 0.60413136\n",
      "Iteration 231, loss = 0.60356405\n",
      "Iteration 232, loss = 0.60299480\n",
      "Iteration 233, loss = 0.60242362\n",
      "Iteration 234, loss = 0.60185048\n",
      "Iteration 235, loss = 0.60127539\n",
      "Iteration 236, loss = 0.60069834\n",
      "Iteration 237, loss = 0.60011931\n",
      "Iteration 238, loss = 0.59953831\n",
      "Iteration 239, loss = 0.59895532\n",
      "Iteration 240, loss = 0.59837033\n",
      "Iteration 241, loss = 0.59778335\n",
      "Iteration 242, loss = 0.59719436\n",
      "Iteration 243, loss = 0.59660336\n",
      "Iteration 244, loss = 0.59601034\n",
      "Iteration 245, loss = 0.59541530\n",
      "Iteration 246, loss = 0.59481822\n",
      "Iteration 247, loss = 0.59421910\n",
      "Iteration 248, loss = 0.59361794\n",
      "Iteration 249, loss = 0.59301473\n",
      "Iteration 250, loss = 0.59240946\n",
      "Iteration 251, loss = 0.59180212\n",
      "Iteration 252, loss = 0.59119272\n",
      "Iteration 253, loss = 0.59058124\n",
      "Iteration 254, loss = 0.58996768\n",
      "Iteration 255, loss = 0.58935203\n",
      "Iteration 256, loss = 0.58873429\n",
      "Iteration 257, loss = 0.58811446\n",
      "Iteration 258, loss = 0.58749251\n",
      "Iteration 259, loss = 0.58686846\n",
      "Iteration 260, loss = 0.58624230\n",
      "Iteration 261, loss = 0.58561401\n",
      "Iteration 262, loss = 0.58498360\n",
      "Iteration 263, loss = 0.58435106\n",
      "Iteration 264, loss = 0.58371639\n",
      "Iteration 265, loss = 0.58307958\n",
      "Iteration 266, loss = 0.58244062\n",
      "Iteration 267, loss = 0.58179952\n",
      "Iteration 268, loss = 0.58115626\n",
      "Iteration 269, loss = 0.58051084\n",
      "Iteration 270, loss = 0.57986327\n",
      "Iteration 271, loss = 0.57921352\n",
      "Iteration 272, loss = 0.57856161\n",
      "Iteration 273, loss = 0.57790753\n",
      "Iteration 274, loss = 0.57725126\n",
      "Iteration 275, loss = 0.57659282\n",
      "Iteration 276, loss = 0.57593219\n",
      "Iteration 277, loss = 0.57526937\n",
      "Iteration 278, loss = 0.57460436\n",
      "Iteration 279, loss = 0.57393715\n",
      "Iteration 280, loss = 0.57326775\n",
      "Iteration 281, loss = 0.57259614\n",
      "Iteration 282, loss = 0.57192233\n",
      "Iteration 283, loss = 0.57124631\n",
      "Iteration 284, loss = 0.57056809\n",
      "Iteration 285, loss = 0.56988765\n",
      "Iteration 286, loss = 0.56920499\n",
      "Iteration 287, loss = 0.56852012\n",
      "Iteration 288, loss = 0.56783303\n",
      "Iteration 289, loss = 0.56714371\n",
      "Iteration 290, loss = 0.56645218\n",
      "Iteration 291, loss = 0.56575841\n",
      "Iteration 292, loss = 0.56506242\n",
      "Iteration 293, loss = 0.56436421\n",
      "Iteration 294, loss = 0.56366376\n",
      "Iteration 295, loss = 0.56296108\n",
      "Iteration 296, loss = 0.56225617\n",
      "Iteration 297, loss = 0.56154902\n",
      "Iteration 298, loss = 0.56083964\n",
      "Iteration 299, loss = 0.56012802\n",
      "Iteration 300, loss = 0.55941417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72532400\n",
      "Iteration 2, loss = 0.71021869\n",
      "Iteration 3, loss = 0.70429313\n",
      "Iteration 4, loss = 0.70176508\n",
      "Iteration 5, loss = 0.70166001\n",
      "Iteration 6, loss = 0.70146096\n",
      "Iteration 7, loss = 0.70127575\n",
      "Iteration 8, loss = 0.70107836\n",
      "Iteration 9, loss = 0.70086625\n",
      "Iteration 10, loss = 0.70064123\n",
      "Iteration 11, loss = 0.70040454\n",
      "Iteration 12, loss = 0.70015740\n",
      "Iteration 13, loss = 0.69990093\n",
      "Iteration 14, loss = 0.69963611\n",
      "Iteration 15, loss = 0.69936385\n",
      "Iteration 16, loss = 0.69908493\n",
      "Iteration 17, loss = 0.69880009\n",
      "Iteration 18, loss = 0.69850996\n",
      "Iteration 19, loss = 0.69821515\n",
      "Iteration 20, loss = 0.69791616\n",
      "Iteration 21, loss = 0.69761347\n",
      "Iteration 22, loss = 0.69730749\n",
      "Iteration 23, loss = 0.69699862\n",
      "Iteration 24, loss = 0.69668718\n",
      "Iteration 25, loss = 0.69637348\n",
      "Iteration 26, loss = 0.69605780\n",
      "Iteration 27, loss = 0.69574037\n",
      "Iteration 28, loss = 0.69542141\n",
      "Iteration 29, loss = 0.69510113\n",
      "Iteration 30, loss = 0.69477969\n",
      "Iteration 31, loss = 0.69445724\n",
      "Iteration 32, loss = 0.69413394\n",
      "Iteration 33, loss = 0.69380989\n",
      "Iteration 34, loss = 0.69348522\n",
      "Iteration 35, loss = 0.69316002\n",
      "Iteration 36, loss = 0.69283437\n",
      "Iteration 37, loss = 0.69250835\n",
      "Iteration 38, loss = 0.69218203\n",
      "Iteration 39, loss = 0.69185548\n",
      "Iteration 40, loss = 0.69152873\n",
      "Iteration 41, loss = 0.69120185\n",
      "Iteration 42, loss = 0.69087486\n",
      "Iteration 43, loss = 0.69054780\n",
      "Iteration 44, loss = 0.69022071\n",
      "Iteration 45, loss = 0.68989361\n",
      "Iteration 46, loss = 0.68956653\n",
      "Iteration 47, loss = 0.68923947\n",
      "Iteration 48, loss = 0.68891246\n",
      "Iteration 49, loss = 0.68858551\n",
      "Iteration 50, loss = 0.68825863\n",
      "Iteration 51, loss = 0.68793183\n",
      "Iteration 52, loss = 0.68760511\n",
      "Iteration 53, loss = 0.68727849\n",
      "Iteration 54, loss = 0.68695195\n",
      "Iteration 55, loss = 0.68662551\n",
      "Iteration 56, loss = 0.68629917\n",
      "Iteration 57, loss = 0.68597292\n",
      "Iteration 58, loss = 0.68564676\n",
      "Iteration 59, loss = 0.68532069\n",
      "Iteration 60, loss = 0.68499470\n",
      "Iteration 61, loss = 0.68466879\n",
      "Iteration 62, loss = 0.68434296\n",
      "Iteration 63, loss = 0.68401720\n",
      "Iteration 64, loss = 0.68369151\n",
      "Iteration 65, loss = 0.68336587\n",
      "Iteration 66, loss = 0.68304028\n",
      "Iteration 67, loss = 0.68271473\n",
      "Iteration 68, loss = 0.68238921\n",
      "Iteration 69, loss = 0.68206373\n",
      "Iteration 70, loss = 0.68173826\n",
      "Iteration 71, loss = 0.68141280\n",
      "Iteration 72, loss = 0.68108734\n",
      "Iteration 73, loss = 0.68076187\n",
      "Iteration 74, loss = 0.68043639\n",
      "Iteration 75, loss = 0.68011088\n",
      "Iteration 76, loss = 0.67978533\n",
      "Iteration 77, loss = 0.67945974\n",
      "Iteration 78, loss = 0.67913409\n",
      "Iteration 79, loss = 0.67880838\n",
      "Iteration 80, loss = 0.67848259\n",
      "Iteration 81, loss = 0.67815672\n",
      "Iteration 82, loss = 0.67783076\n",
      "Iteration 83, loss = 0.67750469\n",
      "Iteration 84, loss = 0.67717851\n",
      "Iteration 85, loss = 0.67685221\n",
      "Iteration 86, loss = 0.67652577\n",
      "Iteration 87, loss = 0.67619919\n",
      "Iteration 88, loss = 0.67587246\n",
      "Iteration 89, loss = 0.67554556\n",
      "Iteration 90, loss = 0.67521849\n",
      "Iteration 91, loss = 0.67489124\n",
      "Iteration 92, loss = 0.67456380\n",
      "Iteration 93, loss = 0.67423616\n",
      "Iteration 94, loss = 0.67390830\n",
      "Iteration 95, loss = 0.67358022\n",
      "Iteration 96, loss = 0.67325191\n",
      "Iteration 97, loss = 0.67292336\n",
      "Iteration 98, loss = 0.67259456\n",
      "Iteration 99, loss = 0.67226550\n",
      "Iteration 100, loss = 0.67193616\n",
      "Iteration 101, loss = 0.67160655\n",
      "Iteration 102, loss = 0.67127664\n",
      "Iteration 103, loss = 0.67094644\n",
      "Iteration 104, loss = 0.67061592\n",
      "Iteration 105, loss = 0.67028509\n",
      "Iteration 106, loss = 0.66995392\n",
      "Iteration 107, loss = 0.66962242\n",
      "Iteration 108, loss = 0.66929057\n",
      "Iteration 109, loss = 0.66895837\n",
      "Iteration 110, loss = 0.66862580\n",
      "Iteration 111, loss = 0.66829285\n",
      "Iteration 112, loss = 0.66795951\n",
      "Iteration 113, loss = 0.66762578\n",
      "Iteration 114, loss = 0.66729164\n",
      "Iteration 115, loss = 0.66695710\n",
      "Iteration 116, loss = 0.66662212\n",
      "Iteration 117, loss = 0.66628672\n",
      "Iteration 118, loss = 0.66595087\n",
      "Iteration 119, loss = 0.66561457\n",
      "Iteration 120, loss = 0.66527781\n",
      "Iteration 121, loss = 0.66494058\n",
      "Iteration 122, loss = 0.66460288\n",
      "Iteration 123, loss = 0.66426468\n",
      "Iteration 124, loss = 0.66392599\n",
      "Iteration 125, loss = 0.66358679\n",
      "Iteration 126, loss = 0.66324707\n",
      "Iteration 127, loss = 0.66290683\n",
      "Iteration 128, loss = 0.66256606\n",
      "Iteration 129, loss = 0.66222474\n",
      "Iteration 130, loss = 0.66188288\n",
      "Iteration 131, loss = 0.66154045\n",
      "Iteration 132, loss = 0.66119745\n",
      "Iteration 133, loss = 0.66085387\n",
      "Iteration 134, loss = 0.66050971\n",
      "Iteration 135, loss = 0.66016495\n",
      "Iteration 136, loss = 0.65981958\n",
      "Iteration 137, loss = 0.65947360\n",
      "Iteration 138, loss = 0.65912700\n",
      "Iteration 139, loss = 0.65877977\n",
      "Iteration 140, loss = 0.65843189\n",
      "Iteration 141, loss = 0.65808337\n",
      "Iteration 142, loss = 0.65773419\n",
      "Iteration 143, loss = 0.65738434\n",
      "Iteration 144, loss = 0.65703381\n",
      "Iteration 145, loss = 0.65668260\n",
      "Iteration 146, loss = 0.65633070\n",
      "Iteration 147, loss = 0.65597810\n",
      "Iteration 148, loss = 0.65562479\n",
      "Iteration 149, loss = 0.65527076\n",
      "Iteration 150, loss = 0.65491600\n",
      "Iteration 151, loss = 0.65456050\n",
      "Iteration 152, loss = 0.65420427\n",
      "Iteration 153, loss = 0.65384727\n",
      "Iteration 154, loss = 0.65348952\n",
      "Iteration 155, loss = 0.65313100\n",
      "Iteration 156, loss = 0.65277170\n",
      "Iteration 157, loss = 0.65241161\n",
      "Iteration 158, loss = 0.65205073\n",
      "Iteration 159, loss = 0.65168904\n",
      "Iteration 160, loss = 0.65132655\n",
      "Iteration 161, loss = 0.65096323\n",
      "Iteration 162, loss = 0.65059908\n",
      "Iteration 163, loss = 0.65023410\n",
      "Iteration 164, loss = 0.64986827\n",
      "Iteration 165, loss = 0.64950159\n",
      "Iteration 166, loss = 0.64913404\n",
      "Iteration 167, loss = 0.64876563\n",
      "Iteration 168, loss = 0.64839634\n",
      "Iteration 169, loss = 0.64802616\n",
      "Iteration 170, loss = 0.64765508\n",
      "Iteration 171, loss = 0.64728311\n",
      "Iteration 172, loss = 0.64691022\n",
      "Iteration 173, loss = 0.64653641\n",
      "Iteration 174, loss = 0.64616168\n",
      "Iteration 175, loss = 0.64578601\n",
      "Iteration 176, loss = 0.64540940\n",
      "Iteration 177, loss = 0.64503184\n",
      "Iteration 178, loss = 0.64465331\n",
      "Iteration 179, loss = 0.64427382\n",
      "Iteration 180, loss = 0.64389335\n",
      "Iteration 181, loss = 0.64351191\n",
      "Iteration 182, loss = 0.64312946\n",
      "Iteration 183, loss = 0.64274602\n",
      "Iteration 184, loss = 0.64236158\n",
      "Iteration 185, loss = 0.64197611\n",
      "Iteration 186, loss = 0.64158963\n",
      "Iteration 187, loss = 0.64120211\n",
      "Iteration 188, loss = 0.64081355\n",
      "Iteration 189, loss = 0.64042395\n",
      "Iteration 190, loss = 0.64003329\n",
      "Iteration 191, loss = 0.63964157\n",
      "Iteration 192, loss = 0.63924878\n",
      "Iteration 193, loss = 0.63885492\n",
      "Iteration 194, loss = 0.63845996\n",
      "Iteration 195, loss = 0.63806392\n",
      "Iteration 196, loss = 0.63766677\n",
      "Iteration 197, loss = 0.63726852\n",
      "Iteration 198, loss = 0.63686914\n",
      "Iteration 199, loss = 0.63646865\n",
      "Iteration 200, loss = 0.63606702\n",
      "Iteration 201, loss = 0.63566426\n",
      "Iteration 202, loss = 0.63526035\n",
      "Iteration 203, loss = 0.63485528\n",
      "Iteration 204, loss = 0.63444905\n",
      "Iteration 205, loss = 0.63404166\n",
      "Iteration 206, loss = 0.63363308\n",
      "Iteration 207, loss = 0.63322333\n",
      "Iteration 208, loss = 0.63281238\n",
      "Iteration 209, loss = 0.63240023\n",
      "Iteration 210, loss = 0.63198688\n",
      "Iteration 211, loss = 0.63157231\n",
      "Iteration 212, loss = 0.63115652\n",
      "Iteration 213, loss = 0.63073951\n",
      "Iteration 214, loss = 0.63032125\n",
      "Iteration 215, loss = 0.62990176\n",
      "Iteration 216, loss = 0.62948102\n",
      "Iteration 217, loss = 0.62905902\n",
      "Iteration 218, loss = 0.62863575\n",
      "Iteration 219, loss = 0.62821121\n",
      "Iteration 220, loss = 0.62778540\n",
      "Iteration 221, loss = 0.62735830\n",
      "Iteration 222, loss = 0.62692990\n",
      "Iteration 223, loss = 0.62650021\n",
      "Iteration 224, loss = 0.62606921\n",
      "Iteration 225, loss = 0.62563689\n",
      "Iteration 226, loss = 0.62520326\n",
      "Iteration 227, loss = 0.62476830\n",
      "Iteration 228, loss = 0.62433200\n",
      "Iteration 229, loss = 0.62389436\n",
      "Iteration 230, loss = 0.62345537\n",
      "Iteration 231, loss = 0.62301503\n",
      "Iteration 232, loss = 0.62257332\n",
      "Iteration 233, loss = 0.62213025\n",
      "Iteration 234, loss = 0.62168580\n",
      "Iteration 235, loss = 0.62123997\n",
      "Iteration 236, loss = 0.62079275\n",
      "Iteration 237, loss = 0.62034413\n",
      "Iteration 238, loss = 0.61989412\n",
      "Iteration 239, loss = 0.61944269\n",
      "Iteration 240, loss = 0.61898985\n",
      "Iteration 241, loss = 0.61853558\n",
      "Iteration 242, loss = 0.61807989\n",
      "Iteration 243, loss = 0.61762276\n",
      "Iteration 244, loss = 0.61716419\n",
      "Iteration 245, loss = 0.61670418\n",
      "Iteration 246, loss = 0.61624271\n",
      "Iteration 247, loss = 0.61577977\n",
      "Iteration 248, loss = 0.61531538\n",
      "Iteration 249, loss = 0.61484951\n",
      "Iteration 250, loss = 0.61438216\n",
      "Iteration 251, loss = 0.61391332\n",
      "Iteration 252, loss = 0.61344300\n",
      "Iteration 253, loss = 0.61297118\n",
      "Iteration 254, loss = 0.61249785\n",
      "Iteration 255, loss = 0.61202301\n",
      "Iteration 256, loss = 0.61154666\n",
      "Iteration 257, loss = 0.61106879\n",
      "Iteration 258, loss = 0.61058939\n",
      "Iteration 259, loss = 0.61010845\n",
      "Iteration 260, loss = 0.60962598\n",
      "Iteration 261, loss = 0.60914197\n",
      "Iteration 262, loss = 0.60865640\n",
      "Iteration 263, loss = 0.60816927\n",
      "Iteration 264, loss = 0.60768059\n",
      "Iteration 265, loss = 0.60719033\n",
      "Iteration 266, loss = 0.60669851\n",
      "Iteration 267, loss = 0.60620510\n",
      "Iteration 268, loss = 0.60571011\n",
      "Iteration 269, loss = 0.60521354\n",
      "Iteration 270, loss = 0.60471536\n",
      "Iteration 271, loss = 0.60421559\n",
      "Iteration 272, loss = 0.60371421\n",
      "Iteration 273, loss = 0.60321122\n",
      "Iteration 274, loss = 0.60270661\n",
      "Iteration 275, loss = 0.60220039\n",
      "Iteration 276, loss = 0.60169254\n",
      "Iteration 277, loss = 0.60118305\n",
      "Iteration 278, loss = 0.60067193\n",
      "Iteration 279, loss = 0.60015917\n",
      "Iteration 280, loss = 0.59964476\n",
      "Iteration 281, loss = 0.59912870\n",
      "Iteration 282, loss = 0.59861099\n",
      "Iteration 283, loss = 0.59809162\n",
      "Iteration 284, loss = 0.59757058\n",
      "Iteration 285, loss = 0.59704787\n",
      "Iteration 286, loss = 0.59652349\n",
      "Iteration 287, loss = 0.59599742\n",
      "Iteration 288, loss = 0.59546968\n",
      "Iteration 289, loss = 0.59494025\n",
      "Iteration 290, loss = 0.59440912\n",
      "Iteration 291, loss = 0.59387630\n",
      "Iteration 292, loss = 0.59334178\n",
      "Iteration 293, loss = 0.59280555\n",
      "Iteration 294, loss = 0.59226762\n",
      "Iteration 295, loss = 0.59172797\n",
      "Iteration 296, loss = 0.59118661\n",
      "Iteration 297, loss = 0.59064352\n",
      "Iteration 298, loss = 0.59009871\n",
      "Iteration 299, loss = 0.58955217\n",
      "Iteration 300, loss = 0.58900390\n",
      "Iteration 1, loss = 0.72021058\n",
      "Iteration 2, loss = 0.70585284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.70022030\n",
      "Iteration 4, loss = 0.69782701\n",
      "Iteration 5, loss = 0.69771899\n",
      "Iteration 6, loss = 0.69752158\n",
      "Iteration 7, loss = 0.69733572\n",
      "Iteration 8, loss = 0.69713717\n",
      "Iteration 9, loss = 0.69692369\n",
      "Iteration 10, loss = 0.69669708\n",
      "Iteration 11, loss = 0.69645859\n",
      "Iteration 12, loss = 0.69620945\n",
      "Iteration 13, loss = 0.69595077\n",
      "Iteration 14, loss = 0.69568356\n",
      "Iteration 15, loss = 0.69540869\n",
      "Iteration 16, loss = 0.69512698\n",
      "Iteration 17, loss = 0.69483915\n",
      "Iteration 18, loss = 0.69454585\n",
      "Iteration 19, loss = 0.69424766\n",
      "Iteration 20, loss = 0.69394512\n",
      "Iteration 21, loss = 0.69363868\n",
      "Iteration 22, loss = 0.69332878\n",
      "Iteration 23, loss = 0.69301579\n",
      "Iteration 24, loss = 0.69270005\n",
      "Iteration 25, loss = 0.69238187\n",
      "Iteration 26, loss = 0.69206151\n",
      "Iteration 27, loss = 0.69173923\n",
      "Iteration 28, loss = 0.69141524\n",
      "Iteration 29, loss = 0.69108973\n",
      "Iteration 30, loss = 0.69076289\n",
      "Iteration 31, loss = 0.69043487\n",
      "Iteration 32, loss = 0.69010580\n",
      "Iteration 33, loss = 0.68977581\n",
      "Iteration 34, loss = 0.68944502\n",
      "Iteration 35, loss = 0.68911351\n",
      "Iteration 36, loss = 0.68878138\n",
      "Iteration 37, loss = 0.68844871\n",
      "Iteration 38, loss = 0.68811555\n",
      "Iteration 39, loss = 0.68778198\n",
      "Iteration 40, loss = 0.68744804\n",
      "Iteration 41, loss = 0.68711378\n",
      "Iteration 42, loss = 0.68677925\n",
      "Iteration 43, loss = 0.68644447\n",
      "Iteration 44, loss = 0.68610948\n",
      "Iteration 45, loss = 0.68577430\n",
      "Iteration 46, loss = 0.68543896\n",
      "Iteration 47, loss = 0.68510347\n",
      "Iteration 48, loss = 0.68476785\n",
      "Iteration 49, loss = 0.68443212\n",
      "Iteration 50, loss = 0.68409628\n",
      "Iteration 51, loss = 0.68376035\n",
      "Iteration 52, loss = 0.68342433\n",
      "Iteration 53, loss = 0.68308822\n",
      "Iteration 54, loss = 0.68275203\n",
      "Iteration 55, loss = 0.68241576\n",
      "Iteration 56, loss = 0.68207941\n",
      "Iteration 57, loss = 0.68174297\n",
      "Iteration 58, loss = 0.68140646\n",
      "Iteration 59, loss = 0.68106986\n",
      "Iteration 60, loss = 0.68073317\n",
      "Iteration 61, loss = 0.68039639\n",
      "Iteration 62, loss = 0.68005950\n",
      "Iteration 63, loss = 0.67972252\n",
      "Iteration 64, loss = 0.67938542\n",
      "Iteration 65, loss = 0.67904821\n",
      "Iteration 66, loss = 0.67871087\n",
      "Iteration 67, loss = 0.67837340\n",
      "Iteration 68, loss = 0.67803579\n",
      "Iteration 69, loss = 0.67769803\n",
      "Iteration 70, loss = 0.67736012\n",
      "Iteration 71, loss = 0.67702204\n",
      "Iteration 72, loss = 0.67668379\n",
      "Iteration 73, loss = 0.67634536\n",
      "Iteration 74, loss = 0.67600673\n",
      "Iteration 75, loss = 0.67566791\n",
      "Iteration 76, loss = 0.67532887\n",
      "Iteration 77, loss = 0.67498962\n",
      "Iteration 78, loss = 0.67465013\n",
      "Iteration 79, loss = 0.67431041\n",
      "Iteration 80, loss = 0.67397044\n",
      "Iteration 81, loss = 0.67363021\n",
      "Iteration 82, loss = 0.67328971\n",
      "Iteration 83, loss = 0.67294893\n",
      "Iteration 84, loss = 0.67260786\n",
      "Iteration 85, loss = 0.67226650\n",
      "Iteration 86, loss = 0.67192482\n",
      "Iteration 87, loss = 0.67158283\n",
      "Iteration 88, loss = 0.67124051\n",
      "Iteration 89, loss = 0.67089785\n",
      "Iteration 90, loss = 0.67055484\n",
      "Iteration 91, loss = 0.67021148\n",
      "Iteration 92, loss = 0.66986774\n",
      "Iteration 93, loss = 0.66952363\n",
      "Iteration 94, loss = 0.66917912\n",
      "Iteration 95, loss = 0.66883422\n",
      "Iteration 96, loss = 0.66848891\n",
      "Iteration 97, loss = 0.66814318\n",
      "Iteration 98, loss = 0.66779702\n",
      "Iteration 99, loss = 0.66745043\n",
      "Iteration 100, loss = 0.66710338\n",
      "Iteration 101, loss = 0.66675587\n",
      "Iteration 102, loss = 0.66640790\n",
      "Iteration 103, loss = 0.66605945\n",
      "Iteration 104, loss = 0.66571051\n",
      "Iteration 105, loss = 0.66536107\n",
      "Iteration 106, loss = 0.66501112\n",
      "Iteration 107, loss = 0.66466065\n",
      "Iteration 108, loss = 0.66430966\n",
      "Iteration 109, loss = 0.66395813\n",
      "Iteration 110, loss = 0.66360605\n",
      "Iteration 111, loss = 0.66325341\n",
      "Iteration 112, loss = 0.66290020\n",
      "Iteration 113, loss = 0.66254642\n",
      "Iteration 114, loss = 0.66219205\n",
      "Iteration 115, loss = 0.66183709\n",
      "Iteration 116, loss = 0.66148152\n",
      "Iteration 117, loss = 0.66112533\n",
      "Iteration 118, loss = 0.66076852\n",
      "Iteration 119, loss = 0.66041108\n",
      "Iteration 120, loss = 0.66005299\n",
      "Iteration 121, loss = 0.65969424\n",
      "Iteration 122, loss = 0.65933484\n",
      "Iteration 123, loss = 0.65897476\n",
      "Iteration 124, loss = 0.65861399\n",
      "Iteration 125, loss = 0.65825254\n",
      "Iteration 126, loss = 0.65789038\n",
      "Iteration 127, loss = 0.65752752\n",
      "Iteration 128, loss = 0.65716393\n",
      "Iteration 129, loss = 0.65679961\n",
      "Iteration 130, loss = 0.65643456\n",
      "Iteration 131, loss = 0.65606875\n",
      "Iteration 132, loss = 0.65570219\n",
      "Iteration 133, loss = 0.65533486\n",
      "Iteration 134, loss = 0.65496676\n",
      "Iteration 135, loss = 0.65459787\n",
      "Iteration 136, loss = 0.65422818\n",
      "Iteration 137, loss = 0.65385769\n",
      "Iteration 138, loss = 0.65348639\n",
      "Iteration 139, loss = 0.65311427\n",
      "Iteration 140, loss = 0.65274131\n",
      "Iteration 141, loss = 0.65236751\n",
      "Iteration 142, loss = 0.65199286\n",
      "Iteration 143, loss = 0.65161736\n",
      "Iteration 144, loss = 0.65124098\n",
      "Iteration 145, loss = 0.65086373\n",
      "Iteration 146, loss = 0.65048559\n",
      "Iteration 147, loss = 0.65010656\n",
      "Iteration 148, loss = 0.64972662\n",
      "Iteration 149, loss = 0.64934577\n",
      "Iteration 150, loss = 0.64896400\n",
      "Iteration 151, loss = 0.64858129\n",
      "Iteration 152, loss = 0.64819765\n",
      "Iteration 153, loss = 0.64781305\n",
      "Iteration 154, loss = 0.64742750\n",
      "Iteration 155, loss = 0.64704098\n",
      "Iteration 156, loss = 0.64665349\n",
      "Iteration 157, loss = 0.64626501\n",
      "Iteration 158, loss = 0.64587553\n",
      "Iteration 159, loss = 0.64548506\n",
      "Iteration 160, loss = 0.64509357\n",
      "Iteration 161, loss = 0.64470106\n",
      "Iteration 162, loss = 0.64430752\n",
      "Iteration 163, loss = 0.64391295\n",
      "Iteration 164, loss = 0.64351733\n",
      "Iteration 165, loss = 0.64312065\n",
      "Iteration 166, loss = 0.64272291\n",
      "Iteration 167, loss = 0.64232410\n",
      "Iteration 168, loss = 0.64192421\n",
      "Iteration 169, loss = 0.64152323\n",
      "Iteration 170, loss = 0.64112115\n",
      "Iteration 171, loss = 0.64071796\n",
      "Iteration 172, loss = 0.64031366\n",
      "Iteration 173, loss = 0.63990823\n",
      "Iteration 174, loss = 0.63950167\n",
      "Iteration 175, loss = 0.63909397\n",
      "Iteration 176, loss = 0.63868512\n",
      "Iteration 177, loss = 0.63827511\n",
      "Iteration 178, loss = 0.63786393\n",
      "Iteration 179, loss = 0.63745158\n",
      "Iteration 180, loss = 0.63703805\n",
      "Iteration 181, loss = 0.63662332\n",
      "Iteration 182, loss = 0.63620740\n",
      "Iteration 183, loss = 0.63579026\n",
      "Iteration 184, loss = 0.63537191\n",
      "Iteration 185, loss = 0.63495234\n",
      "Iteration 186, loss = 0.63453153\n",
      "Iteration 187, loss = 0.63410948\n",
      "Iteration 188, loss = 0.63368617\n",
      "Iteration 189, loss = 0.63326161\n",
      "Iteration 190, loss = 0.63283579\n",
      "Iteration 191, loss = 0.63240869\n",
      "Iteration 192, loss = 0.63198030\n",
      "Iteration 193, loss = 0.63155063\n",
      "Iteration 194, loss = 0.63111965\n",
      "Iteration 195, loss = 0.63068737\n",
      "Iteration 196, loss = 0.63025378\n",
      "Iteration 197, loss = 0.62981886\n",
      "Iteration 198, loss = 0.62938260\n",
      "Iteration 199, loss = 0.62894501\n",
      "Iteration 200, loss = 0.62850608\n",
      "Iteration 201, loss = 0.62806578\n",
      "Iteration 202, loss = 0.62762413\n",
      "Iteration 203, loss = 0.62718110\n",
      "Iteration 204, loss = 0.62673669\n",
      "Iteration 205, loss = 0.62629090\n",
      "Iteration 206, loss = 0.62584371\n",
      "Iteration 207, loss = 0.62539512\n",
      "Iteration 208, loss = 0.62494512\n",
      "Iteration 209, loss = 0.62449370\n",
      "Iteration 210, loss = 0.62404086\n",
      "Iteration 211, loss = 0.62358658\n",
      "Iteration 212, loss = 0.62313087\n",
      "Iteration 213, loss = 0.62267370\n",
      "Iteration 214, loss = 0.62221508\n",
      "Iteration 215, loss = 0.62175499\n",
      "Iteration 216, loss = 0.62129343\n",
      "Iteration 217, loss = 0.62083040\n",
      "Iteration 218, loss = 0.62036587\n",
      "Iteration 219, loss = 0.61989986\n",
      "Iteration 220, loss = 0.61943234\n",
      "Iteration 221, loss = 0.61896332\n",
      "Iteration 222, loss = 0.61849277\n",
      "Iteration 223, loss = 0.61802071\n",
      "Iteration 224, loss = 0.61754711\n",
      "Iteration 225, loss = 0.61707198\n",
      "Iteration 226, loss = 0.61659530\n",
      "Iteration 227, loss = 0.61611707\n",
      "Iteration 228, loss = 0.61563729\n",
      "Iteration 229, loss = 0.61515593\n",
      "Iteration 230, loss = 0.61467300\n",
      "Iteration 231, loss = 0.61418849\n",
      "Iteration 232, loss = 0.61370240\n",
      "Iteration 233, loss = 0.61321471\n",
      "Iteration 234, loss = 0.61272542\n",
      "Iteration 235, loss = 0.61223452\n",
      "Iteration 236, loss = 0.61174200\n",
      "Iteration 237, loss = 0.61124786\n",
      "Iteration 238, loss = 0.61075210\n",
      "Iteration 239, loss = 0.61025470\n",
      "Iteration 240, loss = 0.60975566\n",
      "Iteration 241, loss = 0.60925497\n",
      "Iteration 242, loss = 0.60875262\n",
      "Iteration 243, loss = 0.60824861\n",
      "Iteration 244, loss = 0.60774294\n",
      "Iteration 245, loss = 0.60723559\n",
      "Iteration 246, loss = 0.60672656\n",
      "Iteration 247, loss = 0.60621584\n",
      "Iteration 248, loss = 0.60570343\n",
      "Iteration 249, loss = 0.60518932\n",
      "Iteration 250, loss = 0.60467350\n",
      "Iteration 251, loss = 0.60415597\n",
      "Iteration 252, loss = 0.60363672\n",
      "Iteration 253, loss = 0.60311574\n",
      "Iteration 254, loss = 0.60259304\n",
      "Iteration 255, loss = 0.60206860\n",
      "Iteration 256, loss = 0.60154241\n",
      "Iteration 257, loss = 0.60101448\n",
      "Iteration 258, loss = 0.60048479\n",
      "Iteration 259, loss = 0.59995334\n",
      "Iteration 260, loss = 0.59942013\n",
      "Iteration 261, loss = 0.59888515\n",
      "Iteration 262, loss = 0.59834839\n",
      "Iteration 263, loss = 0.59780984\n",
      "Iteration 264, loss = 0.59726951\n",
      "Iteration 265, loss = 0.59672738\n",
      "Iteration 266, loss = 0.59618346\n",
      "Iteration 267, loss = 0.59563773\n",
      "Iteration 268, loss = 0.59509019\n",
      "Iteration 269, loss = 0.59454084\n",
      "Iteration 270, loss = 0.59398966\n",
      "Iteration 271, loss = 0.59343666\n",
      "Iteration 272, loss = 0.59288183\n",
      "Iteration 273, loss = 0.59232517\n",
      "Iteration 274, loss = 0.59176667\n",
      "Iteration 275, loss = 0.59120632\n",
      "Iteration 276, loss = 0.59064412\n",
      "Iteration 277, loss = 0.59008007\n",
      "Iteration 278, loss = 0.58951416\n",
      "Iteration 279, loss = 0.58894638\n",
      "Iteration 280, loss = 0.58837674\n",
      "Iteration 281, loss = 0.58780522\n",
      "Iteration 282, loss = 0.58723183\n",
      "Iteration 283, loss = 0.58665656\n",
      "Iteration 284, loss = 0.58607940\n",
      "Iteration 285, loss = 0.58550035\n",
      "Iteration 286, loss = 0.58491941\n",
      "Iteration 287, loss = 0.58433657\n",
      "Iteration 288, loss = 0.58375183\n",
      "Iteration 289, loss = 0.58316519\n",
      "Iteration 290, loss = 0.58257663\n",
      "Iteration 291, loss = 0.58198617\n",
      "Iteration 292, loss = 0.58139378\n",
      "Iteration 293, loss = 0.58079948\n",
      "Iteration 294, loss = 0.58020325\n",
      "Iteration 295, loss = 0.57960510\n",
      "Iteration 296, loss = 0.57900501\n",
      "Iteration 297, loss = 0.57840300\n",
      "Iteration 298, loss = 0.57779904\n",
      "Iteration 299, loss = 0.57719315\n",
      "Iteration 300, loss = 0.57658531\n",
      "Iteration 1, loss = 0.72125764\n",
      "Iteration 2, loss = 6.12884308\n",
      "Iteration 3, loss = 2.34841684\n",
      "Iteration 4, loss = 2.50849460\n",
      "Iteration 5, loss = 3.68536723\n",
      "Iteration 6, loss = 2.89389375\n",
      "Iteration 7, loss = 1.04936206\n",
      "Iteration 8, loss = 1.71316219\n",
      "Iteration 9, loss = 2.67603753\n",
      "Iteration 10, loss = 2.43803460\n",
      "Iteration 11, loss = 1.34449879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.81847281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72532400\n",
      "Iteration 2, loss = 6.05467456\n",
      "Iteration 3, loss = 2.38678022\n",
      "Iteration 4, loss = 2.43803129\n",
      "Iteration 5, loss = 3.55572142\n",
      "Iteration 6, loss = 2.68019959\n",
      "Iteration 7, loss = 0.86237063\n",
      "Iteration 8, loss = 1.86790221\n",
      "Iteration 9, loss = 2.69015880\n",
      "Iteration 10, loss = 2.34043850\n",
      "Iteration 11, loss = 1.19368476\n",
      "Iteration 12, loss = 0.94213748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72021058\n",
      "Iteration 2, loss = 6.08978475\n",
      "Iteration 3, loss = 2.36438808\n",
      "Iteration 4, loss = 2.47116891\n",
      "Iteration 5, loss = 3.61956890\n",
      "Iteration 6, loss = 2.78758297\n",
      "Iteration 7, loss = 0.94881553\n",
      "Iteration 8, loss = 1.79694503\n",
      "Iteration 9, loss = 2.69920649\n",
      "Iteration 10, loss = 2.41232758\n",
      "Iteration 11, loss = 1.29160615\n",
      "Iteration 12, loss = 0.85752016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08724508\n",
      "Iteration 2, loss = 2.88977434\n",
      "Iteration 3, loss = 0.88501968\n",
      "Iteration 4, loss = 1.57273403\n",
      "Iteration 5, loss = 2.49999045\n",
      "Iteration 6, loss = 1.09212130\n",
      "Iteration 7, loss = 1.91658865\n",
      "Iteration 8, loss = 1.86022647\n",
      "Iteration 9, loss = 1.52465719\n",
      "Iteration 10, loss = 1.83547405\n",
      "Iteration 11, loss = 1.52717655\n",
      "Iteration 12, loss = 1.57858259\n",
      "Iteration 13, loss = 1.57544487\n",
      "Iteration 14, loss = 1.40348903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09735780\n",
      "Iteration 2, loss = 2.91425045\n",
      "Iteration 3, loss = 0.81422927\n",
      "Iteration 4, loss = 1.30161285\n",
      "Iteration 5, loss = 2.59121661\n",
      "Iteration 6, loss = 1.11084615\n",
      "Iteration 7, loss = 1.88532879\n",
      "Iteration 8, loss = 1.96918545\n",
      "Iteration 9, loss = 1.38768153\n",
      "Iteration 10, loss = 1.91891981\n",
      "Iteration 11, loss = 1.46138379\n",
      "Iteration 12, loss = 1.62851882\n",
      "Iteration 13, loss = 1.55554677\n",
      "Iteration 14, loss = 1.44679496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09724985\n",
      "Iteration 2, loss = 2.90915154\n",
      "Iteration 3, loss = 0.84917405\n",
      "Iteration 4, loss = 1.43876454\n",
      "Iteration 5, loss = 2.57047944\n",
      "Iteration 6, loss = 1.08150032\n",
      "Iteration 7, loss = 1.87833435\n",
      "Iteration 8, loss = 1.95119661\n",
      "Iteration 9, loss = 1.43188868\n",
      "Iteration 10, loss = 1.88987110\n",
      "Iteration 11, loss = 1.50013409\n",
      "Iteration 12, loss = 1.59970849\n",
      "Iteration 13, loss = 1.57743919\n",
      "Iteration 14, loss = 1.42209518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08724508\n",
      "Iteration 2, loss = 11.29563714\n",
      "Iteration 3, loss = 8.80013411\n",
      "Iteration 4, loss = 2.44383258\n",
      "Iteration 5, loss = 5.57135912\n",
      "Iteration 6, loss = 8.48374583\n",
      "Iteration 7, loss = 8.13267563\n",
      "Iteration 8, loss = 5.54079541\n",
      "Iteration 9, loss = 1.40414973\n",
      "Iteration 10, loss = 3.90386753\n",
      "Iteration 11, loss = 6.47435414\n",
      "Iteration 12, loss = 6.91114417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09735780\n",
      "Iteration 2, loss = 11.15468772\n",
      "Iteration 3, loss = 8.85094914\n",
      "Iteration 4, loss = 2.69158303\n",
      "Iteration 5, loss = 5.27245000\n",
      "Iteration 6, loss = 8.08996599\n",
      "Iteration 7, loss = 7.59478074\n",
      "Iteration 8, loss = 4.83577520\n",
      "Iteration 9, loss = 0.79126353\n",
      "Iteration 10, loss = 4.15810597\n",
      "Iteration 11, loss = 6.17075076\n",
      "Iteration 12, loss = 6.13004255\n",
      "Iteration 13, loss = 4.45171592\n",
      "Iteration 14, loss = 1.50336087\n",
      "Iteration 15, loss = 2.52375547\n",
      "Iteration 16, loss = 4.51985343\n",
      "Iteration 17, loss = 4.81721991\n",
      "Iteration 18, loss = 3.68138508\n",
      "Iteration 19, loss = 1.40225590\n",
      "Iteration 20, loss = 1.85471490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09724985\n",
      "Iteration 2, loss = 11.22870269\n",
      "Iteration 3, loss = 8.83553466\n",
      "Iteration 4, loss = 2.58101607\n",
      "Iteration 5, loss = 5.41386675\n",
      "Iteration 6, loss = 8.27969236\n",
      "Iteration 7, loss = 7.85966491\n",
      "Iteration 8, loss = 5.18834874\n",
      "Iteration 9, loss = 1.05382597\n",
      "Iteration 10, loss = 4.15674036\n",
      "Iteration 11, loss = 6.55194713\n",
      "Iteration 12, loss = 6.84101999\n",
      "Iteration 13, loss = 5.44670709\n",
      "Iteration 14, loss = 2.69657226\n",
      "Iteration 15, loss = 1.24123261\n",
      "Iteration 16, loss = 3.33568888\n",
      "Iteration 17, loss = 3.83109460\n",
      "Iteration 18, loss = 2.90148180\n",
      "Iteration 19, loss = 0.95696015\n",
      "Iteration 20, loss = 1.98216712\n",
      "Iteration 21, loss = 3.15287714\n",
      "Iteration 22, loss = 2.95217438\n",
      "Iteration 23, loss = 1.59315023\n",
      "Iteration 24, loss = 0.96327752\n",
      "Iteration 25, loss = 2.10394912\n",
      "Iteration 26, loss = 2.11289320\n",
      "Iteration 27, loss = 1.09714325\n",
      "Iteration 28, loss = 1.04798866\n",
      "Iteration 29, loss = 1.78343784\n",
      "Iteration 30, loss = 1.48776571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11512437\n",
      "Iteration 2, loss = 1.92174809\n",
      "Iteration 3, loss = 0.83045214\n",
      "Iteration 4, loss = 1.02070006\n",
      "Iteration 5, loss = 1.41718479\n",
      "Iteration 6, loss = 1.19049068\n",
      "Iteration 7, loss = 1.22569309\n",
      "Iteration 8, loss = 1.22131005\n",
      "Iteration 9, loss = 1.16592967\n",
      "Iteration 10, loss = 1.13012811\n",
      "Iteration 11, loss = 1.11216888\n",
      "Iteration 12, loss = 1.06259824\n",
      "Iteration 13, loss = 1.03793190\n",
      "Iteration 14, loss = 0.99708982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12736457\n",
      "Iteration 2, loss = 1.94365653\n",
      "Iteration 3, loss = 0.78979086\n",
      "Iteration 4, loss = 0.93244927\n",
      "Iteration 5, loss = 1.34805036\n",
      "Iteration 6, loss = 1.26839912\n",
      "Iteration 7, loss = 1.20372941\n",
      "Iteration 8, loss = 1.22916245\n",
      "Iteration 9, loss = 1.16584087\n",
      "Iteration 10, loss = 1.14494470\n",
      "Iteration 11, loss = 1.10722880\n",
      "Iteration 12, loss = 1.07824328\n",
      "Iteration 13, loss = 1.03714504\n",
      "Iteration 14, loss = 1.00915090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12511159\n",
      "Iteration 2, loss = 1.93547488\n",
      "Iteration 3, loss = 0.80881538\n",
      "Iteration 4, loss = 0.97527286\n",
      "Iteration 5, loss = 1.38718756\n",
      "Iteration 6, loss = 1.22882935\n",
      "Iteration 7, loss = 1.21735743\n",
      "Iteration 8, loss = 1.22575949\n",
      "Iteration 9, loss = 1.16589839\n",
      "Iteration 10, loss = 1.13776722\n",
      "Iteration 11, loss = 1.11031004\n",
      "Iteration 12, loss = 1.07037687\n",
      "Iteration 13, loss = 1.03797358\n",
      "Iteration 14, loss = 1.00310634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11512437\n",
      "Iteration 2, loss = 7.77184203\n",
      "Iteration 3, loss = 6.03794942\n",
      "Iteration 4, loss = 1.58828335\n",
      "Iteration 5, loss = 4.01989835\n",
      "Iteration 6, loss = 5.95349126\n",
      "Iteration 7, loss = 5.58756063\n",
      "Iteration 8, loss = 3.66176483\n",
      "Iteration 9, loss = 0.87595097\n",
      "Iteration 10, loss = 2.77563937\n",
      "Iteration 11, loss = 4.30617806\n",
      "Iteration 12, loss = 4.35745586\n",
      "Iteration 13, loss = 3.23854174\n",
      "Iteration 14, loss = 1.27427473\n",
      "Iteration 15, loss = 1.49345218\n",
      "Iteration 16, loss = 2.83089859\n",
      "Iteration 17, loss = 3.07242089\n",
      "Iteration 18, loss = 2.35969404\n",
      "Iteration 19, loss = 1.00074575\n",
      "Iteration 20, loss = 1.22460670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12736457\n",
      "Iteration 2, loss = 7.67369580\n",
      "Iteration 3, loss = 6.08114239\n",
      "Iteration 4, loss = 1.76249180\n",
      "Iteration 5, loss = 3.82236374\n",
      "Iteration 6, loss = 5.72119839\n",
      "Iteration 7, loss = 5.28787233\n",
      "Iteration 8, loss = 3.27846526\n",
      "Iteration 9, loss = 0.70115987\n",
      "Iteration 10, loss = 2.76182229\n",
      "Iteration 11, loss = 3.89532081\n",
      "Iteration 12, loss = 3.63118648\n",
      "Iteration 13, loss = 2.27048004\n",
      "Iteration 14, loss = 0.68619461\n",
      "Iteration 15, loss = 2.02793827\n",
      "Iteration 16, loss = 2.75720763\n",
      "Iteration 17, loss = 2.38987492\n",
      "Iteration 18, loss = 1.19561998\n",
      "Iteration 19, loss = 0.97246091\n",
      "Iteration 20, loss = 1.86974006\n",
      "Iteration 21, loss = 1.93273094\n",
      "Iteration 22, loss = 1.22114996\n",
      "Iteration 23, loss = 0.72165453\n",
      "Iteration 24, loss = 1.37474772\n",
      "Iteration 25, loss = 1.51832102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12511159\n",
      "Iteration 2, loss = 7.72081058\n",
      "Iteration 3, loss = 6.06508299\n",
      "Iteration 4, loss = 1.68432715\n",
      "Iteration 5, loss = 3.91185237\n",
      "Iteration 6, loss = 5.83064970\n",
      "Iteration 7, loss = 5.43560593\n",
      "Iteration 8, loss = 3.47328469\n",
      "Iteration 9, loss = 0.76868330\n",
      "Iteration 10, loss = 2.79813901\n",
      "Iteration 11, loss = 4.15607672\n",
      "Iteration 12, loss = 4.06964497\n",
      "Iteration 13, loss = 2.84292596\n",
      "Iteration 14, loss = 0.92006498\n",
      "Iteration 15, loss = 1.83716205\n",
      "Iteration 16, loss = 3.02561633\n",
      "Iteration 17, loss = 3.08799959\n",
      "Iteration 18, loss = 2.19926181\n",
      "Iteration 19, loss = 0.80765852\n",
      "Iteration 20, loss = 1.47974966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72125764\n",
      "Iteration 2, loss = 3.06894024\n",
      "Iteration 3, loss = 9.83878201\n",
      "Iteration 4, loss = 0.84567507\n",
      "Iteration 5, loss = 1.18646264\n",
      "Iteration 6, loss = 0.72139753\n",
      "Iteration 7, loss = 0.95256169\n",
      "Iteration 8, loss = 1.77292190\n",
      "Iteration 9, loss = 3.56738358\n",
      "Iteration 10, loss = 1.02148830\n",
      "Iteration 11, loss = 1.03859816\n",
      "Iteration 12, loss = 0.88375757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72532400\n",
      "Iteration 2, loss = 3.20651906\n",
      "Iteration 3, loss = 9.48746653\n",
      "Iteration 4, loss = 0.77509931\n",
      "Iteration 5, loss = 1.06257568\n",
      "Iteration 6, loss = 0.70365129\n",
      "Iteration 7, loss = 0.97319840\n",
      "Iteration 8, loss = 1.66728820\n",
      "Iteration 9, loss = 3.15930719\n",
      "Iteration 10, loss = 1.23309136\n",
      "Iteration 11, loss = 1.02568611\n",
      "Iteration 12, loss = 0.79284594\n",
      "Iteration 13, loss = 0.69265723\n",
      "Iteration 14, loss = 0.69478219\n",
      "Iteration 15, loss = 0.69670803\n",
      "Iteration 16, loss = 0.69667319\n",
      "Iteration 17, loss = 0.69567127\n",
      "Iteration 18, loss = 0.69462393\n",
      "Iteration 19, loss = 0.69407849\n",
      "Iteration 20, loss = 0.69403964\n",
      "Iteration 21, loss = 0.69419084\n",
      "Iteration 22, loss = 0.69430794\n",
      "Iteration 23, loss = 0.69438223\n",
      "Iteration 24, loss = 0.69444684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72021058\n",
      "Iteration 2, loss = 3.13563103\n",
      "Iteration 3, loss = 9.66891312\n",
      "Iteration 4, loss = 0.80892149\n",
      "Iteration 5, loss = 1.12382810\n",
      "Iteration 6, loss = 0.71188171\n",
      "Iteration 7, loss = 0.97120601\n",
      "Iteration 8, loss = 1.73732787\n",
      "Iteration 9, loss = 3.32957658\n",
      "Iteration 10, loss = 1.12492062\n",
      "Iteration 11, loss = 1.04364378\n",
      "Iteration 12, loss = 0.82620583\n",
      "Iteration 13, loss = 0.69346545\n",
      "Iteration 14, loss = 0.69664528\n",
      "Iteration 15, loss = 0.69797918\n",
      "Iteration 16, loss = 0.69730231\n",
      "Iteration 17, loss = 0.69590531\n",
      "Iteration 18, loss = 0.69466467\n",
      "Iteration 19, loss = 0.69400283\n",
      "Iteration 20, loss = 0.69384654\n",
      "Iteration 21, loss = 0.69388506\n",
      "Iteration 22, loss = 0.69391984\n",
      "Iteration 23, loss = 0.69393276\n",
      "Iteration 24, loss = 0.69393776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72125764\n",
      "Iteration 2, loss = 18.13962621\n",
      "Iteration 3, loss = 18.13962442\n",
      "Iteration 4, loss = 17.90404487\n",
      "Iteration 5, loss = 17.90404535\n",
      "Iteration 6, loss = 17.90404538\n",
      "Iteration 7, loss = 8.63224893\n",
      "Iteration 8, loss = 8.87490154\n",
      "Iteration 9, loss = 15.33783017\n",
      "Iteration 10, loss = 14.65730809\n",
      "Iteration 11, loss = 9.76026551\n",
      "Iteration 12, loss = 2.76816918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72532400\n",
      "Iteration 2, loss = 17.90404678\n",
      "Iteration 3, loss = 17.90404501\n",
      "Iteration 4, loss = 18.13813528\n",
      "Iteration 5, loss = 18.13962471\n",
      "Iteration 6, loss = 18.13962471\n",
      "Iteration 7, loss = 6.37023783\n",
      "Iteration 8, loss = 11.38422232\n",
      "Iteration 9, loss = 17.89951564\n",
      "Iteration 10, loss = 17.51015539\n",
      "Iteration 11, loss = 12.74555326\n",
      "Iteration 12, loss = 5.86317716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72021058\n",
      "Iteration 2, loss = 18.02183643\n",
      "Iteration 3, loss = 18.02183466\n",
      "Iteration 4, loss = 18.02183452\n",
      "Iteration 5, loss = 18.02183498\n",
      "Iteration 6, loss = 18.02183499\n",
      "Iteration 7, loss = 7.53858621\n",
      "Iteration 8, loss = 10.09960288\n",
      "Iteration 9, loss = 16.65036242\n",
      "Iteration 10, loss = 16.05128830\n",
      "Iteration 11, loss = 11.21958434\n",
      "Iteration 12, loss = 4.28602486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08724508\n",
      "Iteration 2, loss = 18.13962480\n",
      "Iteration 3, loss = 0.98910082\n",
      "Iteration 4, loss = 3.14581152\n",
      "Iteration 5, loss = 2.66619158\n",
      "Iteration 6, loss = 0.73822799\n",
      "Iteration 7, loss = 0.80793640\n",
      "Iteration 8, loss = 0.69441451\n",
      "Iteration 9, loss = 1.15936312\n",
      "Iteration 10, loss = 0.86911644\n",
      "Iteration 11, loss = 3.44417070\n",
      "Iteration 12, loss = 1.84593802\n",
      "Iteration 13, loss = 4.78569777\n",
      "Iteration 14, loss = 0.70431873\n",
      "Iteration 15, loss = 0.76593202\n",
      "Iteration 16, loss = 0.77140032\n",
      "Iteration 17, loss = 0.91011979\n",
      "Iteration 18, loss = 0.76824765\n",
      "Iteration 19, loss = 0.71139669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09735780\n",
      "Iteration 2, loss = 17.90404537\n",
      "Iteration 3, loss = 1.36539535\n",
      "Iteration 4, loss = 3.41808248\n",
      "Iteration 5, loss = 2.71631257\n",
      "Iteration 6, loss = 0.77125394\n",
      "Iteration 7, loss = 1.03332450\n",
      "Iteration 8, loss = 0.69417314\n",
      "Iteration 9, loss = 1.59756276\n",
      "Iteration 10, loss = 1.05046217\n",
      "Iteration 11, loss = 4.81195021\n",
      "Iteration 12, loss = 1.21762534\n",
      "Iteration 13, loss = 3.79672732\n",
      "Iteration 14, loss = 1.58923130\n",
      "Iteration 15, loss = 1.01291071\n",
      "Iteration 16, loss = 0.86186314\n",
      "Iteration 17, loss = 0.70871654\n",
      "Iteration 18, loss = 0.69805351\n",
      "Iteration 19, loss = 0.69726925\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09724985\n",
      "Iteration 2, loss = 18.02183503\n",
      "Iteration 3, loss = 1.18331908\n",
      "Iteration 4, loss = 3.34652613\n",
      "Iteration 5, loss = 2.70032936\n",
      "Iteration 6, loss = 0.75209334\n",
      "Iteration 7, loss = 0.92126735\n",
      "Iteration 8, loss = 0.69213581\n",
      "Iteration 9, loss = 1.46520350\n",
      "Iteration 10, loss = 1.01092905\n",
      "Iteration 11, loss = 4.47781004\n",
      "Iteration 12, loss = 1.15850541\n",
      "Iteration 13, loss = 3.86442930\n",
      "Iteration 14, loss = 1.63493481\n",
      "Iteration 15, loss = 1.23652897\n",
      "Iteration 16, loss = 0.95656415\n",
      "Iteration 17, loss = 0.70807047\n",
      "Iteration 18, loss = 0.69753051\n",
      "Iteration 19, loss = 0.69787706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08724508\n",
      "Iteration 2, loss = 18.13962667\n",
      "Iteration 3, loss = 18.13962590\n",
      "Iteration 4, loss = 18.13962469\n",
      "Iteration 5, loss = 17.90404528\n",
      "Iteration 6, loss = 17.90404581\n",
      "Iteration 7, loss = 17.90404588\n",
      "Iteration 8, loss = 17.90404559\n",
      "Iteration 9, loss = 10.54542494\n",
      "Iteration 10, loss = 18.13962478\n",
      "Iteration 11, loss = 18.13962499\n",
      "Iteration 12, loss = 18.13962501\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09735780\n",
      "Iteration 2, loss = 17.90404724\n",
      "Iteration 3, loss = 17.90404651\n",
      "Iteration 4, loss = 17.90404528\n",
      "Iteration 5, loss = 18.13962467\n",
      "Iteration 6, loss = 18.13962513\n",
      "Iteration 7, loss = 18.13962517\n",
      "Iteration 8, loss = 18.13962489\n",
      "Iteration 9, loss = 3.97125204\n",
      "Iteration 10, loss = 17.90404541\n",
      "Iteration 11, loss = 17.90404571\n",
      "Iteration 12, loss = 17.90404578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09724985\n",
      "Iteration 2, loss = 18.02183689\n",
      "Iteration 3, loss = 18.02183614\n",
      "Iteration 4, loss = 18.02183493\n",
      "Iteration 5, loss = 18.02183492\n",
      "Iteration 6, loss = 18.02183541\n",
      "Iteration 7, loss = 18.02183547\n",
      "Iteration 8, loss = 18.02183518\n",
      "Iteration 9, loss = 7.20595597\n",
      "Iteration 10, loss = 18.02183504\n",
      "Iteration 11, loss = 18.02183529\n",
      "Iteration 12, loss = 18.02183533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11512437\n",
      "Iteration 2, loss = 18.13962537\n",
      "Iteration 3, loss = 0.69039190\n",
      "Iteration 4, loss = 0.72461944\n",
      "Iteration 5, loss = 4.82157856\n",
      "Iteration 6, loss = 8.22234008\n",
      "Iteration 7, loss = 2.07587432\n",
      "Iteration 8, loss = 0.71676222\n",
      "Iteration 9, loss = 0.70097569\n",
      "Iteration 10, loss = 0.71350562\n",
      "Iteration 11, loss = 0.89979740\n",
      "Iteration 12, loss = 2.60042395\n",
      "Iteration 13, loss = 2.98506320\n",
      "Iteration 14, loss = 0.69854537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12736457\n",
      "Iteration 2, loss = 17.90404594\n",
      "Iteration 3, loss = 0.71421763\n",
      "Iteration 4, loss = 1.44193490\n",
      "Iteration 5, loss = 10.29736512\n",
      "Iteration 6, loss = 0.69419682\n",
      "Iteration 7, loss = 0.69350549\n",
      "Iteration 8, loss = 0.74325302\n",
      "Iteration 9, loss = 1.05415941\n",
      "Iteration 10, loss = 3.85730847\n",
      "Iteration 11, loss = 1.12812521\n",
      "Iteration 12, loss = 0.96745594\n",
      "Iteration 13, loss = 0.97679592\n",
      "Iteration 14, loss = 0.71835759\n",
      "Iteration 15, loss = 0.69868808\n",
      "Iteration 16, loss = 0.69771012\n",
      "Iteration 17, loss = 0.69704491\n",
      "Iteration 18, loss = 0.69431511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12511159\n",
      "Iteration 2, loss = 18.02183559\n",
      "Iteration 3, loss = 0.69790847\n",
      "Iteration 4, loss = 1.03159880\n",
      "Iteration 5, loss = 9.19676312\n",
      "Iteration 6, loss = 1.74848643\n",
      "Iteration 7, loss = 2.91751553\n",
      "Iteration 8, loss = 0.75975611\n",
      "Iteration 9, loss = 0.75305058\n",
      "Iteration 10, loss = 0.98206381\n",
      "Iteration 11, loss = 1.71656533\n",
      "Iteration 12, loss = 1.90501339\n",
      "Iteration 13, loss = 1.32969200\n",
      "Iteration 14, loss = 0.69718671\n",
      "Iteration 15, loss = 0.70729246\n",
      "Iteration 16, loss = 0.69872222\n",
      "Iteration 17, loss = 0.69238430\n",
      "Iteration 18, loss = 0.69405840\n",
      "Iteration 19, loss = 0.69231693\n",
      "Iteration 20, loss = 0.69122231\n",
      "Iteration 21, loss = 0.69075009\n",
      "Iteration 22, loss = 0.69029339\n",
      "Iteration 23, loss = 0.68987471\n",
      "Iteration 24, loss = 0.68985074\n",
      "Iteration 25, loss = 0.69457728\n",
      "Iteration 26, loss = 0.75899046\n",
      "Iteration 27, loss = 1.45888916\n",
      "Iteration 28, loss = 2.67033768\n",
      "Iteration 29, loss = 0.77232974\n",
      "Iteration 30, loss = 0.69122925\n",
      "Iteration 31, loss = 0.69074670\n",
      "Iteration 32, loss = 0.68975804\n",
      "Iteration 33, loss = 0.69091728\n",
      "Iteration 34, loss = 0.69114308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11512437\n",
      "Iteration 2, loss = 18.13962734\n",
      "Iteration 3, loss = 18.13962729\n",
      "Iteration 4, loss = 18.13962747\n",
      "Iteration 5, loss = 17.90404981\n",
      "Iteration 6, loss = 17.90405060\n",
      "Iteration 7, loss = 17.90405047\n",
      "Iteration 8, loss = 17.90404999\n",
      "Iteration 9, loss = 1.28370346\n",
      "Iteration 10, loss = 18.13962962\n",
      "Iteration 11, loss = 18.13962950\n",
      "Iteration 12, loss = 18.13962879\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12736457\n",
      "Iteration 2, loss = 17.90404790\n",
      "Iteration 3, loss = 17.90404790\n",
      "Iteration 4, loss = 17.90404807\n",
      "Iteration 5, loss = 18.13962921\n",
      "Iteration 6, loss = 18.13962992\n",
      "Iteration 7, loss = 18.13962975\n",
      "Iteration 8, loss = 18.13962925\n",
      "Iteration 9, loss = 3.20268763\n",
      "Iteration 10, loss = 12.32312161\n",
      "Iteration 11, loss = 11.58135007\n",
      "Iteration 12, loss = 4.36531348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12511159\n",
      "Iteration 2, loss = 18.02183755\n",
      "Iteration 3, loss = 18.02183753\n",
      "Iteration 4, loss = 18.02183770\n",
      "Iteration 5, loss = 18.02183942\n",
      "Iteration 6, loss = 18.02184017\n",
      "Iteration 7, loss = 18.02184002\n",
      "Iteration 8, loss = 18.02183953\n",
      "Iteration 9, loss = 1.19743421\n",
      "Iteration 10, loss = 12.06617538\n",
      "Iteration 11, loss = 12.38350364\n",
      "Iteration 12, loss = 5.70524551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72125764\n",
      "Iteration 2, loss = 18.13962452\n",
      "Iteration 3, loss = 17.90407403\n",
      "Iteration 4, loss = 18.14972353\n",
      "Iteration 5, loss = 1301.87840161\n",
      "Iteration 6, loss = 9832287063197.91406250\n",
      "Iteration 7, loss = 1196535183519776657212993636925440.00000000\n",
      "Iteration 8, loss = 8526779604940517223931978936188444328612889262038604192374704687691071488.00000000\n",
      "Iteration 9, loss = 899879605462422477476059868886607401569497248549443511809248161487213864006886282923090793753437228666065963772664537230591513524548130398653694292787200.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.72532400\n",
      "Iteration 2, loss = 17.90404513\n",
      "Iteration 3, loss = 18.13965551\n",
      "Iteration 4, loss = 17.91482090\n",
      "Iteration 5, loss = 1655.44879263\n",
      "Iteration 6, loss = 14270856713707.66796875\n",
      "Iteration 7, loss = 2843162466489558938013041720033280.00000000\n",
      "Iteration 8, loss = 42988793023680575625802835047860578338723144307608724935418047186140659712.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 25799464903590808905483118611291246050070235061874127775389420221156803095572801535713969046629091643900520481380811815839697212525136380129877009463508992.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72021058\n",
      "Iteration 2, loss = 18.02183477\n",
      "Iteration 3, loss = 18.02186455\n",
      "Iteration 4, loss = 18.03225855\n",
      "Iteration 5, loss = 1480.59986241\n",
      "Iteration 6, loss = 12241902138272.90625000\n",
      "Iteration 7, loss = 1990828622682534158711018928209920.00000000\n",
      "Iteration 8, loss = 22655292086365988695555212253487808415069026419053760512005055510441099264.00000000\n",
      "Iteration 9, loss = 6818262834027099726039773546254530975254814407410070966723398445076558874790453136535433129716344255613636278882054915622630065245133332696186980470358016.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.72125764\n",
      "Iteration 2, loss = 18.13983555\n",
      "Iteration 3, loss = 17.90428464\n",
      "Iteration 4, loss = 17.90417232\n",
      "Iteration 5, loss = 18.13970903\n",
      "Iteration 6, loss = 18.13968718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 5.78896532\n",
      "Iteration 8, loss = 17.90409020\n",
      "Iteration 9, loss = 12.90235750\n",
      "Iteration 10, loss = 9.43512383\n",
      "Iteration 11, loss = 12.39908317\n",
      "Iteration 12, loss = 0.72431323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72532400\n",
      "Iteration 2, loss = 17.90425612\n",
      "Iteration 3, loss = 18.13986755\n",
      "Iteration 4, loss = 18.13975395\n",
      "Iteration 5, loss = 17.90412753\n",
      "Iteration 6, loss = 17.90410555\n",
      "Iteration 7, loss = 6.40614172\n",
      "Iteration 8, loss = 18.13967062\n",
      "Iteration 9, loss = 15.40378236\n",
      "Iteration 10, loss = 4.54068250\n",
      "Iteration 11, loss = 6.49361907\n",
      "Iteration 12, loss = 4.54044709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72021058\n",
      "Iteration 2, loss = 18.02204441\n",
      "Iteration 3, loss = 18.02207443\n",
      "Iteration 4, loss = 18.02196219\n",
      "Iteration 5, loss = 18.02191759\n",
      "Iteration 6, loss = 18.02189583\n",
      "Iteration 7, loss = 6.14377256\n",
      "Iteration 8, loss = 18.02187996\n",
      "Iteration 9, loss = 14.24204715\n",
      "Iteration 10, loss = 6.76394373\n",
      "Iteration 11, loss = 9.24616529\n",
      "Iteration 12, loss = 2.42595407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08724508\n",
      "Iteration 2, loss = 18.13964204\n",
      "Iteration 3, loss = 18.14123902\n",
      "Iteration 4, loss = 59.12927921\n",
      "Iteration 5, loss = 372772501.57880265\n",
      "Iteration 6, loss = 1821720211291847306248192.00000000\n",
      "Iteration 7, loss = 696616615378287151555405118250319009091822171489042432.00000000\n",
      "Iteration 8, loss = 6361796997479160175334029460363620540015163944947663787738600054694204196925662977585171446054199275166632723349504.00000000\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 1.09735780\n",
      "Iteration 2, loss = 17.90406316\n",
      "Iteration 3, loss = 17.90566627\n",
      "Iteration 4, loss = 58.65763836\n",
      "Iteration 5, loss = 503342712.40115166\n",
      "Iteration 6, loss = 3332465990901801418752000.00000000\n",
      "Iteration 7, loss = 3299487199791019746636040953336952989952075834987642880.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 143195658684935071161544022477979057903486629844432757122498296859098461114961120167152419754576075207851648649003008.00000000\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 1.09724985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 18.02185248\n",
      "Iteration 3, loss = 18.02344582\n",
      "Iteration 4, loss = 58.80468857\n",
      "Iteration 5, loss = 448318525.27306229\n",
      "Iteration 6, loss = 2664989213075025168433152.00000000\n",
      "Iteration 7, loss = 1846971792277624616094055359901981859392693526276866048.00000000\n",
      "Iteration 8, loss = 45231521528802580696941757287926876851334717926859432698394832734034001927061915631060773881109460338538542847229952.00000000\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08724508\n",
      "Iteration 2, loss = 18.13983560\n",
      "Iteration 3, loss = 18.13986522\n",
      "Iteration 4, loss = 17.90429430\n",
      "Iteration 5, loss = 17.90425632\n",
      "Iteration 6, loss = 1.18422893\n",
      "Iteration 7, loss = 18.13979097\n",
      "Iteration 8, loss = 18.13977042\n",
      "Iteration 9, loss = 17.90417464\n",
      "Iteration 10, loss = 17.90415700\n",
      "Iteration 11, loss = 17.90413582\n",
      "Iteration 12, loss = 11.53848986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09735780\n",
      "Iteration 2, loss = 17.90425617\n",
      "Iteration 3, loss = 17.90429120\n",
      "Iteration 4, loss = 18.13988307\n",
      "Iteration 5, loss = 18.13984637\n",
      "Iteration 6, loss = 5.44046784\n",
      "Iteration 7, loss = 17.90421671\n",
      "Iteration 8, loss = 1.96406043\n",
      "Iteration 9, loss = 17.90418146\n",
      "Iteration 10, loss = 10.00719150\n",
      "Iteration 11, loss = 3.56546924\n",
      "Iteration 12, loss = 17.80973542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09724985\n",
      "Iteration 2, loss = 18.02204446\n",
      "Iteration 3, loss = 18.02207679\n",
      "Iteration 4, loss = 18.02208735\n",
      "Iteration 5, loss = 18.02205034\n",
      "Iteration 6, loss = 2.82866389\n",
      "Iteration 7, loss = 18.02200105\n",
      "Iteration 8, loss = 4.35190836\n",
      "Iteration 9, loss = 18.02196734\n",
      "Iteration 10, loss = 8.51873995\n",
      "Iteration 11, loss = 6.08284789\n",
      "Iteration 12, loss = 15.00777019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11512437\n",
      "Iteration 2, loss = 18.13963863\n",
      "Iteration 3, loss = 18.14108529\n",
      "Iteration 4, loss = 37.52210188\n",
      "Iteration 5, loss = 72886575.51283082\n",
      "Iteration 6, loss = 16480053308859869036544.00000000\n",
      "Iteration 7, loss = 43893122030719923967880334993791376236549314183168.00000000\n",
      "Iteration 8, loss = 5975360547629856849883728673071068730609973113064480857423302717607394597445927534213067144584111660728320.00000000\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 1.12736457\n",
      "Iteration 2, loss = 17.90405965\n",
      "Iteration 3, loss = 17.90551567\n",
      "Iteration 4, loss = 37.53986968\n",
      "Iteration 5, loss = 109462411.33282554\n",
      "Iteration 6, loss = 46977058006462451679232.00000000\n",
      "Iteration 7, loss = 586241097236985868586174941131207550524294086787072.00000000\n",
      "Iteration 8, loss = 1347295454753196370086779404872486770201005119250389162430006382163865459038052398348709025624230660786880512.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12511159\n",
      "Iteration 2, loss = 18.02184905\n",
      "Iteration 3, loss = 18.02329673\n",
      "Iteration 4, loss = 37.53487475\n",
      "Iteration 5, loss = 95111573.22412017\n",
      "Iteration 6, loss = 33217172570834082988032.00000000\n",
      "Iteration 7, loss = 246907375795168908650679792054532744413314862284800.00000000\n",
      "Iteration 8, loss = 223824073386826867051854946131813751446265354319594411956488900734301976941031708672207348297162746081837056.00000000\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11512437\n",
      "Iteration 2, loss = 18.13983690\n",
      "Iteration 3, loss = 18.13985209\n",
      "Iteration 4, loss = 17.90436425\n",
      "Iteration 5, loss = 17.90440145\n",
      "Iteration 6, loss = 18.14003383\n",
      "Iteration 7, loss = 18.14004575\n",
      "Iteration 8, loss = 18.14001917\n",
      "Iteration 9, loss = 17.90441985\n",
      "Iteration 10, loss = 17.90440085\n",
      "Iteration 11, loss = 17.90436494\n",
      "Iteration 12, loss = 18.13990651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12736457\n",
      "Iteration 2, loss = 17.90425746\n",
      "Iteration 3, loss = 17.90427597\n",
      "Iteration 4, loss = 18.13994864\n",
      "Iteration 5, loss = 18.13998425\n",
      "Iteration 6, loss = 17.90445547\n",
      "Iteration 7, loss = 17.90446594\n",
      "Iteration 8, loss = 17.90443740\n",
      "Iteration 9, loss = 18.13999369\n",
      "Iteration 10, loss = 18.13997302\n",
      "Iteration 11, loss = 18.13993727\n",
      "Iteration 12, loss = 17.90432235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12511159\n",
      "Iteration 2, loss = 18.02204575\n",
      "Iteration 3, loss = 18.02206270\n",
      "Iteration 4, loss = 18.02215461\n",
      "Iteration 5, loss = 18.02219085\n",
      "Iteration 6, loss = 18.02224225\n",
      "Iteration 7, loss = 18.02225337\n",
      "Iteration 8, loss = 18.02222606\n",
      "Iteration 9, loss = 18.02220479\n",
      "Iteration 10, loss = 18.02218512\n",
      "Iteration 11, loss = 18.02214951\n",
      "Iteration 12, loss = 18.02211297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69320797\n",
      "Iteration 2, loss = 0.69309800\n",
      "Iteration 3, loss = 0.69292126\n",
      "Iteration 4, loss = 0.69270414\n",
      "Iteration 5, loss = 0.69244363\n",
      "Iteration 6, loss = 0.69214743\n",
      "Iteration 7, loss = 0.69181883\n",
      "Iteration 8, loss = 0.69146233\n",
      "Iteration 9, loss = 0.69108172\n",
      "Iteration 10, loss = 0.69068079\n",
      "Iteration 11, loss = 0.69026329\n",
      "Iteration 12, loss = 0.68983289\n",
      "Iteration 13, loss = 0.68939355\n",
      "Iteration 14, loss = 0.68894921\n",
      "Iteration 15, loss = 0.68850453\n",
      "Iteration 16, loss = 0.68806417\n",
      "Iteration 17, loss = 0.68763418\n",
      "Iteration 18, loss = 0.68722052\n",
      "Iteration 19, loss = 0.68683174\n",
      "Iteration 20, loss = 0.68647583\n",
      "Iteration 21, loss = 0.68616538\n",
      "Iteration 22, loss = 0.68591130\n",
      "Iteration 23, loss = 0.68573250\n",
      "Iteration 24, loss = 0.68564348\n",
      "Iteration 25, loss = 0.68567257\n",
      "Iteration 26, loss = 0.68583740\n",
      "Iteration 27, loss = 0.68617945\n",
      "Iteration 28, loss = 0.68671580\n",
      "Iteration 29, loss = 0.68750419\n",
      "Iteration 30, loss = 0.68855005\n",
      "Iteration 31, loss = 0.68992757\n",
      "Iteration 32, loss = 0.69160577\n",
      "Iteration 33, loss = 0.69367028\n",
      "Iteration 34, loss = 0.69601137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69803140\n",
      "Iteration 2, loss = 0.69816581\n",
      "Iteration 3, loss = 0.69801740\n",
      "Iteration 4, loss = 0.69794987\n",
      "Iteration 5, loss = 0.69783215\n",
      "Iteration 6, loss = 0.69772625\n",
      "Iteration 7, loss = 0.69762119\n",
      "Iteration 8, loss = 0.69753881\n",
      "Iteration 9, loss = 0.69748481\n",
      "Iteration 10, loss = 0.69748106\n",
      "Iteration 11, loss = 0.69753488\n",
      "Iteration 12, loss = 0.69767702\n",
      "Iteration 13, loss = 0.69791138\n",
      "Iteration 14, loss = 0.69828339\n",
      "Iteration 15, loss = 0.69878577\n",
      "Iteration 16, loss = 0.69948540\n",
      "Iteration 17, loss = 0.70034837\n",
      "Iteration 18, loss = 0.70147151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69652269\n",
      "Iteration 2, loss = 0.69642643\n",
      "Iteration 3, loss = 0.69627033\n",
      "Iteration 4, loss = 0.69607900\n",
      "Iteration 5, loss = 0.69584928\n",
      "Iteration 6, loss = 0.69558817\n",
      "Iteration 7, loss = 0.69529850\n",
      "Iteration 8, loss = 0.69498430\n",
      "Iteration 9, loss = 0.69464886\n",
      "Iteration 10, loss = 0.69429558\n",
      "Iteration 11, loss = 0.69392766\n",
      "Iteration 12, loss = 0.69354841\n",
      "Iteration 13, loss = 0.69316108\n",
      "Iteration 14, loss = 0.69276925\n",
      "Iteration 15, loss = 0.69237654\n",
      "Iteration 16, loss = 0.69198719\n",
      "Iteration 17, loss = 0.69160558\n",
      "Iteration 18, loss = 0.69123722\n",
      "Iteration 19, loss = 0.69088774\n",
      "Iteration 20, loss = 0.69056470\n",
      "Iteration 21, loss = 0.69027558\n",
      "Iteration 22, loss = 0.69003113\n",
      "Iteration 23, loss = 0.68984129\n",
      "Iteration 24, loss = 0.68972144\n",
      "Iteration 25, loss = 0.68968438\n",
      "Iteration 26, loss = 0.68975180\n",
      "Iteration 27, loss = 0.68993885\n",
      "Iteration 28, loss = 0.69027500\n",
      "Iteration 29, loss = 0.69077489\n",
      "Iteration 30, loss = 0.69147608\n",
      "Iteration 31, loss = 0.69238511\n",
      "Iteration 32, loss = 0.69354511\n",
      "Iteration 33, loss = 0.69493926\n",
      "Iteration 34, loss = 0.69661004\n",
      "Iteration 35, loss = 0.69849377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69320797\n",
      "Iteration 2, loss = 3.27943865\n",
      "Iteration 3, loss = 3.44538238\n",
      "Iteration 4, loss = 3.05850928\n",
      "Iteration 5, loss = 0.66091765\n",
      "Iteration 6, loss = 2.41458205\n",
      "Iteration 7, loss = 1.85362408\n",
      "Iteration 8, loss = 0.76142186\n",
      "Iteration 9, loss = 1.63784720\n",
      "Iteration 10, loss = 1.09693539\n",
      "Iteration 11, loss = 0.89734259\n",
      "Iteration 12, loss = 1.31869422\n",
      "Iteration 13, loss = 0.77581953\n",
      "Iteration 14, loss = 0.95948438\n",
      "Iteration 15, loss = 1.14302558\n",
      "Iteration 16, loss = 0.68072925\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69803140\n",
      "Iteration 2, loss = 9.10632487\n",
      "Iteration 3, loss = 2.39041801\n",
      "Iteration 4, loss = 5.41535499\n",
      "Iteration 5, loss = 7.45221243\n",
      "Iteration 6, loss = 6.42880998\n",
      "Iteration 7, loss = 3.55276711\n",
      "Iteration 8, loss = 0.82559113\n",
      "Iteration 9, loss = 3.15437457\n",
      "Iteration 10, loss = 3.77983409\n",
      "Iteration 11, loss = 2.90003505\n",
      "Iteration 12, loss = 1.02287473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69652269\n",
      "Iteration 2, loss = 3.91158036\n",
      "Iteration 3, loss = 2.82723715\n",
      "Iteration 4, loss = 2.42513604\n",
      "Iteration 5, loss = 0.95306495\n",
      "Iteration 6, loss = 1.60309918\n",
      "Iteration 7, loss = 0.67063413\n",
      "Iteration 8, loss = 1.43592974\n",
      "Iteration 9, loss = 1.08631407\n",
      "Iteration 10, loss = 0.87711326\n",
      "Iteration 11, loss = 1.19665933\n",
      "Iteration 12, loss = 0.68514359\n",
      "Iteration 13, loss = 1.02852742\n",
      "Iteration 14, loss = 0.97488892\n",
      "Iteration 15, loss = 0.67702785\n",
      "Iteration 16, loss = 0.98357225\n",
      "Iteration 17, loss = 0.76033056\n",
      "Iteration 18, loss = 0.76324561\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69820099\n",
      "Iteration 2, loss = 0.69966307\n",
      "Iteration 3, loss = 0.69702320\n",
      "Iteration 4, loss = 0.69633838\n",
      "Iteration 5, loss = 0.69572409\n",
      "Iteration 6, loss = 0.69529516\n",
      "Iteration 7, loss = 0.69491130\n",
      "Iteration 8, loss = 0.69454464\n",
      "Iteration 9, loss = 0.69417448\n",
      "Iteration 10, loss = 0.69379324\n",
      "Iteration 11, loss = 0.69339777\n",
      "Iteration 12, loss = 0.69298750\n",
      "Iteration 13, loss = 0.69256293\n",
      "Iteration 14, loss = 0.69212504\n",
      "Iteration 15, loss = 0.69167499\n",
      "Iteration 16, loss = 0.69121393\n",
      "Iteration 17, loss = 0.69074300\n",
      "Iteration 18, loss = 0.69026321\n",
      "Iteration 19, loss = 0.68977552\n",
      "Iteration 20, loss = 0.68928078\n",
      "Iteration 21, loss = 0.68877978\n",
      "Iteration 22, loss = 0.68827319\n",
      "Iteration 23, loss = 0.68776167\n",
      "Iteration 24, loss = 0.68724575\n",
      "Iteration 25, loss = 0.68672596\n",
      "Iteration 26, loss = 0.68620273\n",
      "Iteration 27, loss = 0.68567648\n",
      "Iteration 28, loss = 0.68514757\n",
      "Iteration 29, loss = 0.68461632\n",
      "Iteration 30, loss = 0.68408303\n",
      "Iteration 31, loss = 0.68354794\n",
      "Iteration 32, loss = 0.68301129\n",
      "Iteration 33, loss = 0.68247329\n",
      "Iteration 34, loss = 0.68193412\n",
      "Iteration 35, loss = 0.68139394\n",
      "Iteration 36, loss = 0.68085289\n",
      "Iteration 37, loss = 0.68031111\n",
      "Iteration 38, loss = 0.67976871\n",
      "Iteration 39, loss = 0.67922579\n",
      "Iteration 40, loss = 0.67868243\n",
      "Iteration 41, loss = 0.67813871\n",
      "Iteration 42, loss = 0.67759471\n",
      "Iteration 43, loss = 0.67705047\n",
      "Iteration 44, loss = 0.67650605\n",
      "Iteration 45, loss = 0.67596150\n",
      "Iteration 46, loss = 0.67541685\n",
      "Iteration 47, loss = 0.67487213\n",
      "Iteration 48, loss = 0.67432738\n",
      "Iteration 49, loss = 0.67378261\n",
      "Iteration 50, loss = 0.67323784\n",
      "Iteration 51, loss = 0.67269309\n",
      "Iteration 52, loss = 0.67214837\n",
      "Iteration 53, loss = 0.67160369\n",
      "Iteration 54, loss = 0.67105905\n",
      "Iteration 55, loss = 0.67051446\n",
      "Iteration 56, loss = 0.66996991\n",
      "Iteration 57, loss = 0.66942542\n",
      "Iteration 58, loss = 0.66888097\n",
      "Iteration 59, loss = 0.66833657\n",
      "Iteration 60, loss = 0.66779221\n",
      "Iteration 61, loss = 0.66724787\n",
      "Iteration 62, loss = 0.66670357\n",
      "Iteration 63, loss = 0.66615927\n",
      "Iteration 64, loss = 0.66561499\n",
      "Iteration 65, loss = 0.66507071\n",
      "Iteration 66, loss = 0.66452641\n",
      "Iteration 67, loss = 0.66398209\n",
      "Iteration 68, loss = 0.66343774\n",
      "Iteration 69, loss = 0.66289334\n",
      "Iteration 70, loss = 0.66234888\n",
      "Iteration 71, loss = 0.66180435\n",
      "Iteration 72, loss = 0.66125973\n",
      "Iteration 73, loss = 0.66071502\n",
      "Iteration 74, loss = 0.66017020\n",
      "Iteration 75, loss = 0.65962525\n",
      "Iteration 76, loss = 0.65908016\n",
      "Iteration 77, loss = 0.65853492\n",
      "Iteration 78, loss = 0.65798952\n",
      "Iteration 79, loss = 0.65744393\n",
      "Iteration 80, loss = 0.65689815\n",
      "Iteration 81, loss = 0.65635216\n",
      "Iteration 82, loss = 0.65580594\n",
      "Iteration 83, loss = 0.65525949\n",
      "Iteration 84, loss = 0.65471278\n",
      "Iteration 85, loss = 0.65416581\n",
      "Iteration 86, loss = 0.65361856\n",
      "Iteration 87, loss = 0.65307101\n",
      "Iteration 88, loss = 0.65252314\n",
      "Iteration 89, loss = 0.65197496\n",
      "Iteration 90, loss = 0.65142643\n",
      "Iteration 91, loss = 0.65087755\n",
      "Iteration 92, loss = 0.65032830\n",
      "Iteration 93, loss = 0.64977867\n",
      "Iteration 94, loss = 0.64922865\n",
      "Iteration 95, loss = 0.64867821\n",
      "Iteration 96, loss = 0.64812734\n",
      "Iteration 97, loss = 0.64757604\n",
      "Iteration 98, loss = 0.64702429\n",
      "Iteration 99, loss = 0.64647207\n",
      "Iteration 100, loss = 0.64591937\n",
      "Iteration 101, loss = 0.64536617\n",
      "Iteration 102, loss = 0.64481246\n",
      "Iteration 103, loss = 0.64425824\n",
      "Iteration 104, loss = 0.64370347\n",
      "Iteration 105, loss = 0.64314816\n",
      "Iteration 106, loss = 0.64259229\n",
      "Iteration 107, loss = 0.64203584\n",
      "Iteration 108, loss = 0.64147879\n",
      "Iteration 109, loss = 0.64092115\n",
      "Iteration 110, loss = 0.64036289\n",
      "Iteration 111, loss = 0.63980400\n",
      "Iteration 112, loss = 0.63924447\n",
      "Iteration 113, loss = 0.63868428\n",
      "Iteration 114, loss = 0.63812343\n",
      "Iteration 115, loss = 0.63756189\n",
      "Iteration 116, loss = 0.63699966\n",
      "Iteration 117, loss = 0.63643672\n",
      "Iteration 118, loss = 0.63587306\n",
      "Iteration 119, loss = 0.63530868\n",
      "Iteration 120, loss = 0.63474354\n",
      "Iteration 121, loss = 0.63417765\n",
      "Iteration 122, loss = 0.63361099\n",
      "Iteration 123, loss = 0.63304355\n",
      "Iteration 124, loss = 0.63247531\n",
      "Iteration 125, loss = 0.63190627\n",
      "Iteration 126, loss = 0.63133641\n",
      "Iteration 127, loss = 0.63076572\n",
      "Iteration 128, loss = 0.63019419\n",
      "Iteration 129, loss = 0.62962180\n",
      "Iteration 130, loss = 0.62904855\n",
      "Iteration 131, loss = 0.62847442\n",
      "Iteration 132, loss = 0.62789940\n",
      "Iteration 133, loss = 0.62732348\n",
      "Iteration 134, loss = 0.62674665\n",
      "Iteration 135, loss = 0.62616889\n",
      "Iteration 136, loss = 0.62559020\n",
      "Iteration 137, loss = 0.62501056\n",
      "Iteration 138, loss = 0.62442997\n",
      "Iteration 139, loss = 0.62384840\n",
      "Iteration 140, loss = 0.62326585\n",
      "Iteration 141, loss = 0.62268232\n",
      "Iteration 142, loss = 0.62209778\n",
      "Iteration 143, loss = 0.62151222\n",
      "Iteration 144, loss = 0.62092565\n",
      "Iteration 145, loss = 0.62033804\n",
      "Iteration 146, loss = 0.61974938\n",
      "Iteration 147, loss = 0.61915967\n",
      "Iteration 148, loss = 0.61856889\n",
      "Iteration 149, loss = 0.61797703\n",
      "Iteration 150, loss = 0.61738409\n",
      "Iteration 151, loss = 0.61679004\n",
      "Iteration 152, loss = 0.61619489\n",
      "Iteration 153, loss = 0.61559863\n",
      "Iteration 154, loss = 0.61500123\n",
      "Iteration 155, loss = 0.61440269\n",
      "Iteration 156, loss = 0.61380301\n",
      "Iteration 157, loss = 0.61320217\n",
      "Iteration 158, loss = 0.61260016\n",
      "Iteration 159, loss = 0.61199697\n",
      "Iteration 160, loss = 0.61139259\n",
      "Iteration 161, loss = 0.61078702\n",
      "Iteration 162, loss = 0.61018023\n",
      "Iteration 163, loss = 0.60957224\n",
      "Iteration 164, loss = 0.60896301\n",
      "Iteration 165, loss = 0.60835255\n",
      "Iteration 166, loss = 0.60774085\n",
      "Iteration 167, loss = 0.60712789\n",
      "Iteration 168, loss = 0.60651367\n",
      "Iteration 169, loss = 0.60589818\n",
      "Iteration 170, loss = 0.60528141\n",
      "Iteration 171, loss = 0.60466335\n",
      "Iteration 172, loss = 0.60404399\n",
      "Iteration 173, loss = 0.60342332\n",
      "Iteration 174, loss = 0.60280134\n",
      "Iteration 175, loss = 0.60217803\n",
      "Iteration 176, loss = 0.60155339\n",
      "Iteration 177, loss = 0.60092741\n",
      "Iteration 178, loss = 0.60030008\n",
      "Iteration 179, loss = 0.59967139\n",
      "Iteration 180, loss = 0.59904133\n",
      "Iteration 181, loss = 0.59840990\n",
      "Iteration 182, loss = 0.59777709\n",
      "Iteration 183, loss = 0.59714289\n",
      "Iteration 184, loss = 0.59650729\n",
      "Iteration 185, loss = 0.59587028\n",
      "Iteration 186, loss = 0.59523186\n",
      "Iteration 187, loss = 0.59459202\n",
      "Iteration 188, loss = 0.59395075\n",
      "Iteration 189, loss = 0.59330805\n",
      "Iteration 190, loss = 0.59266390\n",
      "Iteration 191, loss = 0.59201830\n",
      "Iteration 192, loss = 0.59137124\n",
      "Iteration 193, loss = 0.59072271\n",
      "Iteration 194, loss = 0.59007272\n",
      "Iteration 195, loss = 0.58942124\n",
      "Iteration 196, loss = 0.58876828\n",
      "Iteration 197, loss = 0.58811383\n",
      "Iteration 198, loss = 0.58745788\n",
      "Iteration 199, loss = 0.58680042\n",
      "Iteration 200, loss = 0.58614145\n",
      "Iteration 201, loss = 0.58548095\n",
      "Iteration 202, loss = 0.58481894\n",
      "Iteration 203, loss = 0.58415539\n",
      "Iteration 204, loss = 0.58349031\n",
      "Iteration 205, loss = 0.58282368\n",
      "Iteration 206, loss = 0.58215550\n",
      "Iteration 207, loss = 0.58148576\n",
      "Iteration 208, loss = 0.58081447\n",
      "Iteration 209, loss = 0.58014161\n",
      "Iteration 210, loss = 0.57946717\n",
      "Iteration 211, loss = 0.57879116\n",
      "Iteration 212, loss = 0.57811356\n",
      "Iteration 213, loss = 0.57743438\n",
      "Iteration 214, loss = 0.57675360\n",
      "Iteration 215, loss = 0.57607122\n",
      "Iteration 216, loss = 0.57538724\n",
      "Iteration 217, loss = 0.57470165\n",
      "Iteration 218, loss = 0.57401445\n",
      "Iteration 219, loss = 0.57332563\n",
      "Iteration 220, loss = 0.57263518\n",
      "Iteration 221, loss = 0.57194311\n",
      "Iteration 222, loss = 0.57124940\n",
      "Iteration 223, loss = 0.57055406\n",
      "Iteration 224, loss = 0.56985708\n",
      "Iteration 225, loss = 0.56915846\n",
      "Iteration 226, loss = 0.56845819\n",
      "Iteration 227, loss = 0.56775626\n",
      "Iteration 228, loss = 0.56705268\n",
      "Iteration 229, loss = 0.56634744\n",
      "Iteration 230, loss = 0.56564054\n",
      "Iteration 231, loss = 0.56493197\n",
      "Iteration 232, loss = 0.56422174\n",
      "Iteration 233, loss = 0.56350983\n",
      "Iteration 234, loss = 0.56279625\n",
      "Iteration 235, loss = 0.56208101\n",
      "Iteration 236, loss = 0.56136412\n",
      "Iteration 237, loss = 0.56064569\n",
      "Iteration 238, loss = 0.55992595\n",
      "Iteration 239, loss = 0.55920572\n",
      "Iteration 240, loss = 0.55848751\n",
      "Iteration 241, loss = 0.55777928\n",
      "Iteration 242, loss = 0.55710627\n",
      "Iteration 243, loss = 0.55655005\n",
      "Iteration 244, loss = 0.55637521\n",
      "Iteration 245, loss = 0.55744995\n",
      "Iteration 246, loss = 0.56260269\n",
      "Iteration 247, loss = 0.58086256\n",
      "Iteration 248, loss = 0.63729480\n",
      "Iteration 249, loss = 0.77716855\n",
      "Iteration 250, loss = 0.96545489\n",
      "Iteration 251, loss = 1.02936937\n",
      "Iteration 252, loss = 0.98074237\n",
      "Iteration 253, loss = 0.98711010\n",
      "Iteration 254, loss = 0.97505389\n",
      "Iteration 255, loss = 0.96856517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70059838\n",
      "Iteration 2, loss = 0.70264622\n",
      "Iteration 3, loss = 0.69909694\n",
      "Iteration 4, loss = 0.69827134\n",
      "Iteration 5, loss = 0.69756115\n",
      "Iteration 6, loss = 0.69711746\n",
      "Iteration 7, loss = 0.69674888\n",
      "Iteration 8, loss = 0.69641639\n",
      "Iteration 9, loss = 0.69609089\n",
      "Iteration 10, loss = 0.69576102\n",
      "Iteration 11, loss = 0.69542153\n",
      "Iteration 12, loss = 0.69507068\n",
      "Iteration 13, loss = 0.69470825\n",
      "Iteration 14, loss = 0.69433478\n",
      "Iteration 15, loss = 0.69395111\n",
      "Iteration 16, loss = 0.69355814\n",
      "Iteration 17, loss = 0.69315680\n",
      "Iteration 18, loss = 0.69274795\n",
      "Iteration 19, loss = 0.69233238\n",
      "Iteration 20, loss = 0.69191083\n",
      "Iteration 21, loss = 0.69148395\n",
      "Iteration 22, loss = 0.69105233\n",
      "Iteration 23, loss = 0.69061652\n",
      "Iteration 24, loss = 0.69017699\n",
      "Iteration 25, loss = 0.68973417\n",
      "Iteration 26, loss = 0.68928845\n",
      "Iteration 27, loss = 0.68884018\n",
      "Iteration 28, loss = 0.68838966\n",
      "Iteration 29, loss = 0.68793718\n",
      "Iteration 30, loss = 0.68748298\n",
      "Iteration 31, loss = 0.68702729\n",
      "Iteration 32, loss = 0.68657030\n",
      "Iteration 33, loss = 0.68611220\n",
      "Iteration 34, loss = 0.68565313\n",
      "Iteration 35, loss = 0.68519324\n",
      "Iteration 36, loss = 0.68473266\n",
      "Iteration 37, loss = 0.68427149\n",
      "Iteration 38, loss = 0.68380983\n",
      "Iteration 39, loss = 0.68334778\n",
      "Iteration 40, loss = 0.68288541\n",
      "Iteration 41, loss = 0.68242278\n",
      "Iteration 42, loss = 0.68195996\n",
      "Iteration 43, loss = 0.68149699\n",
      "Iteration 44, loss = 0.68103393\n",
      "Iteration 45, loss = 0.68057082\n",
      "Iteration 46, loss = 0.68010768\n",
      "Iteration 47, loss = 0.67964455\n",
      "Iteration 48, loss = 0.67918145\n",
      "Iteration 49, loss = 0.67871840\n",
      "Iteration 50, loss = 0.67825543\n",
      "Iteration 51, loss = 0.67779254\n",
      "Iteration 52, loss = 0.67732975\n",
      "Iteration 53, loss = 0.67686706\n",
      "Iteration 54, loss = 0.67640449\n",
      "Iteration 55, loss = 0.67594204\n",
      "Iteration 56, loss = 0.67547972\n",
      "Iteration 57, loss = 0.67501751\n",
      "Iteration 58, loss = 0.67455543\n",
      "Iteration 59, loss = 0.67409347\n",
      "Iteration 60, loss = 0.67363164\n",
      "Iteration 61, loss = 0.67316992\n",
      "Iteration 62, loss = 0.67270831\n",
      "Iteration 63, loss = 0.67224681\n",
      "Iteration 64, loss = 0.67178542\n",
      "Iteration 65, loss = 0.67132411\n",
      "Iteration 66, loss = 0.67086290\n",
      "Iteration 67, loss = 0.67040177\n",
      "Iteration 68, loss = 0.66994071\n",
      "Iteration 69, loss = 0.66947971\n",
      "Iteration 70, loss = 0.66901878\n",
      "Iteration 71, loss = 0.66855788\n",
      "Iteration 72, loss = 0.66809703\n",
      "Iteration 73, loss = 0.66763620\n",
      "Iteration 74, loss = 0.66717538\n",
      "Iteration 75, loss = 0.66671458\n",
      "Iteration 76, loss = 0.66625377\n",
      "Iteration 77, loss = 0.66579295\n",
      "Iteration 78, loss = 0.66533211\n",
      "Iteration 79, loss = 0.66487123\n",
      "Iteration 80, loss = 0.66441031\n",
      "Iteration 81, loss = 0.66394933\n",
      "Iteration 82, loss = 0.66348828\n",
      "Iteration 83, loss = 0.66302716\n",
      "Iteration 84, loss = 0.66256595\n",
      "Iteration 85, loss = 0.66210465\n",
      "Iteration 86, loss = 0.66164323\n",
      "Iteration 87, loss = 0.66118169\n",
      "Iteration 88, loss = 0.66072002\n",
      "Iteration 89, loss = 0.66025821\n",
      "Iteration 90, loss = 0.65979625\n",
      "Iteration 91, loss = 0.65933412\n",
      "Iteration 92, loss = 0.65887182\n",
      "Iteration 93, loss = 0.65840933\n",
      "Iteration 94, loss = 0.65794664\n",
      "Iteration 95, loss = 0.65748375\n",
      "Iteration 96, loss = 0.65702064\n",
      "Iteration 97, loss = 0.65655731\n",
      "Iteration 98, loss = 0.65609373\n",
      "Iteration 99, loss = 0.65562991\n",
      "Iteration 100, loss = 0.65516582\n",
      "Iteration 101, loss = 0.65470147\n",
      "Iteration 102, loss = 0.65423683\n",
      "Iteration 103, loss = 0.65377190\n",
      "Iteration 104, loss = 0.65330667\n",
      "Iteration 105, loss = 0.65284113\n",
      "Iteration 106, loss = 0.65237526\n",
      "Iteration 107, loss = 0.65190907\n",
      "Iteration 108, loss = 0.65144253\n",
      "Iteration 109, loss = 0.65097563\n",
      "Iteration 110, loss = 0.65050837\n",
      "Iteration 111, loss = 0.65004074\n",
      "Iteration 112, loss = 0.64957273\n",
      "Iteration 113, loss = 0.64910432\n",
      "Iteration 114, loss = 0.64863551\n",
      "Iteration 115, loss = 0.64816629\n",
      "Iteration 116, loss = 0.64769665\n",
      "Iteration 117, loss = 0.64722657\n",
      "Iteration 118, loss = 0.64675605\n",
      "Iteration 119, loss = 0.64628507\n",
      "Iteration 120, loss = 0.64581364\n",
      "Iteration 121, loss = 0.64534173\n",
      "Iteration 122, loss = 0.64486935\n",
      "Iteration 123, loss = 0.64439647\n",
      "Iteration 124, loss = 0.64392309\n",
      "Iteration 125, loss = 0.64344920\n",
      "Iteration 126, loss = 0.64297480\n",
      "Iteration 127, loss = 0.64249987\n",
      "Iteration 128, loss = 0.64202440\n",
      "Iteration 129, loss = 0.64154838\n",
      "Iteration 130, loss = 0.64107181\n",
      "Iteration 131, loss = 0.64059467\n",
      "Iteration 132, loss = 0.64011696\n",
      "Iteration 133, loss = 0.63963867\n",
      "Iteration 134, loss = 0.63915978\n",
      "Iteration 135, loss = 0.63868030\n",
      "Iteration 136, loss = 0.63820020\n",
      "Iteration 137, loss = 0.63771949\n",
      "Iteration 138, loss = 0.63723815\n",
      "Iteration 139, loss = 0.63675617\n",
      "Iteration 140, loss = 0.63627355\n",
      "Iteration 141, loss = 0.63579028\n",
      "Iteration 142, loss = 0.63530634\n",
      "Iteration 143, loss = 0.63482173\n",
      "Iteration 144, loss = 0.63433645\n",
      "Iteration 145, loss = 0.63385047\n",
      "Iteration 146, loss = 0.63336380\n",
      "Iteration 147, loss = 0.63287643\n",
      "Iteration 148, loss = 0.63238834\n",
      "Iteration 149, loss = 0.63189953\n",
      "Iteration 150, loss = 0.63141000\n",
      "Iteration 151, loss = 0.63091972\n",
      "Iteration 152, loss = 0.63042870\n",
      "Iteration 153, loss = 0.62993692\n",
      "Iteration 154, loss = 0.62944439\n",
      "Iteration 155, loss = 0.62895108\n",
      "Iteration 156, loss = 0.62845699\n",
      "Iteration 157, loss = 0.62796212\n",
      "Iteration 158, loss = 0.62746646\n",
      "Iteration 159, loss = 0.62696999\n",
      "Iteration 160, loss = 0.62647271\n",
      "Iteration 161, loss = 0.62597462\n",
      "Iteration 162, loss = 0.62547570\n",
      "Iteration 163, loss = 0.62497595\n",
      "Iteration 164, loss = 0.62447535\n",
      "Iteration 165, loss = 0.62397391\n",
      "Iteration 166, loss = 0.62347161\n",
      "Iteration 167, loss = 0.62296845\n",
      "Iteration 168, loss = 0.62246442\n",
      "Iteration 169, loss = 0.62195951\n",
      "Iteration 170, loss = 0.62145372\n",
      "Iteration 171, loss = 0.62094703\n",
      "Iteration 172, loss = 0.62043944\n",
      "Iteration 173, loss = 0.61993095\n",
      "Iteration 174, loss = 0.61942154\n",
      "Iteration 175, loss = 0.61891121\n",
      "Iteration 176, loss = 0.61839994\n",
      "Iteration 177, loss = 0.61788775\n",
      "Iteration 178, loss = 0.61737461\n",
      "Iteration 179, loss = 0.61686052\n",
      "Iteration 180, loss = 0.61634547\n",
      "Iteration 181, loss = 0.61582946\n",
      "Iteration 182, loss = 0.61531248\n",
      "Iteration 183, loss = 0.61479452\n",
      "Iteration 184, loss = 0.61427557\n",
      "Iteration 185, loss = 0.61375564\n",
      "Iteration 186, loss = 0.61323471\n",
      "Iteration 187, loss = 0.61271277\n",
      "Iteration 188, loss = 0.61218982\n",
      "Iteration 189, loss = 0.61166586\n",
      "Iteration 190, loss = 0.61114087\n",
      "Iteration 191, loss = 0.61061485\n",
      "Iteration 192, loss = 0.61008779\n",
      "Iteration 193, loss = 0.60955969\n",
      "Iteration 194, loss = 0.60903054\n",
      "Iteration 195, loss = 0.60850034\n",
      "Iteration 196, loss = 0.60796907\n",
      "Iteration 197, loss = 0.60743674\n",
      "Iteration 198, loss = 0.60690333\n",
      "Iteration 199, loss = 0.60636884\n",
      "Iteration 200, loss = 0.60583326\n",
      "Iteration 201, loss = 0.60529660\n",
      "Iteration 202, loss = 0.60475883\n",
      "Iteration 203, loss = 0.60421996\n",
      "Iteration 204, loss = 0.60367998\n",
      "Iteration 205, loss = 0.60313889\n",
      "Iteration 206, loss = 0.60259667\n",
      "Iteration 207, loss = 0.60205333\n",
      "Iteration 208, loss = 0.60150886\n",
      "Iteration 209, loss = 0.60096325\n",
      "Iteration 210, loss = 0.60041649\n",
      "Iteration 211, loss = 0.59986859\n",
      "Iteration 212, loss = 0.59931953\n",
      "Iteration 213, loss = 0.59876931\n",
      "Iteration 214, loss = 0.59821793\n",
      "Iteration 215, loss = 0.59766537\n",
      "Iteration 216, loss = 0.59711164\n",
      "Iteration 217, loss = 0.59655673\n",
      "Iteration 218, loss = 0.59600064\n",
      "Iteration 219, loss = 0.59544335\n",
      "Iteration 220, loss = 0.59488487\n",
      "Iteration 221, loss = 0.59432519\n",
      "Iteration 222, loss = 0.59376431\n",
      "Iteration 223, loss = 0.59320221\n",
      "Iteration 224, loss = 0.59263890\n",
      "Iteration 225, loss = 0.59207437\n",
      "Iteration 226, loss = 0.59150861\n",
      "Iteration 227, loss = 0.59094163\n",
      "Iteration 228, loss = 0.59037341\n",
      "Iteration 229, loss = 0.58980395\n",
      "Iteration 230, loss = 0.58923326\n",
      "Iteration 231, loss = 0.58866131\n",
      "Iteration 232, loss = 0.58808812\n",
      "Iteration 233, loss = 0.58751367\n",
      "Iteration 234, loss = 0.58693796\n",
      "Iteration 235, loss = 0.58636099\n",
      "Iteration 236, loss = 0.58578275\n",
      "Iteration 237, loss = 0.58520324\n",
      "Iteration 238, loss = 0.58462245\n",
      "Iteration 239, loss = 0.58404039\n",
      "Iteration 240, loss = 0.58345704\n",
      "Iteration 241, loss = 0.58287240\n",
      "Iteration 242, loss = 0.58228648\n",
      "Iteration 243, loss = 0.58169926\n",
      "Iteration 244, loss = 0.58111074\n",
      "Iteration 245, loss = 0.58052093\n",
      "Iteration 246, loss = 0.57992980\n",
      "Iteration 247, loss = 0.57933738\n",
      "Iteration 248, loss = 0.57874363\n",
      "Iteration 249, loss = 0.57814858\n",
      "Iteration 250, loss = 0.57755221\n",
      "Iteration 251, loss = 0.57695451\n",
      "Iteration 252, loss = 0.57635550\n",
      "Iteration 253, loss = 0.57575515\n",
      "Iteration 254, loss = 0.57515348\n",
      "Iteration 255, loss = 0.57455048\n",
      "Iteration 256, loss = 0.57394614\n",
      "Iteration 257, loss = 0.57334048\n",
      "Iteration 258, loss = 0.57273351\n",
      "Iteration 259, loss = 0.57212529\n",
      "Iteration 260, loss = 0.57151599\n",
      "Iteration 261, loss = 0.57090618\n",
      "Iteration 262, loss = 0.57029757\n",
      "Iteration 263, loss = 0.56969544\n",
      "Iteration 264, loss = 0.56911641\n",
      "Iteration 265, loss = 0.56861296\n",
      "Iteration 266, loss = 0.56835258\n",
      "Iteration 267, loss = 0.56887467\n",
      "Iteration 268, loss = 0.57191127\n",
      "Iteration 269, loss = 0.58299691\n",
      "Iteration 270, loss = 0.61826008\n",
      "Iteration 271, loss = 0.71525280\n",
      "Iteration 272, loss = 0.89109570\n",
      "Iteration 273, loss = 1.02741174\n",
      "Iteration 274, loss = 0.99443120\n",
      "Iteration 275, loss = 0.99071313\n",
      "Iteration 276, loss = 0.97869139\n",
      "Iteration 277, loss = 0.97723234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963462\n",
      "Iteration 2, loss = 0.70131678\n",
      "Iteration 3, loss = 0.69835442\n",
      "Iteration 4, loss = 0.69763075\n",
      "Iteration 5, loss = 0.69699666\n",
      "Iteration 6, loss = 0.69657976\n",
      "Iteration 7, loss = 0.69622080\n",
      "Iteration 8, loss = 0.69588761\n",
      "Iteration 9, loss = 0.69555628\n",
      "Iteration 10, loss = 0.69521765\n",
      "Iteration 11, loss = 0.69486772\n",
      "Iteration 12, loss = 0.69450532\n",
      "Iteration 13, loss = 0.69413061\n",
      "Iteration 14, loss = 0.69374430\n",
      "Iteration 15, loss = 0.69334732\n",
      "Iteration 16, loss = 0.69294068\n",
      "Iteration 17, loss = 0.69252533\n",
      "Iteration 18, loss = 0.69210217\n",
      "Iteration 19, loss = 0.69167205\n",
      "Iteration 20, loss = 0.69123571\n",
      "Iteration 21, loss = 0.69079383\n",
      "Iteration 22, loss = 0.69034703\n",
      "Iteration 23, loss = 0.68989587\n",
      "Iteration 24, loss = 0.68944084\n",
      "Iteration 25, loss = 0.68898238\n",
      "Iteration 26, loss = 0.68852090\n",
      "Iteration 27, loss = 0.68805675\n",
      "Iteration 28, loss = 0.68759025\n",
      "Iteration 29, loss = 0.68712170\n",
      "Iteration 30, loss = 0.68665134\n",
      "Iteration 31, loss = 0.68617940\n",
      "Iteration 32, loss = 0.68570610\n",
      "Iteration 33, loss = 0.68523162\n",
      "Iteration 34, loss = 0.68475610\n",
      "Iteration 35, loss = 0.68427972\n",
      "Iteration 36, loss = 0.68380258\n",
      "Iteration 37, loss = 0.68332480\n",
      "Iteration 38, loss = 0.68284650\n",
      "Iteration 39, loss = 0.68236774\n",
      "Iteration 40, loss = 0.68188863\n",
      "Iteration 41, loss = 0.68140922\n",
      "Iteration 42, loss = 0.68092957\n",
      "Iteration 43, loss = 0.68044974\n",
      "Iteration 44, loss = 0.67996978\n",
      "Iteration 45, loss = 0.67948973\n",
      "Iteration 46, loss = 0.67900961\n",
      "Iteration 47, loss = 0.67852947\n",
      "Iteration 48, loss = 0.67804933\n",
      "Iteration 49, loss = 0.67756920\n",
      "Iteration 50, loss = 0.67708911\n",
      "Iteration 51, loss = 0.67660907\n",
      "Iteration 52, loss = 0.67612909\n",
      "Iteration 53, loss = 0.67564918\n",
      "Iteration 54, loss = 0.67516935\n",
      "Iteration 55, loss = 0.67468961\n",
      "Iteration 56, loss = 0.67420995\n",
      "Iteration 57, loss = 0.67373037\n",
      "Iteration 58, loss = 0.67325089\n",
      "Iteration 59, loss = 0.67277148\n",
      "Iteration 60, loss = 0.67229216\n",
      "Iteration 61, loss = 0.67181292\n",
      "Iteration 62, loss = 0.67133375\n",
      "Iteration 63, loss = 0.67085465\n",
      "Iteration 64, loss = 0.67037560\n",
      "Iteration 65, loss = 0.66989661\n",
      "Iteration 66, loss = 0.66941767\n",
      "Iteration 67, loss = 0.66893876\n",
      "Iteration 68, loss = 0.66845988\n",
      "Iteration 69, loss = 0.66798102\n",
      "Iteration 70, loss = 0.66750217\n",
      "Iteration 71, loss = 0.66702331\n",
      "Iteration 72, loss = 0.66654445\n",
      "Iteration 73, loss = 0.66606556\n",
      "Iteration 74, loss = 0.66558664\n",
      "Iteration 75, loss = 0.66510768\n",
      "Iteration 76, loss = 0.66462866\n",
      "Iteration 77, loss = 0.66414958\n",
      "Iteration 78, loss = 0.66367042\n",
      "Iteration 79, loss = 0.66319117\n",
      "Iteration 80, loss = 0.66271183\n",
      "Iteration 81, loss = 0.66223237\n",
      "Iteration 82, loss = 0.66175278\n",
      "Iteration 83, loss = 0.66127307\n",
      "Iteration 84, loss = 0.66079320\n",
      "Iteration 85, loss = 0.66031318\n",
      "Iteration 86, loss = 0.65983299\n",
      "Iteration 87, loss = 0.65935261\n",
      "Iteration 88, loss = 0.65887205\n",
      "Iteration 89, loss = 0.65839127\n",
      "Iteration 90, loss = 0.65791028\n",
      "Iteration 91, loss = 0.65742906\n",
      "Iteration 92, loss = 0.65694760\n",
      "Iteration 93, loss = 0.65646589\n",
      "Iteration 94, loss = 0.65598392\n",
      "Iteration 95, loss = 0.65550166\n",
      "Iteration 96, loss = 0.65501912\n",
      "Iteration 97, loss = 0.65453629\n",
      "Iteration 98, loss = 0.65405314\n",
      "Iteration 99, loss = 0.65356967\n",
      "Iteration 100, loss = 0.65308586\n",
      "Iteration 101, loss = 0.65260172\n",
      "Iteration 102, loss = 0.65211721\n",
      "Iteration 103, loss = 0.65163234\n",
      "Iteration 104, loss = 0.65114709\n",
      "Iteration 105, loss = 0.65066145\n",
      "Iteration 106, loss = 0.65017541\n",
      "Iteration 107, loss = 0.64968895\n",
      "Iteration 108, loss = 0.64920208\n",
      "Iteration 109, loss = 0.64871477\n",
      "Iteration 110, loss = 0.64822701\n",
      "Iteration 111, loss = 0.64773879\n",
      "Iteration 112, loss = 0.64725011\n",
      "Iteration 113, loss = 0.64676096\n",
      "Iteration 114, loss = 0.64627131\n",
      "Iteration 115, loss = 0.64578116\n",
      "Iteration 116, loss = 0.64529050\n",
      "Iteration 117, loss = 0.64479932\n",
      "Iteration 118, loss = 0.64430761\n",
      "Iteration 119, loss = 0.64381536\n",
      "Iteration 120, loss = 0.64332256\n",
      "Iteration 121, loss = 0.64282919\n",
      "Iteration 122, loss = 0.64233525\n",
      "Iteration 123, loss = 0.64184072\n",
      "Iteration 124, loss = 0.64134560\n",
      "Iteration 125, loss = 0.64084987\n",
      "Iteration 126, loss = 0.64035353\n",
      "Iteration 127, loss = 0.63985657\n",
      "Iteration 128, loss = 0.63935897\n",
      "Iteration 129, loss = 0.63886072\n",
      "Iteration 130, loss = 0.63836182\n",
      "Iteration 131, loss = 0.63786226\n",
      "Iteration 132, loss = 0.63736202\n",
      "Iteration 133, loss = 0.63686109\n",
      "Iteration 134, loss = 0.63635947\n",
      "Iteration 135, loss = 0.63585715\n",
      "Iteration 136, loss = 0.63535411\n",
      "Iteration 137, loss = 0.63485035\n",
      "Iteration 138, loss = 0.63434585\n",
      "Iteration 139, loss = 0.63384061\n",
      "Iteration 140, loss = 0.63333461\n",
      "Iteration 141, loss = 0.63282786\n",
      "Iteration 142, loss = 0.63232033\n",
      "Iteration 143, loss = 0.63181202\n",
      "Iteration 144, loss = 0.63130292\n",
      "Iteration 145, loss = 0.63079302\n",
      "Iteration 146, loss = 0.63028232\n",
      "Iteration 147, loss = 0.62977079\n",
      "Iteration 148, loss = 0.62925843\n",
      "Iteration 149, loss = 0.62874524\n",
      "Iteration 150, loss = 0.62823120\n",
      "Iteration 151, loss = 0.62771631\n",
      "Iteration 152, loss = 0.62720056\n",
      "Iteration 153, loss = 0.62668393\n",
      "Iteration 154, loss = 0.62616641\n",
      "Iteration 155, loss = 0.62564801\n",
      "Iteration 156, loss = 0.62512871\n",
      "Iteration 157, loss = 0.62460850\n",
      "Iteration 158, loss = 0.62408738\n",
      "Iteration 159, loss = 0.62356532\n",
      "Iteration 160, loss = 0.62304234\n",
      "Iteration 161, loss = 0.62251841\n",
      "Iteration 162, loss = 0.62199353\n",
      "Iteration 163, loss = 0.62146769\n",
      "Iteration 164, loss = 0.62094088\n",
      "Iteration 165, loss = 0.62041309\n",
      "Iteration 166, loss = 0.61988432\n",
      "Iteration 167, loss = 0.61935456\n",
      "Iteration 168, loss = 0.61882379\n",
      "Iteration 169, loss = 0.61829201\n",
      "Iteration 170, loss = 0.61775922\n",
      "Iteration 171, loss = 0.61722540\n",
      "Iteration 172, loss = 0.61669054\n",
      "Iteration 173, loss = 0.61615465\n",
      "Iteration 174, loss = 0.61561770\n",
      "Iteration 175, loss = 0.61507970\n",
      "Iteration 176, loss = 0.61454063\n",
      "Iteration 177, loss = 0.61400048\n",
      "Iteration 178, loss = 0.61345925\n",
      "Iteration 179, loss = 0.61291694\n",
      "Iteration 180, loss = 0.61237353\n",
      "Iteration 181, loss = 0.61182901\n",
      "Iteration 182, loss = 0.61128338\n",
      "Iteration 183, loss = 0.61073663\n",
      "Iteration 184, loss = 0.61018875\n",
      "Iteration 185, loss = 0.60963974\n",
      "Iteration 186, loss = 0.60908959\n",
      "Iteration 187, loss = 0.60853829\n",
      "Iteration 188, loss = 0.60798583\n",
      "Iteration 189, loss = 0.60743220\n",
      "Iteration 190, loss = 0.60687741\n",
      "Iteration 191, loss = 0.60632144\n",
      "Iteration 192, loss = 0.60576428\n",
      "Iteration 193, loss = 0.60520593\n",
      "Iteration 194, loss = 0.60464638\n",
      "Iteration 195, loss = 0.60408562\n",
      "Iteration 196, loss = 0.60352366\n",
      "Iteration 197, loss = 0.60296047\n",
      "Iteration 198, loss = 0.60239605\n",
      "Iteration 199, loss = 0.60183041\n",
      "Iteration 200, loss = 0.60126352\n",
      "Iteration 201, loss = 0.60069538\n",
      "Iteration 202, loss = 0.60012600\n",
      "Iteration 203, loss = 0.59955535\n",
      "Iteration 204, loss = 0.59898344\n",
      "Iteration 205, loss = 0.59841025\n",
      "Iteration 206, loss = 0.59783579\n",
      "Iteration 207, loss = 0.59726004\n",
      "Iteration 208, loss = 0.59668300\n",
      "Iteration 209, loss = 0.59610467\n",
      "Iteration 210, loss = 0.59552503\n",
      "Iteration 211, loss = 0.59494408\n",
      "Iteration 212, loss = 0.59436181\n",
      "Iteration 213, loss = 0.59377823\n",
      "Iteration 214, loss = 0.59319332\n",
      "Iteration 215, loss = 0.59260707\n",
      "Iteration 216, loss = 0.59201949\n",
      "Iteration 217, loss = 0.59143056\n",
      "Iteration 218, loss = 0.59084029\n",
      "Iteration 219, loss = 0.59024866\n",
      "Iteration 220, loss = 0.58965566\n",
      "Iteration 221, loss = 0.58906131\n",
      "Iteration 222, loss = 0.58846558\n",
      "Iteration 223, loss = 0.58786847\n",
      "Iteration 224, loss = 0.58726998\n",
      "Iteration 225, loss = 0.58667011\n",
      "Iteration 226, loss = 0.58606884\n",
      "Iteration 227, loss = 0.58546618\n",
      "Iteration 228, loss = 0.58486211\n",
      "Iteration 229, loss = 0.58425664\n",
      "Iteration 230, loss = 0.58364975\n",
      "Iteration 231, loss = 0.58304145\n",
      "Iteration 232, loss = 0.58243173\n",
      "Iteration 233, loss = 0.58182058\n",
      "Iteration 234, loss = 0.58120801\n",
      "Iteration 235, loss = 0.58059399\n",
      "Iteration 236, loss = 0.57997854\n",
      "Iteration 237, loss = 0.57936165\n",
      "Iteration 238, loss = 0.57874330\n",
      "Iteration 239, loss = 0.57812351\n",
      "Iteration 240, loss = 0.57750226\n",
      "Iteration 241, loss = 0.57687955\n",
      "Iteration 242, loss = 0.57625537\n",
      "Iteration 243, loss = 0.57562973\n",
      "Iteration 244, loss = 0.57500262\n",
      "Iteration 245, loss = 0.57437403\n",
      "Iteration 246, loss = 0.57374396\n",
      "Iteration 247, loss = 0.57311241\n",
      "Iteration 248, loss = 0.57247937\n",
      "Iteration 249, loss = 0.57184484\n",
      "Iteration 250, loss = 0.57120882\n",
      "Iteration 251, loss = 0.57057131\n",
      "Iteration 252, loss = 0.56993229\n",
      "Iteration 253, loss = 0.56929178\n",
      "Iteration 254, loss = 0.56864975\n",
      "Iteration 255, loss = 0.56800622\n",
      "Iteration 256, loss = 0.56736118\n",
      "Iteration 257, loss = 0.56671462\n",
      "Iteration 258, loss = 0.56606655\n",
      "Iteration 259, loss = 0.56541696\n",
      "Iteration 260, loss = 0.56476586\n",
      "Iteration 261, loss = 0.56411325\n",
      "Iteration 262, loss = 0.56345920\n",
      "Iteration 263, loss = 0.56280388\n",
      "Iteration 264, loss = 0.56214791\n",
      "Iteration 265, loss = 0.56149338\n",
      "Iteration 266, loss = 0.56084744\n",
      "Iteration 267, loss = 0.56023498\n",
      "Iteration 268, loss = 0.55974292\n",
      "Iteration 269, loss = 0.55967855\n",
      "Iteration 270, loss = 0.56113388\n",
      "Iteration 271, loss = 0.56797580\n",
      "Iteration 272, loss = 0.59338856\n",
      "Iteration 273, loss = 0.67531491\n",
      "Iteration 274, loss = 0.86774467\n",
      "Iteration 275, loss = 1.06151118\n",
      "Iteration 276, loss = 1.04965719\n",
      "Iteration 277, loss = 1.01686717\n",
      "Iteration 278, loss = 1.02415994\n",
      "Iteration 279, loss = 1.01028852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69820099\n",
      "Iteration 2, loss = 8.71040722\n",
      "Iteration 3, loss = 2.56370765\n",
      "Iteration 4, loss = 4.63636121\n",
      "Iteration 5, loss = 6.54279853\n",
      "Iteration 6, loss = 5.62768817\n",
      "Iteration 7, loss = 2.98945791\n",
      "Iteration 8, loss = 0.98114844\n",
      "Iteration 9, loss = 2.93266542\n",
      "Iteration 10, loss = 3.21781666\n",
      "Iteration 11, loss = 2.11949657\n",
      "Iteration 12, loss = 0.69329795\n",
      "Iteration 13, loss = 1.91234883\n",
      "Iteration 14, loss = 2.36130836\n",
      "Iteration 15, loss = 1.69966171\n",
      "Iteration 16, loss = 0.69390726\n",
      "Iteration 17, loss = 1.48363711\n",
      "Iteration 18, loss = 1.84925671\n",
      "Iteration 19, loss = 1.29588779\n",
      "Iteration 20, loss = 0.69916097\n",
      "Iteration 21, loss = 1.30205340\n",
      "Iteration 22, loss = 1.43349658\n",
      "Iteration 23, loss = 0.90947313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70059838\n",
      "Iteration 2, loss = 8.60473519\n",
      "Iteration 3, loss = 2.60725672\n",
      "Iteration 4, loss = 4.58196504\n",
      "Iteration 5, loss = 6.40164919\n",
      "Iteration 6, loss = 5.36104485\n",
      "Iteration 7, loss = 2.58893673\n",
      "Iteration 8, loss = 1.37573414\n",
      "Iteration 9, loss = 3.14270172\n",
      "Iteration 10, loss = 3.25037383\n",
      "Iteration 11, loss = 2.06103992\n",
      "Iteration 12, loss = 0.70613391\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963462\n",
      "Iteration 2, loss = 8.65537847\n",
      "Iteration 3, loss = 2.57792516\n",
      "Iteration 4, loss = 4.61735436\n",
      "Iteration 5, loss = 6.48325209\n",
      "Iteration 6, loss = 5.50989018\n",
      "Iteration 7, loss = 2.80951305\n",
      "Iteration 8, loss = 1.14930042\n",
      "Iteration 9, loss = 2.99281813\n",
      "Iteration 10, loss = 3.17199454\n",
      "Iteration 11, loss = 2.01809128\n",
      "Iteration 12, loss = 0.70996147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71479195\n",
      "Iteration 2, loss = 0.70128048\n",
      "Iteration 3, loss = 0.70109006\n",
      "Iteration 4, loss = 0.69821705\n",
      "Iteration 5, loss = 0.69793928\n",
      "Iteration 6, loss = 0.69776768\n",
      "Iteration 7, loss = 0.69755335\n",
      "Iteration 8, loss = 0.69734943\n",
      "Iteration 9, loss = 0.69713150\n",
      "Iteration 10, loss = 0.69689983\n",
      "Iteration 11, loss = 0.69665632\n",
      "Iteration 12, loss = 0.69640197\n",
      "Iteration 13, loss = 0.69613789\n",
      "Iteration 14, loss = 0.69586510\n",
      "Iteration 15, loss = 0.69558453\n",
      "Iteration 16, loss = 0.69529701\n",
      "Iteration 17, loss = 0.69500326\n",
      "Iteration 18, loss = 0.69470396\n",
      "Iteration 19, loss = 0.69439971\n",
      "Iteration 20, loss = 0.69409104\n",
      "Iteration 21, loss = 0.69377844\n",
      "Iteration 22, loss = 0.69346234\n",
      "Iteration 23, loss = 0.69314314\n",
      "Iteration 24, loss = 0.69282117\n",
      "Iteration 25, loss = 0.69249675\n",
      "Iteration 26, loss = 0.69217017\n",
      "Iteration 27, loss = 0.69184166\n",
      "Iteration 28, loss = 0.69151147\n",
      "Iteration 29, loss = 0.69117978\n",
      "Iteration 30, loss = 0.69084679\n",
      "Iteration 31, loss = 0.69051264\n",
      "Iteration 32, loss = 0.69017748\n",
      "Iteration 33, loss = 0.68984144\n",
      "Iteration 34, loss = 0.68950463\n",
      "Iteration 35, loss = 0.68916715\n",
      "Iteration 36, loss = 0.68882910\n",
      "Iteration 37, loss = 0.68849055\n",
      "Iteration 38, loss = 0.68815157\n",
      "Iteration 39, loss = 0.68781223\n",
      "Iteration 40, loss = 0.68747257\n",
      "Iteration 41, loss = 0.68713265\n",
      "Iteration 42, loss = 0.68679251\n",
      "Iteration 43, loss = 0.68645218\n",
      "Iteration 44, loss = 0.68611170\n",
      "Iteration 45, loss = 0.68577109\n",
      "Iteration 46, loss = 0.68543038\n",
      "Iteration 47, loss = 0.68508958\n",
      "Iteration 48, loss = 0.68474872\n",
      "Iteration 49, loss = 0.68440781\n",
      "Iteration 50, loss = 0.68406686\n",
      "Iteration 51, loss = 0.68372587\n",
      "Iteration 52, loss = 0.68338486\n",
      "Iteration 53, loss = 0.68304382\n",
      "Iteration 54, loss = 0.68270278\n",
      "Iteration 55, loss = 0.68236171\n",
      "Iteration 56, loss = 0.68202063\n",
      "Iteration 57, loss = 0.68167954\n",
      "Iteration 58, loss = 0.68133843\n",
      "Iteration 59, loss = 0.68099730\n",
      "Iteration 60, loss = 0.68065616\n",
      "Iteration 61, loss = 0.68031498\n",
      "Iteration 62, loss = 0.67997377\n",
      "Iteration 63, loss = 0.67963253\n",
      "Iteration 64, loss = 0.67929124\n",
      "Iteration 65, loss = 0.67894990\n",
      "Iteration 66, loss = 0.67860851\n",
      "Iteration 67, loss = 0.67826705\n",
      "Iteration 68, loss = 0.67792553\n",
      "Iteration 69, loss = 0.67758392\n",
      "Iteration 70, loss = 0.67724223\n",
      "Iteration 71, loss = 0.67690043\n",
      "Iteration 72, loss = 0.67655854\n",
      "Iteration 73, loss = 0.67621653\n",
      "Iteration 74, loss = 0.67587440\n",
      "Iteration 75, loss = 0.67553213\n",
      "Iteration 76, loss = 0.67518973\n",
      "Iteration 77, loss = 0.67484717\n",
      "Iteration 78, loss = 0.67450445\n",
      "Iteration 79, loss = 0.67416156\n",
      "Iteration 80, loss = 0.67381850\n",
      "Iteration 81, loss = 0.67347524\n",
      "Iteration 82, loss = 0.67313178\n",
      "Iteration 83, loss = 0.67278811\n",
      "Iteration 84, loss = 0.67244423\n",
      "Iteration 85, loss = 0.67210011\n",
      "Iteration 86, loss = 0.67175576\n",
      "Iteration 87, loss = 0.67141116\n",
      "Iteration 88, loss = 0.67106629\n",
      "Iteration 89, loss = 0.67072116\n",
      "Iteration 90, loss = 0.67037575\n",
      "Iteration 91, loss = 0.67003005\n",
      "Iteration 92, loss = 0.66968405\n",
      "Iteration 93, loss = 0.66933774\n",
      "Iteration 94, loss = 0.66899111\n",
      "Iteration 95, loss = 0.66864415\n",
      "Iteration 96, loss = 0.66829685\n",
      "Iteration 97, loss = 0.66794921\n",
      "Iteration 98, loss = 0.66760120\n",
      "Iteration 99, loss = 0.66725282\n",
      "Iteration 100, loss = 0.66690407\n",
      "Iteration 101, loss = 0.66655492\n",
      "Iteration 102, loss = 0.66620538\n",
      "Iteration 103, loss = 0.66585543\n",
      "Iteration 104, loss = 0.66550505\n",
      "Iteration 105, loss = 0.66515425\n",
      "Iteration 106, loss = 0.66480301\n",
      "Iteration 107, loss = 0.66445133\n",
      "Iteration 108, loss = 0.66409918\n",
      "Iteration 109, loss = 0.66374657\n",
      "Iteration 110, loss = 0.66339347\n",
      "Iteration 111, loss = 0.66303989\n",
      "Iteration 112, loss = 0.66268582\n",
      "Iteration 113, loss = 0.66233123\n",
      "Iteration 114, loss = 0.66197613\n",
      "Iteration 115, loss = 0.66162050\n",
      "Iteration 116, loss = 0.66126434\n",
      "Iteration 117, loss = 0.66090763\n",
      "Iteration 118, loss = 0.66055037\n",
      "Iteration 119, loss = 0.66019254\n",
      "Iteration 120, loss = 0.65983413\n",
      "Iteration 121, loss = 0.65947514\n",
      "Iteration 122, loss = 0.65911556\n",
      "Iteration 123, loss = 0.65875538\n",
      "Iteration 124, loss = 0.65839458\n",
      "Iteration 125, loss = 0.65803316\n",
      "Iteration 126, loss = 0.65767111\n",
      "Iteration 127, loss = 0.65730841\n",
      "Iteration 128, loss = 0.65694507\n",
      "Iteration 129, loss = 0.65658106\n",
      "Iteration 130, loss = 0.65621639\n",
      "Iteration 131, loss = 0.65585104\n",
      "Iteration 132, loss = 0.65548500\n",
      "Iteration 133, loss = 0.65511826\n",
      "Iteration 134, loss = 0.65475082\n",
      "Iteration 135, loss = 0.65438266\n",
      "Iteration 136, loss = 0.65401377\n",
      "Iteration 137, loss = 0.65364415\n",
      "Iteration 138, loss = 0.65327379\n",
      "Iteration 139, loss = 0.65290267\n",
      "Iteration 140, loss = 0.65253079\n",
      "Iteration 141, loss = 0.65215814\n",
      "Iteration 142, loss = 0.65178471\n",
      "Iteration 143, loss = 0.65141049\n",
      "Iteration 144, loss = 0.65103546\n",
      "Iteration 145, loss = 0.65065963\n",
      "Iteration 146, loss = 0.65028299\n",
      "Iteration 147, loss = 0.64990551\n",
      "Iteration 148, loss = 0.64952720\n",
      "Iteration 149, loss = 0.64914805\n",
      "Iteration 150, loss = 0.64876804\n",
      "Iteration 151, loss = 0.64838717\n",
      "Iteration 152, loss = 0.64800542\n",
      "Iteration 153, loss = 0.64762280\n",
      "Iteration 154, loss = 0.64723929\n",
      "Iteration 155, loss = 0.64685487\n",
      "Iteration 156, loss = 0.64646955\n",
      "Iteration 157, loss = 0.64608331\n",
      "Iteration 158, loss = 0.64569615\n",
      "Iteration 159, loss = 0.64530805\n",
      "Iteration 160, loss = 0.64491900\n",
      "Iteration 161, loss = 0.64452901\n",
      "Iteration 162, loss = 0.64413805\n",
      "Iteration 163, loss = 0.64374612\n",
      "Iteration 164, loss = 0.64335321\n",
      "Iteration 165, loss = 0.64295931\n",
      "Iteration 166, loss = 0.64256442\n",
      "Iteration 167, loss = 0.64216852\n",
      "Iteration 168, loss = 0.64177161\n",
      "Iteration 169, loss = 0.64137367\n",
      "Iteration 170, loss = 0.64097470\n",
      "Iteration 171, loss = 0.64057468\n",
      "Iteration 172, loss = 0.64017362\n",
      "Iteration 173, loss = 0.63977150\n",
      "Iteration 174, loss = 0.63936831\n",
      "Iteration 175, loss = 0.63896405\n",
      "Iteration 176, loss = 0.63855870\n",
      "Iteration 177, loss = 0.63815226\n",
      "Iteration 178, loss = 0.63774471\n",
      "Iteration 179, loss = 0.63733605\n",
      "Iteration 180, loss = 0.63692628\n",
      "Iteration 181, loss = 0.63651537\n",
      "Iteration 182, loss = 0.63610333\n",
      "Iteration 183, loss = 0.63569015\n",
      "Iteration 184, loss = 0.63527580\n",
      "Iteration 185, loss = 0.63486030\n",
      "Iteration 186, loss = 0.63444363\n",
      "Iteration 187, loss = 0.63402577\n",
      "Iteration 188, loss = 0.63360673\n",
      "Iteration 189, loss = 0.63318649\n",
      "Iteration 190, loss = 0.63276505\n",
      "Iteration 191, loss = 0.63234239\n",
      "Iteration 192, loss = 0.63191851\n",
      "Iteration 193, loss = 0.63149339\n",
      "Iteration 194, loss = 0.63106704\n",
      "Iteration 195, loss = 0.63063945\n",
      "Iteration 196, loss = 0.63021059\n",
      "Iteration 197, loss = 0.62978047\n",
      "Iteration 198, loss = 0.62934908\n",
      "Iteration 199, loss = 0.62891641\n",
      "Iteration 200, loss = 0.62848245\n",
      "Iteration 201, loss = 0.62804719\n",
      "Iteration 202, loss = 0.62761063\n",
      "Iteration 203, loss = 0.62717275\n",
      "Iteration 204, loss = 0.62673355\n",
      "Iteration 205, loss = 0.62629301\n",
      "Iteration 206, loss = 0.62585114\n",
      "Iteration 207, loss = 0.62540792\n",
      "Iteration 208, loss = 0.62496335\n",
      "Iteration 209, loss = 0.62451741\n",
      "Iteration 210, loss = 0.62407010\n",
      "Iteration 211, loss = 0.62362142\n",
      "Iteration 212, loss = 0.62317134\n",
      "Iteration 213, loss = 0.62271987\n",
      "Iteration 214, loss = 0.62226699\n",
      "Iteration 215, loss = 0.62181270\n",
      "Iteration 216, loss = 0.62135700\n",
      "Iteration 217, loss = 0.62089986\n",
      "Iteration 218, loss = 0.62044129\n",
      "Iteration 219, loss = 0.61998127\n",
      "Iteration 220, loss = 0.61951981\n",
      "Iteration 221, loss = 0.61905688\n",
      "Iteration 222, loss = 0.61859248\n",
      "Iteration 223, loss = 0.61812661\n",
      "Iteration 224, loss = 0.61765926\n",
      "Iteration 225, loss = 0.61719042\n",
      "Iteration 226, loss = 0.61672007\n",
      "Iteration 227, loss = 0.61624822\n",
      "Iteration 228, loss = 0.61577486\n",
      "Iteration 229, loss = 0.61529998\n",
      "Iteration 230, loss = 0.61482356\n",
      "Iteration 231, loss = 0.61434561\n",
      "Iteration 232, loss = 0.61386611\n",
      "Iteration 233, loss = 0.61338506\n",
      "Iteration 234, loss = 0.61290245\n",
      "Iteration 235, loss = 0.61241828\n",
      "Iteration 236, loss = 0.61193253\n",
      "Iteration 237, loss = 0.61144519\n",
      "Iteration 238, loss = 0.61095627\n",
      "Iteration 239, loss = 0.61046575\n",
      "Iteration 240, loss = 0.60997362\n",
      "Iteration 241, loss = 0.60947988\n",
      "Iteration 242, loss = 0.60898453\n",
      "Iteration 243, loss = 0.60848754\n",
      "Iteration 244, loss = 0.60798893\n",
      "Iteration 245, loss = 0.60748867\n",
      "Iteration 246, loss = 0.60698677\n",
      "Iteration 247, loss = 0.60648321\n",
      "Iteration 248, loss = 0.60597798\n",
      "Iteration 249, loss = 0.60547109\n",
      "Iteration 250, loss = 0.60496252\n",
      "Iteration 251, loss = 0.60445227\n",
      "Iteration 252, loss = 0.60394033\n",
      "Iteration 253, loss = 0.60342669\n",
      "Iteration 254, loss = 0.60291135\n",
      "Iteration 255, loss = 0.60239430\n",
      "Iteration 256, loss = 0.60187553\n",
      "Iteration 257, loss = 0.60135503\n",
      "Iteration 258, loss = 0.60083281\n",
      "Iteration 259, loss = 0.60030884\n",
      "Iteration 260, loss = 0.59978314\n",
      "Iteration 261, loss = 0.59925568\n",
      "Iteration 262, loss = 0.59872646\n",
      "Iteration 263, loss = 0.59819548\n",
      "Iteration 264, loss = 0.59766273\n",
      "Iteration 265, loss = 0.59712821\n",
      "Iteration 266, loss = 0.59659190\n",
      "Iteration 267, loss = 0.59605380\n",
      "Iteration 268, loss = 0.59551391\n",
      "Iteration 269, loss = 0.59497222\n",
      "Iteration 270, loss = 0.59442872\n",
      "Iteration 271, loss = 0.59388340\n",
      "Iteration 272, loss = 0.59333627\n",
      "Iteration 273, loss = 0.59278731\n",
      "Iteration 274, loss = 0.59223651\n",
      "Iteration 275, loss = 0.59168388\n",
      "Iteration 276, loss = 0.59112941\n",
      "Iteration 277, loss = 0.59057309\n",
      "Iteration 278, loss = 0.59001491\n",
      "Iteration 279, loss = 0.58945488\n",
      "Iteration 280, loss = 0.58889297\n",
      "Iteration 281, loss = 0.58832920\n",
      "Iteration 282, loss = 0.58776355\n",
      "Iteration 283, loss = 0.58719602\n",
      "Iteration 284, loss = 0.58662660\n",
      "Iteration 285, loss = 0.58605528\n",
      "Iteration 286, loss = 0.58548207\n",
      "Iteration 287, loss = 0.58490696\n",
      "Iteration 288, loss = 0.58432993\n",
      "Iteration 289, loss = 0.58375100\n",
      "Iteration 290, loss = 0.58317014\n",
      "Iteration 291, loss = 0.58258736\n",
      "Iteration 292, loss = 0.58200266\n",
      "Iteration 293, loss = 0.58141602\n",
      "Iteration 294, loss = 0.58082744\n",
      "Iteration 295, loss = 0.58023692\n",
      "Iteration 296, loss = 0.57964446\n",
      "Iteration 297, loss = 0.57905004\n",
      "Iteration 298, loss = 0.57845367\n",
      "Iteration 299, loss = 0.57785534\n",
      "Iteration 300, loss = 0.57725504\n",
      "Iteration 1, loss = 0.71290991\n",
      "Iteration 2, loss = 0.69738686\n",
      "Iteration 3, loss = 0.69719208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.69391711\n",
      "Iteration 5, loss = 0.69364170\n",
      "Iteration 6, loss = 0.69349580\n",
      "Iteration 7, loss = 0.69330625\n",
      "Iteration 8, loss = 0.69313411\n",
      "Iteration 9, loss = 0.69295040\n",
      "Iteration 10, loss = 0.69275495\n",
      "Iteration 11, loss = 0.69254952\n",
      "Iteration 12, loss = 0.69233484\n",
      "Iteration 13, loss = 0.69211185\n",
      "Iteration 14, loss = 0.69188141\n",
      "Iteration 15, loss = 0.69164429\n",
      "Iteration 16, loss = 0.69140117\n",
      "Iteration 17, loss = 0.69115269\n",
      "Iteration 18, loss = 0.69089939\n",
      "Iteration 19, loss = 0.69064177\n",
      "Iteration 20, loss = 0.69038030\n",
      "Iteration 21, loss = 0.69011537\n",
      "Iteration 22, loss = 0.68984735\n",
      "Iteration 23, loss = 0.68957657\n",
      "Iteration 24, loss = 0.68930331\n",
      "Iteration 25, loss = 0.68902784\n",
      "Iteration 26, loss = 0.68875040\n",
      "Iteration 27, loss = 0.68847119\n",
      "Iteration 28, loss = 0.68819041\n",
      "Iteration 29, loss = 0.68790823\n",
      "Iteration 30, loss = 0.68762479\n",
      "Iteration 31, loss = 0.68734023\n",
      "Iteration 32, loss = 0.68705468\n",
      "Iteration 33, loss = 0.68676824\n",
      "Iteration 34, loss = 0.68648100\n",
      "Iteration 35, loss = 0.68619307\n",
      "Iteration 36, loss = 0.68590450\n",
      "Iteration 37, loss = 0.68561537\n",
      "Iteration 38, loss = 0.68532573\n",
      "Iteration 39, loss = 0.68503566\n",
      "Iteration 40, loss = 0.68474518\n",
      "Iteration 41, loss = 0.68445434\n",
      "Iteration 42, loss = 0.68416318\n",
      "Iteration 43, loss = 0.68387173\n",
      "Iteration 44, loss = 0.68358001\n",
      "Iteration 45, loss = 0.68328806\n",
      "Iteration 46, loss = 0.68299590\n",
      "Iteration 47, loss = 0.68270353\n",
      "Iteration 48, loss = 0.68241099\n",
      "Iteration 49, loss = 0.68211827\n",
      "Iteration 50, loss = 0.68182540\n",
      "Iteration 51, loss = 0.68153238\n",
      "Iteration 52, loss = 0.68123921\n",
      "Iteration 53, loss = 0.68094592\n",
      "Iteration 54, loss = 0.68065249\n",
      "Iteration 55, loss = 0.68035893\n",
      "Iteration 56, loss = 0.68006525\n",
      "Iteration 57, loss = 0.67977144\n",
      "Iteration 58, loss = 0.67947751\n",
      "Iteration 59, loss = 0.67918345\n",
      "Iteration 60, loss = 0.67888927\n",
      "Iteration 61, loss = 0.67859496\n",
      "Iteration 62, loss = 0.67830052\n",
      "Iteration 63, loss = 0.67800595\n",
      "Iteration 64, loss = 0.67771123\n",
      "Iteration 65, loss = 0.67741638\n",
      "Iteration 66, loss = 0.67712138\n",
      "Iteration 67, loss = 0.67682623\n",
      "Iteration 68, loss = 0.67653093\n",
      "Iteration 69, loss = 0.67623546\n",
      "Iteration 70, loss = 0.67593983\n",
      "Iteration 71, loss = 0.67564403\n",
      "Iteration 72, loss = 0.67534805\n",
      "Iteration 73, loss = 0.67505188\n",
      "Iteration 74, loss = 0.67475553\n",
      "Iteration 75, loss = 0.67445898\n",
      "Iteration 76, loss = 0.67416222\n",
      "Iteration 77, loss = 0.67386526\n",
      "Iteration 78, loss = 0.67356808\n",
      "Iteration 79, loss = 0.67327068\n",
      "Iteration 80, loss = 0.67297305\n",
      "Iteration 81, loss = 0.67267518\n",
      "Iteration 82, loss = 0.67237707\n",
      "Iteration 83, loss = 0.67207871\n",
      "Iteration 84, loss = 0.67178009\n",
      "Iteration 85, loss = 0.67148120\n",
      "Iteration 86, loss = 0.67118205\n",
      "Iteration 87, loss = 0.67088261\n",
      "Iteration 88, loss = 0.67058289\n",
      "Iteration 89, loss = 0.67028288\n",
      "Iteration 90, loss = 0.66998257\n",
      "Iteration 91, loss = 0.66968195\n",
      "Iteration 92, loss = 0.66938102\n",
      "Iteration 93, loss = 0.66907976\n",
      "Iteration 94, loss = 0.66877818\n",
      "Iteration 95, loss = 0.66847626\n",
      "Iteration 96, loss = 0.66817399\n",
      "Iteration 97, loss = 0.66787138\n",
      "Iteration 98, loss = 0.66756841\n",
      "Iteration 99, loss = 0.66726507\n",
      "Iteration 100, loss = 0.66696137\n",
      "Iteration 101, loss = 0.66665728\n",
      "Iteration 102, loss = 0.66635281\n",
      "Iteration 103, loss = 0.66604794\n",
      "Iteration 104, loss = 0.66574268\n",
      "Iteration 105, loss = 0.66543700\n",
      "Iteration 106, loss = 0.66513092\n",
      "Iteration 107, loss = 0.66482441\n",
      "Iteration 108, loss = 0.66451747\n",
      "Iteration 109, loss = 0.66421009\n",
      "Iteration 110, loss = 0.66390228\n",
      "Iteration 111, loss = 0.66359401\n",
      "Iteration 112, loss = 0.66328529\n",
      "Iteration 113, loss = 0.66297610\n",
      "Iteration 114, loss = 0.66266644\n",
      "Iteration 115, loss = 0.66235630\n",
      "Iteration 116, loss = 0.66204568\n",
      "Iteration 117, loss = 0.66173456\n",
      "Iteration 118, loss = 0.66142295\n",
      "Iteration 119, loss = 0.66111083\n",
      "Iteration 120, loss = 0.66079819\n",
      "Iteration 121, loss = 0.66048504\n",
      "Iteration 122, loss = 0.66017136\n",
      "Iteration 123, loss = 0.65985714\n",
      "Iteration 124, loss = 0.65954239\n",
      "Iteration 125, loss = 0.65922708\n",
      "Iteration 126, loss = 0.65891123\n",
      "Iteration 127, loss = 0.65859480\n",
      "Iteration 128, loss = 0.65827782\n",
      "Iteration 129, loss = 0.65796025\n",
      "Iteration 130, loss = 0.65764211\n",
      "Iteration 131, loss = 0.65732337\n",
      "Iteration 132, loss = 0.65700404\n",
      "Iteration 133, loss = 0.65668410\n",
      "Iteration 134, loss = 0.65636356\n",
      "Iteration 135, loss = 0.65604240\n",
      "Iteration 136, loss = 0.65572062\n",
      "Iteration 137, loss = 0.65539820\n",
      "Iteration 138, loss = 0.65507515\n",
      "Iteration 139, loss = 0.65475146\n",
      "Iteration 140, loss = 0.65442711\n",
      "Iteration 141, loss = 0.65410211\n",
      "Iteration 142, loss = 0.65377645\n",
      "Iteration 143, loss = 0.65345011\n",
      "Iteration 144, loss = 0.65312310\n",
      "Iteration 145, loss = 0.65279540\n",
      "Iteration 146, loss = 0.65246701\n",
      "Iteration 147, loss = 0.65213793\n",
      "Iteration 148, loss = 0.65180814\n",
      "Iteration 149, loss = 0.65147764\n",
      "Iteration 150, loss = 0.65114642\n",
      "Iteration 151, loss = 0.65081448\n",
      "Iteration 152, loss = 0.65048181\n",
      "Iteration 153, loss = 0.65014840\n",
      "Iteration 154, loss = 0.64981425\n",
      "Iteration 155, loss = 0.64947934\n",
      "Iteration 156, loss = 0.64914368\n",
      "Iteration 157, loss = 0.64880726\n",
      "Iteration 158, loss = 0.64847006\n",
      "Iteration 159, loss = 0.64813209\n",
      "Iteration 160, loss = 0.64779334\n",
      "Iteration 161, loss = 0.64745379\n",
      "Iteration 162, loss = 0.64711345\n",
      "Iteration 163, loss = 0.64677230\n",
      "Iteration 164, loss = 0.64643035\n",
      "Iteration 165, loss = 0.64608758\n",
      "Iteration 166, loss = 0.64574398\n",
      "Iteration 167, loss = 0.64539956\n",
      "Iteration 168, loss = 0.64505430\n",
      "Iteration 169, loss = 0.64470820\n",
      "Iteration 170, loss = 0.64436125\n",
      "Iteration 171, loss = 0.64401344\n",
      "Iteration 172, loss = 0.64366478\n",
      "Iteration 173, loss = 0.64331524\n",
      "Iteration 174, loss = 0.64296483\n",
      "Iteration 175, loss = 0.64261354\n",
      "Iteration 176, loss = 0.64226136\n",
      "Iteration 177, loss = 0.64190829\n",
      "Iteration 178, loss = 0.64155432\n",
      "Iteration 179, loss = 0.64119944\n",
      "Iteration 180, loss = 0.64084365\n",
      "Iteration 181, loss = 0.64048694\n",
      "Iteration 182, loss = 0.64012930\n",
      "Iteration 183, loss = 0.63977073\n",
      "Iteration 184, loss = 0.63941122\n",
      "Iteration 185, loss = 0.63905077\n",
      "Iteration 186, loss = 0.63868936\n",
      "Iteration 187, loss = 0.63832700\n",
      "Iteration 188, loss = 0.63796368\n",
      "Iteration 189, loss = 0.63759938\n",
      "Iteration 190, loss = 0.63723411\n",
      "Iteration 191, loss = 0.63686786\n",
      "Iteration 192, loss = 0.63650061\n",
      "Iteration 193, loss = 0.63613238\n",
      "Iteration 194, loss = 0.63576314\n",
      "Iteration 195, loss = 0.63539289\n",
      "Iteration 196, loss = 0.63502163\n",
      "Iteration 197, loss = 0.63464935\n",
      "Iteration 198, loss = 0.63427604\n",
      "Iteration 199, loss = 0.63390170\n",
      "Iteration 200, loss = 0.63352632\n",
      "Iteration 201, loss = 0.63314990\n",
      "Iteration 202, loss = 0.63277242\n",
      "Iteration 203, loss = 0.63239389\n",
      "Iteration 204, loss = 0.63201430\n",
      "Iteration 205, loss = 0.63163363\n",
      "Iteration 206, loss = 0.63125189\n",
      "Iteration 207, loss = 0.63086907\n",
      "Iteration 208, loss = 0.63048516\n",
      "Iteration 209, loss = 0.63010016\n",
      "Iteration 210, loss = 0.62971406\n",
      "Iteration 211, loss = 0.62932685\n",
      "Iteration 212, loss = 0.62893853\n",
      "Iteration 213, loss = 0.62854909\n",
      "Iteration 214, loss = 0.62815853\n",
      "Iteration 215, loss = 0.62776684\n",
      "Iteration 216, loss = 0.62737401\n",
      "Iteration 217, loss = 0.62698004\n",
      "Iteration 218, loss = 0.62658492\n",
      "Iteration 219, loss = 0.62618865\n",
      "Iteration 220, loss = 0.62579122\n",
      "Iteration 221, loss = 0.62539263\n",
      "Iteration 222, loss = 0.62499286\n",
      "Iteration 223, loss = 0.62459192\n",
      "Iteration 224, loss = 0.62418979\n",
      "Iteration 225, loss = 0.62378647\n",
      "Iteration 226, loss = 0.62338196\n",
      "Iteration 227, loss = 0.62297625\n",
      "Iteration 228, loss = 0.62256933\n",
      "Iteration 229, loss = 0.62216120\n",
      "Iteration 230, loss = 0.62175186\n",
      "Iteration 231, loss = 0.62134128\n",
      "Iteration 232, loss = 0.62092948\n",
      "Iteration 233, loss = 0.62051645\n",
      "Iteration 234, loss = 0.62010217\n",
      "Iteration 235, loss = 0.61968665\n",
      "Iteration 236, loss = 0.61926988\n",
      "Iteration 237, loss = 0.61885184\n",
      "Iteration 238, loss = 0.61843255\n",
      "Iteration 239, loss = 0.61801198\n",
      "Iteration 240, loss = 0.61759014\n",
      "Iteration 241, loss = 0.61716703\n",
      "Iteration 242, loss = 0.61674262\n",
      "Iteration 243, loss = 0.61631693\n",
      "Iteration 244, loss = 0.61588994\n",
      "Iteration 245, loss = 0.61546164\n",
      "Iteration 246, loss = 0.61503204\n",
      "Iteration 247, loss = 0.61460113\n",
      "Iteration 248, loss = 0.61416890\n",
      "Iteration 249, loss = 0.61373534\n",
      "Iteration 250, loss = 0.61330046\n",
      "Iteration 251, loss = 0.61286424\n",
      "Iteration 252, loss = 0.61242668\n",
      "Iteration 253, loss = 0.61198778\n",
      "Iteration 254, loss = 0.61154753\n",
      "Iteration 255, loss = 0.61110592\n",
      "Iteration 256, loss = 0.61066295\n",
      "Iteration 257, loss = 0.61021861\n",
      "Iteration 258, loss = 0.60977291\n",
      "Iteration 259, loss = 0.60932583\n",
      "Iteration 260, loss = 0.60887737\n",
      "Iteration 261, loss = 0.60842752\n",
      "Iteration 262, loss = 0.60797628\n",
      "Iteration 263, loss = 0.60752364\n",
      "Iteration 264, loss = 0.60706961\n",
      "Iteration 265, loss = 0.60661416\n",
      "Iteration 266, loss = 0.60615731\n",
      "Iteration 267, loss = 0.60569904\n",
      "Iteration 268, loss = 0.60523935\n",
      "Iteration 269, loss = 0.60477823\n",
      "Iteration 270, loss = 0.60431568\n",
      "Iteration 271, loss = 0.60385170\n",
      "Iteration 272, loss = 0.60338628\n",
      "Iteration 273, loss = 0.60291941\n",
      "Iteration 274, loss = 0.60245110\n",
      "Iteration 275, loss = 0.60198132\n",
      "Iteration 276, loss = 0.60151009\n",
      "Iteration 277, loss = 0.60103740\n",
      "Iteration 278, loss = 0.60056324\n",
      "Iteration 279, loss = 0.60008761\n",
      "Iteration 280, loss = 0.59961050\n",
      "Iteration 281, loss = 0.59913190\n",
      "Iteration 282, loss = 0.59865183\n",
      "Iteration 283, loss = 0.59817026\n",
      "Iteration 284, loss = 0.59768720\n",
      "Iteration 285, loss = 0.59720264\n",
      "Iteration 286, loss = 0.59671657\n",
      "Iteration 287, loss = 0.59622900\n",
      "Iteration 288, loss = 0.59573992\n",
      "Iteration 289, loss = 0.59524932\n",
      "Iteration 290, loss = 0.59475720\n",
      "Iteration 291, loss = 0.59426356\n",
      "Iteration 292, loss = 0.59376839\n",
      "Iteration 293, loss = 0.59327168\n",
      "Iteration 294, loss = 0.59277345\n",
      "Iteration 295, loss = 0.59227367\n",
      "Iteration 296, loss = 0.59177235\n",
      "Iteration 297, loss = 0.59126948\n",
      "Iteration 298, loss = 0.59076506\n",
      "Iteration 299, loss = 0.59025908\n",
      "Iteration 300, loss = 0.58975155\n",
      "Iteration 1, loss = 0.71448891\n",
      "Iteration 2, loss = 0.70000505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.69981494\n",
      "Iteration 4, loss = 0.69675343\n",
      "Iteration 5, loss = 0.69648390\n",
      "Iteration 6, loss = 0.69633247\n",
      "Iteration 7, loss = 0.69613883\n",
      "Iteration 8, loss = 0.69595975\n",
      "Iteration 9, loss = 0.69576854\n",
      "Iteration 10, loss = 0.69556521\n",
      "Iteration 11, loss = 0.69535150\n",
      "Iteration 12, loss = 0.69512824\n",
      "Iteration 13, loss = 0.69489639\n",
      "Iteration 14, loss = 0.69465686\n",
      "Iteration 15, loss = 0.69441044\n",
      "Iteration 16, loss = 0.69415786\n",
      "Iteration 17, loss = 0.69389977\n",
      "Iteration 18, loss = 0.69363674\n",
      "Iteration 19, loss = 0.69336930\n",
      "Iteration 20, loss = 0.69309793\n",
      "Iteration 21, loss = 0.69282304\n",
      "Iteration 22, loss = 0.69254501\n",
      "Iteration 23, loss = 0.69226419\n",
      "Iteration 24, loss = 0.69198088\n",
      "Iteration 25, loss = 0.69169535\n",
      "Iteration 26, loss = 0.69140785\n",
      "Iteration 27, loss = 0.69111861\n",
      "Iteration 28, loss = 0.69082781\n",
      "Iteration 29, loss = 0.69053563\n",
      "Iteration 30, loss = 0.69024223\n",
      "Iteration 31, loss = 0.68994776\n",
      "Iteration 32, loss = 0.68965233\n",
      "Iteration 33, loss = 0.68935606\n",
      "Iteration 34, loss = 0.68905906\n",
      "Iteration 35, loss = 0.68876140\n",
      "Iteration 36, loss = 0.68846318\n",
      "Iteration 37, loss = 0.68816445\n",
      "Iteration 38, loss = 0.68786529\n",
      "Iteration 39, loss = 0.68756575\n",
      "Iteration 40, loss = 0.68726587\n",
      "Iteration 41, loss = 0.68696570\n",
      "Iteration 42, loss = 0.68666528\n",
      "Iteration 43, loss = 0.68636464\n",
      "Iteration 44, loss = 0.68606381\n",
      "Iteration 45, loss = 0.68576281\n",
      "Iteration 46, loss = 0.68546167\n",
      "Iteration 47, loss = 0.68516040\n",
      "Iteration 48, loss = 0.68485902\n",
      "Iteration 49, loss = 0.68455755\n",
      "Iteration 50, loss = 0.68425599\n",
      "Iteration 51, loss = 0.68395435\n",
      "Iteration 52, loss = 0.68365265\n",
      "Iteration 53, loss = 0.68335088\n",
      "Iteration 54, loss = 0.68304905\n",
      "Iteration 55, loss = 0.68274717\n",
      "Iteration 56, loss = 0.68244523\n",
      "Iteration 57, loss = 0.68214323\n",
      "Iteration 58, loss = 0.68184119\n",
      "Iteration 59, loss = 0.68153908\n",
      "Iteration 60, loss = 0.68123692\n",
      "Iteration 61, loss = 0.68093470\n",
      "Iteration 62, loss = 0.68063241\n",
      "Iteration 63, loss = 0.68033006\n",
      "Iteration 64, loss = 0.68002763\n",
      "Iteration 65, loss = 0.67972513\n",
      "Iteration 66, loss = 0.67942255\n",
      "Iteration 67, loss = 0.67911989\n",
      "Iteration 68, loss = 0.67881713\n",
      "Iteration 69, loss = 0.67851427\n",
      "Iteration 70, loss = 0.67821131\n",
      "Iteration 71, loss = 0.67790823\n",
      "Iteration 72, loss = 0.67760504\n",
      "Iteration 73, loss = 0.67730173\n",
      "Iteration 74, loss = 0.67699828\n",
      "Iteration 75, loss = 0.67669470\n",
      "Iteration 76, loss = 0.67639097\n",
      "Iteration 77, loss = 0.67608708\n",
      "Iteration 78, loss = 0.67578304\n",
      "Iteration 79, loss = 0.67547883\n",
      "Iteration 80, loss = 0.67517444\n",
      "Iteration 81, loss = 0.67486987\n",
      "Iteration 82, loss = 0.67456511\n",
      "Iteration 83, loss = 0.67426015\n",
      "Iteration 84, loss = 0.67395499\n",
      "Iteration 85, loss = 0.67364961\n",
      "Iteration 86, loss = 0.67334401\n",
      "Iteration 87, loss = 0.67303818\n",
      "Iteration 88, loss = 0.67273211\n",
      "Iteration 89, loss = 0.67242580\n",
      "Iteration 90, loss = 0.67211923\n",
      "Iteration 91, loss = 0.67181240\n",
      "Iteration 92, loss = 0.67150530\n",
      "Iteration 93, loss = 0.67119793\n",
      "Iteration 94, loss = 0.67089027\n",
      "Iteration 95, loss = 0.67058232\n",
      "Iteration 96, loss = 0.67027406\n",
      "Iteration 97, loss = 0.66996550\n",
      "Iteration 98, loss = 0.66965662\n",
      "Iteration 99, loss = 0.66934742\n",
      "Iteration 100, loss = 0.66903788\n",
      "Iteration 101, loss = 0.66872800\n",
      "Iteration 102, loss = 0.66841778\n",
      "Iteration 103, loss = 0.66810720\n",
      "Iteration 104, loss = 0.66779625\n",
      "Iteration 105, loss = 0.66748493\n",
      "Iteration 106, loss = 0.66717324\n",
      "Iteration 107, loss = 0.66686115\n",
      "Iteration 108, loss = 0.66654867\n",
      "Iteration 109, loss = 0.66623579\n",
      "Iteration 110, loss = 0.66592249\n",
      "Iteration 111, loss = 0.66560878\n",
      "Iteration 112, loss = 0.66529464\n",
      "Iteration 113, loss = 0.66498007\n",
      "Iteration 114, loss = 0.66466505\n",
      "Iteration 115, loss = 0.66434958\n",
      "Iteration 116, loss = 0.66403366\n",
      "Iteration 117, loss = 0.66371727\n",
      "Iteration 118, loss = 0.66340040\n",
      "Iteration 119, loss = 0.66308306\n",
      "Iteration 120, loss = 0.66276523\n",
      "Iteration 121, loss = 0.66244690\n",
      "Iteration 122, loss = 0.66212807\n",
      "Iteration 123, loss = 0.66180873\n",
      "Iteration 124, loss = 0.66148886\n",
      "Iteration 125, loss = 0.66116847\n",
      "Iteration 126, loss = 0.66084755\n",
      "Iteration 127, loss = 0.66052609\n",
      "Iteration 128, loss = 0.66020407\n",
      "Iteration 129, loss = 0.65988150\n",
      "Iteration 130, loss = 0.65955836\n",
      "Iteration 131, loss = 0.65923465\n",
      "Iteration 132, loss = 0.65891036\n",
      "Iteration 133, loss = 0.65858549\n",
      "Iteration 134, loss = 0.65826002\n",
      "Iteration 135, loss = 0.65793394\n",
      "Iteration 136, loss = 0.65760726\n",
      "Iteration 137, loss = 0.65727996\n",
      "Iteration 138, loss = 0.65695204\n",
      "Iteration 139, loss = 0.65662348\n",
      "Iteration 140, loss = 0.65629428\n",
      "Iteration 141, loss = 0.65596444\n",
      "Iteration 142, loss = 0.65563394\n",
      "Iteration 143, loss = 0.65530277\n",
      "Iteration 144, loss = 0.65497094\n",
      "Iteration 145, loss = 0.65463844\n",
      "Iteration 146, loss = 0.65430524\n",
      "Iteration 147, loss = 0.65397136\n",
      "Iteration 148, loss = 0.65363677\n",
      "Iteration 149, loss = 0.65330148\n",
      "Iteration 150, loss = 0.65296548\n",
      "Iteration 151, loss = 0.65262875\n",
      "Iteration 152, loss = 0.65229130\n",
      "Iteration 153, loss = 0.65195311\n",
      "Iteration 154, loss = 0.65161417\n",
      "Iteration 155, loss = 0.65127449\n",
      "Iteration 156, loss = 0.65093405\n",
      "Iteration 157, loss = 0.65059284\n",
      "Iteration 158, loss = 0.65025086\n",
      "Iteration 159, loss = 0.64990809\n",
      "Iteration 160, loss = 0.64956455\n",
      "Iteration 161, loss = 0.64922020\n",
      "Iteration 162, loss = 0.64887506\n",
      "Iteration 163, loss = 0.64852910\n",
      "Iteration 164, loss = 0.64818233\n",
      "Iteration 165, loss = 0.64783474\n",
      "Iteration 166, loss = 0.64748631\n",
      "Iteration 167, loss = 0.64713705\n",
      "Iteration 168, loss = 0.64678694\n",
      "Iteration 169, loss = 0.64643597\n",
      "Iteration 170, loss = 0.64608415\n",
      "Iteration 171, loss = 0.64573146\n",
      "Iteration 172, loss = 0.64537790\n",
      "Iteration 173, loss = 0.64502345\n",
      "Iteration 174, loss = 0.64466812\n",
      "Iteration 175, loss = 0.64431189\n",
      "Iteration 176, loss = 0.64395476\n",
      "Iteration 177, loss = 0.64359671\n",
      "Iteration 178, loss = 0.64323775\n",
      "Iteration 179, loss = 0.64287787\n",
      "Iteration 180, loss = 0.64251705\n",
      "Iteration 181, loss = 0.64215530\n",
      "Iteration 182, loss = 0.64179259\n",
      "Iteration 183, loss = 0.64142894\n",
      "Iteration 184, loss = 0.64106432\n",
      "Iteration 185, loss = 0.64069874\n",
      "Iteration 186, loss = 0.64033218\n",
      "Iteration 187, loss = 0.63996464\n",
      "Iteration 188, loss = 0.63959612\n",
      "Iteration 189, loss = 0.63922659\n",
      "Iteration 190, loss = 0.63885607\n",
      "Iteration 191, loss = 0.63848453\n",
      "Iteration 192, loss = 0.63811198\n",
      "Iteration 193, loss = 0.63773840\n",
      "Iteration 194, loss = 0.63736380\n",
      "Iteration 195, loss = 0.63698815\n",
      "Iteration 196, loss = 0.63661146\n",
      "Iteration 197, loss = 0.63623372\n",
      "Iteration 198, loss = 0.63585492\n",
      "Iteration 199, loss = 0.63547505\n",
      "Iteration 200, loss = 0.63509411\n",
      "Iteration 201, loss = 0.63471209\n",
      "Iteration 202, loss = 0.63432898\n",
      "Iteration 203, loss = 0.63394478\n",
      "Iteration 204, loss = 0.63355948\n",
      "Iteration 205, loss = 0.63317307\n",
      "Iteration 206, loss = 0.63278554\n",
      "Iteration 207, loss = 0.63239689\n",
      "Iteration 208, loss = 0.63200712\n",
      "Iteration 209, loss = 0.63161621\n",
      "Iteration 210, loss = 0.63122416\n",
      "Iteration 211, loss = 0.63083095\n",
      "Iteration 212, loss = 0.63043660\n",
      "Iteration 213, loss = 0.63004108\n",
      "Iteration 214, loss = 0.62964438\n",
      "Iteration 215, loss = 0.62924652\n",
      "Iteration 216, loss = 0.62884747\n",
      "Iteration 217, loss = 0.62844723\n",
      "Iteration 218, loss = 0.62804579\n",
      "Iteration 219, loss = 0.62764315\n",
      "Iteration 220, loss = 0.62723929\n",
      "Iteration 221, loss = 0.62683422\n",
      "Iteration 222, loss = 0.62642793\n",
      "Iteration 223, loss = 0.62602041\n",
      "Iteration 224, loss = 0.62561164\n",
      "Iteration 225, loss = 0.62520164\n",
      "Iteration 226, loss = 0.62479038\n",
      "Iteration 227, loss = 0.62437787\n",
      "Iteration 228, loss = 0.62396409\n",
      "Iteration 229, loss = 0.62354904\n",
      "Iteration 230, loss = 0.62313271\n",
      "Iteration 231, loss = 0.62271510\n",
      "Iteration 232, loss = 0.62229620\n",
      "Iteration 233, loss = 0.62187600\n",
      "Iteration 234, loss = 0.62145449\n",
      "Iteration 235, loss = 0.62103168\n",
      "Iteration 236, loss = 0.62060754\n",
      "Iteration 237, loss = 0.62018209\n",
      "Iteration 238, loss = 0.61975530\n",
      "Iteration 239, loss = 0.61932718\n",
      "Iteration 240, loss = 0.61889772\n",
      "Iteration 241, loss = 0.61846690\n",
      "Iteration 242, loss = 0.61803473\n",
      "Iteration 243, loss = 0.61760120\n",
      "Iteration 244, loss = 0.61716629\n",
      "Iteration 245, loss = 0.61673002\n",
      "Iteration 246, loss = 0.61629236\n",
      "Iteration 247, loss = 0.61585331\n",
      "Iteration 248, loss = 0.61541287\n",
      "Iteration 249, loss = 0.61497102\n",
      "Iteration 250, loss = 0.61452778\n",
      "Iteration 251, loss = 0.61408311\n",
      "Iteration 252, loss = 0.61363703\n",
      "Iteration 253, loss = 0.61318952\n",
      "Iteration 254, loss = 0.61274058\n",
      "Iteration 255, loss = 0.61229021\n",
      "Iteration 256, loss = 0.61183838\n",
      "Iteration 257, loss = 0.61138511\n",
      "Iteration 258, loss = 0.61093038\n",
      "Iteration 259, loss = 0.61047419\n",
      "Iteration 260, loss = 0.61001653\n",
      "Iteration 261, loss = 0.60955740\n",
      "Iteration 262, loss = 0.60909678\n",
      "Iteration 263, loss = 0.60863468\n",
      "Iteration 264, loss = 0.60817108\n",
      "Iteration 265, loss = 0.60770599\n",
      "Iteration 266, loss = 0.60723939\n",
      "Iteration 267, loss = 0.60677128\n",
      "Iteration 268, loss = 0.60630166\n",
      "Iteration 269, loss = 0.60583051\n",
      "Iteration 270, loss = 0.60535784\n",
      "Iteration 271, loss = 0.60488363\n",
      "Iteration 272, loss = 0.60440788\n",
      "Iteration 273, loss = 0.60393059\n",
      "Iteration 274, loss = 0.60345174\n",
      "Iteration 275, loss = 0.60297134\n",
      "Iteration 276, loss = 0.60248938\n",
      "Iteration 277, loss = 0.60200585\n",
      "Iteration 278, loss = 0.60152075\n",
      "Iteration 279, loss = 0.60103407\n",
      "Iteration 280, loss = 0.60054580\n",
      "Iteration 281, loss = 0.60005595\n",
      "Iteration 282, loss = 0.59956449\n",
      "Iteration 283, loss = 0.59907144\n",
      "Iteration 284, loss = 0.59857679\n",
      "Iteration 285, loss = 0.59808052\n",
      "Iteration 286, loss = 0.59758263\n",
      "Iteration 287, loss = 0.59708313\n",
      "Iteration 288, loss = 0.59658199\n",
      "Iteration 289, loss = 0.59607923\n",
      "Iteration 290, loss = 0.59557482\n",
      "Iteration 291, loss = 0.59506878\n",
      "Iteration 292, loss = 0.59456109\n",
      "Iteration 293, loss = 0.59405174\n",
      "Iteration 294, loss = 0.59354074\n",
      "Iteration 295, loss = 0.59302808\n",
      "Iteration 296, loss = 0.59251375\n",
      "Iteration 297, loss = 0.59199774\n",
      "Iteration 298, loss = 0.59148006\n",
      "Iteration 299, loss = 0.59096070\n",
      "Iteration 300, loss = 0.59043966\n",
      "Iteration 1, loss = 0.71479195\n",
      "Iteration 2, loss = 6.41185070\n",
      "Iteration 3, loss = 2.35080844\n",
      "Iteration 4, loss = 2.74017150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 3.97800520\n",
      "Iteration 6, loss = 3.15970417\n",
      "Iteration 7, loss = 1.19188532\n",
      "Iteration 8, loss = 1.70042392\n",
      "Iteration 9, loss = 2.77039487\n",
      "Iteration 10, loss = 2.58567354\n",
      "Iteration 11, loss = 1.49152672\n",
      "Iteration 12, loss = 0.75840476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71290991\n",
      "Iteration 2, loss = 6.32590444\n",
      "Iteration 3, loss = 2.39239333\n",
      "Iteration 4, loss = 2.65131713\n",
      "Iteration 5, loss = 3.82427678\n",
      "Iteration 6, loss = 2.91632234\n",
      "Iteration 7, loss = 0.94878949\n",
      "Iteration 8, loss = 1.91152418\n",
      "Iteration 9, loss = 2.85975000\n",
      "Iteration 10, loss = 2.57686985\n",
      "Iteration 11, loss = 1.41995025\n",
      "Iteration 12, loss = 0.80763624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448891\n",
      "Iteration 2, loss = 6.37086492\n",
      "Iteration 3, loss = 2.37199632\n",
      "Iteration 4, loss = 2.69776385\n",
      "Iteration 5, loss = 3.90445890\n",
      "Iteration 6, loss = 3.04306511\n",
      "Iteration 7, loss = 1.06850918\n",
      "Iteration 8, loss = 1.81463024\n",
      "Iteration 9, loss = 2.83343737\n",
      "Iteration 10, loss = 2.60707147\n",
      "Iteration 11, loss = 1.48460461\n",
      "Iteration 12, loss = 0.76908716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69320797\n",
      "Iteration 2, loss = 0.69710525\n",
      "Iteration 3, loss = 2.56976897\n",
      "Iteration 4, loss = 18.13962678\n",
      "Iteration 5, loss = 1.91361125\n",
      "Iteration 6, loss = 3.09772102\n",
      "Iteration 7, loss = 2.65331502\n",
      "Iteration 8, loss = 1.10565872\n",
      "Iteration 9, loss = 3.86194285\n",
      "Iteration 10, loss = 1.12604842\n",
      "Iteration 11, loss = 1.39017210\n",
      "Iteration 12, loss = 1.46517193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69803140\n",
      "Iteration 2, loss = 0.78184608\n",
      "Iteration 3, loss = 10.04349917\n",
      "Iteration 4, loss = 8.38031388\n",
      "Iteration 5, loss = 0.70341310\n",
      "Iteration 6, loss = 1.41030397\n",
      "Iteration 7, loss = 0.97348986\n",
      "Iteration 8, loss = 0.89281460\n",
      "Iteration 9, loss = 1.74167982\n",
      "Iteration 10, loss = 2.21473648\n",
      "Iteration 11, loss = 1.23202972\n",
      "Iteration 12, loss = 0.90202425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69652269\n",
      "Iteration 2, loss = 0.70027553\n",
      "Iteration 3, loss = 2.50802123\n",
      "Iteration 4, loss = 18.02183700\n",
      "Iteration 5, loss = 1.98690033\n",
      "Iteration 6, loss = 3.23843954\n",
      "Iteration 7, loss = 2.58530405\n",
      "Iteration 8, loss = 1.10403469\n",
      "Iteration 9, loss = 3.90187882\n",
      "Iteration 10, loss = 1.06282011\n",
      "Iteration 11, loss = 1.34159118\n",
      "Iteration 12, loss = 1.53113258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69320797\n",
      "Iteration 2, loss = 18.13962931\n",
      "Iteration 3, loss = 17.90404990\n",
      "Iteration 4, loss = 17.90404997\n",
      "Iteration 5, loss = 1.58080595\n",
      "Iteration 6, loss = 18.13962990\n",
      "Iteration 7, loss = 18.13963019\n",
      "Iteration 8, loss = 18.13962995\n",
      "Iteration 9, loss = 18.13962955\n",
      "Iteration 10, loss = 1.70634242\n",
      "Iteration 11, loss = 16.85208668\n",
      "Iteration 12, loss = 17.90405129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69803140\n",
      "Iteration 2, loss = 17.90404991\n",
      "Iteration 3, loss = 17.90404744\n",
      "Iteration 4, loss = 18.13962757\n",
      "Iteration 5, loss = 18.13962869\n",
      "Iteration 6, loss = 18.13962876\n",
      "Iteration 7, loss = 18.13962825\n",
      "Iteration 8, loss = 4.18890876\n",
      "Iteration 9, loss = 17.90404889\n",
      "Iteration 10, loss = 17.90404921\n",
      "Iteration 11, loss = 17.90404906\n",
      "Iteration 12, loss = 17.90404853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69652269\n",
      "Iteration 2, loss = 18.02183952\n",
      "Iteration 3, loss = 18.02183927\n",
      "Iteration 4, loss = 18.02183932\n",
      "Iteration 5, loss = 3.28153270\n",
      "Iteration 6, loss = 3.54344862\n",
      "Iteration 7, loss = 8.28163614\n",
      "Iteration 8, loss = 7.05569651\n",
      "Iteration 9, loss = 1.66021428\n",
      "Iteration 10, loss = 2.11992251\n",
      "Iteration 11, loss = 2.87855875\n",
      "Iteration 12, loss = 2.02143480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69820099\n",
      "Iteration 2, loss = 1.50395634\n",
      "Iteration 3, loss = 17.79875046\n",
      "Iteration 4, loss = 2.43959925\n",
      "Iteration 5, loss = 1.28938857\n",
      "Iteration 6, loss = 0.83696073\n",
      "Iteration 7, loss = 1.42330641\n",
      "Iteration 8, loss = 0.86132996\n",
      "Iteration 9, loss = 0.81432839\n",
      "Iteration 10, loss = 0.69403395\n",
      "Iteration 11, loss = 0.71523025\n",
      "Iteration 12, loss = 0.69431066\n",
      "Iteration 13, loss = 0.72229921\n",
      "Iteration 14, loss = 0.72013553\n",
      "Iteration 15, loss = 0.88488995\n",
      "Iteration 16, loss = 1.30003147\n",
      "Iteration 17, loss = 1.81191044\n",
      "Iteration 18, loss = 1.07196595\n",
      "Iteration 19, loss = 1.00201919\n",
      "Iteration 20, loss = 0.80959130\n",
      "Iteration 21, loss = 0.72893009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70059838\n",
      "Iteration 2, loss = 1.72049932\n",
      "Iteration 3, loss = 18.11165388\n",
      "Iteration 4, loss = 2.54189921\n",
      "Iteration 5, loss = 1.04878863\n",
      "Iteration 6, loss = 0.85310591\n",
      "Iteration 7, loss = 1.35858975\n",
      "Iteration 8, loss = 0.75984851\n",
      "Iteration 9, loss = 0.89068811\n",
      "Iteration 10, loss = 0.75382887\n",
      "Iteration 11, loss = 0.95007715\n",
      "Iteration 12, loss = 0.97725371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963462\n",
      "Iteration 2, loss = 1.58621853\n",
      "Iteration 3, loss = 17.96274669\n",
      "Iteration 4, loss = 2.48325158\n",
      "Iteration 5, loss = 1.17517901\n",
      "Iteration 6, loss = 0.84761879\n",
      "Iteration 7, loss = 1.39672987\n",
      "Iteration 8, loss = 0.81318266\n",
      "Iteration 9, loss = 0.85687630\n",
      "Iteration 10, loss = 0.70364791\n",
      "Iteration 11, loss = 0.76928887\n",
      "Iteration 12, loss = 0.73988029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69820099\n",
      "Iteration 2, loss = 18.13962909\n",
      "Iteration 3, loss = 18.13962663\n",
      "Iteration 4, loss = 17.90404764\n",
      "Iteration 5, loss = 17.90404869\n",
      "Iteration 6, loss = 17.90404902\n",
      "Iteration 7, loss = 17.90404917\n",
      "Iteration 8, loss = 1.06728913\n",
      "Iteration 9, loss = 18.13963098\n",
      "Iteration 10, loss = 18.13963160\n",
      "Iteration 11, loss = 18.13963134\n",
      "Iteration 12, loss = 18.13963051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70059838\n",
      "Iteration 2, loss = 17.90404965\n",
      "Iteration 3, loss = 17.90404722\n",
      "Iteration 4, loss = 18.13962702\n",
      "Iteration 5, loss = 18.13962804\n",
      "Iteration 6, loss = 18.13962848\n",
      "Iteration 7, loss = 18.13962885\n",
      "Iteration 8, loss = 3.79571912\n",
      "Iteration 9, loss = 13.99106411\n",
      "Iteration 10, loss = 13.89490606\n",
      "Iteration 11, loss = 7.61429803\n",
      "Iteration 12, loss = 2.14093348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963462\n",
      "Iteration 2, loss = 18.02183928\n",
      "Iteration 3, loss = 18.02183685\n",
      "Iteration 4, loss = 18.02183726\n",
      "Iteration 5, loss = 18.02183830\n",
      "Iteration 6, loss = 18.02183872\n",
      "Iteration 7, loss = 18.02183903\n",
      "Iteration 8, loss = 1.55850396\n",
      "Iteration 9, loss = 12.37194011\n",
      "Iteration 10, loss = 12.69352722\n",
      "Iteration 11, loss = 6.68789345\n",
      "Iteration 12, loss = 2.85858755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71479195\n",
      "Iteration 2, loss = 2.43304704\n",
      "Iteration 3, loss = 9.40471624\n",
      "Iteration 4, loss = 1.34825728\n",
      "Iteration 5, loss = 1.05127644\n",
      "Iteration 6, loss = 0.75479989\n",
      "Iteration 7, loss = 0.93064309\n",
      "Iteration 8, loss = 1.06502237\n",
      "Iteration 9, loss = 2.39047856\n",
      "Iteration 10, loss = 1.61024485\n",
      "Iteration 11, loss = 1.59306697\n",
      "Iteration 12, loss = 0.75148520\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71290991\n",
      "Iteration 2, loss = 2.58504189\n",
      "Iteration 3, loss = 9.11321153\n",
      "Iteration 4, loss = 1.18634039\n",
      "Iteration 5, loss = 1.06898811\n",
      "Iteration 6, loss = 0.75041317\n",
      "Iteration 7, loss = 0.95403759\n",
      "Iteration 8, loss = 0.96513681\n",
      "Iteration 9, loss = 1.88042195\n",
      "Iteration 10, loss = 1.96839260\n",
      "Iteration 11, loss = 1.37509992\n",
      "Iteration 12, loss = 0.76126660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448891\n",
      "Iteration 2, loss = 2.50951469\n",
      "Iteration 3, loss = 9.26334276\n",
      "Iteration 4, loss = 1.27055153\n",
      "Iteration 5, loss = 1.06035901\n",
      "Iteration 6, loss = 0.75360654\n",
      "Iteration 7, loss = 0.94125854\n",
      "Iteration 8, loss = 1.01963796\n",
      "Iteration 9, loss = 2.15739814\n",
      "Iteration 10, loss = 1.78607177\n",
      "Iteration 11, loss = 1.49061482\n",
      "Iteration 12, loss = 0.75118553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71479195\n",
      "Iteration 2, loss = 18.13962834\n",
      "Iteration 3, loss = 18.13962685\n",
      "Iteration 4, loss = 17.90404832\n",
      "Iteration 5, loss = 17.90404875\n",
      "Iteration 6, loss = 17.90404820\n",
      "Iteration 7, loss = 14.97317042\n",
      "Iteration 8, loss = 1.54120006\n",
      "Iteration 9, loss = 8.48802220\n",
      "Iteration 10, loss = 8.95904090\n",
      "Iteration 11, loss = 5.48953401\n",
      "Iteration 12, loss = 0.69486615\n",
      "Iteration 13, loss = 4.19365425\n",
      "Iteration 14, loss = 5.12337135\n",
      "Iteration 15, loss = 3.83923018\n",
      "Iteration 16, loss = 1.22109900\n",
      "Iteration 17, loss = 2.21170256\n",
      "Iteration 18, loss = 3.61411791\n",
      "Iteration 19, loss = 3.53443916\n",
      "Iteration 20, loss = 2.38543003\n",
      "Iteration 21, loss = 0.83220902\n",
      "Iteration 22, loss = 1.36402034\n",
      "Iteration 23, loss = 2.18036385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71290991\n",
      "Iteration 2, loss = 17.90404891\n",
      "Iteration 3, loss = 17.90404745\n",
      "Iteration 4, loss = 18.13962772\n",
      "Iteration 5, loss = 18.13962808\n",
      "Iteration 6, loss = 18.13962749\n",
      "Iteration 7, loss = 12.75383694\n",
      "Iteration 8, loss = 3.99738476\n",
      "Iteration 9, loss = 10.47958331\n",
      "Iteration 10, loss = 10.64334669\n",
      "Iteration 11, loss = 6.99507045\n",
      "Iteration 12, loss = 1.33241510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448891\n",
      "Iteration 2, loss = 18.02183855\n",
      "Iteration 3, loss = 18.02183708\n",
      "Iteration 4, loss = 18.02183795\n",
      "Iteration 5, loss = 18.02183834\n",
      "Iteration 6, loss = 18.02183777\n",
      "Iteration 7, loss = 13.90642048\n",
      "Iteration 8, loss = 2.71539361\n",
      "Iteration 9, loss = 9.21741423\n",
      "Iteration 10, loss = 9.39863758\n",
      "Iteration 11, loss = 5.77753653\n",
      "Iteration 12, loss = 0.69011682\n",
      "Iteration 13, loss = 4.61091671\n",
      "Iteration 14, loss = 5.98393702\n",
      "Iteration 15, loss = 4.95952507\n",
      "Iteration 16, loss = 2.37456047\n",
      "Iteration 17, loss = 1.21076771\n",
      "Iteration 18, loss = 2.89305563\n",
      "Iteration 19, loss = 3.14279307\n",
      "Iteration 20, loss = 2.27037954\n",
      "Iteration 21, loss = 0.89084975\n",
      "Iteration 22, loss = 1.17156458\n",
      "Iteration 23, loss = 1.92268575\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69320797\n",
      "Iteration 2, loss = 1.15437584\n",
      "Iteration 3, loss = 17.90405723\n",
      "Iteration 4, loss = 17.90480779\n",
      "Iteration 5, loss = 23.64198942\n",
      "Iteration 6, loss = 34789530.91558962\n",
      "Iteration 7, loss = 6968610391368902115328.00000000\n",
      "Iteration 8, loss = 53931891369716612100468724539657678265939337936896.00000000\n",
      "Iteration 9, loss = 16746302289199236328318079013944732060909842095510208736770870397722324910866411120660217607182415936094208.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69803140\n",
      "Iteration 2, loss = 4.28941701\n",
      "Iteration 3, loss = 17.90406215\n",
      "Iteration 4, loss = 17.90465868\n",
      "Iteration 5, loss = 19.52699689\n",
      "Iteration 6, loss = 6360838.97886444\n",
      "Iteration 7, loss = 26710004873977073664.00000000\n",
      "Iteration 8, loss = 2255652375756620802564394390006832527782707200.00000000\n",
      "Iteration 9, loss = 3353366965986173670404290586307542353427258377437247864587000991689303847598166195297801778233344.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.69652269\n",
      "Iteration 2, loss = 1.13353076\n",
      "Iteration 3, loss = 18.02184639\n",
      "Iteration 4, loss = 18.02257154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 23.27908579\n",
      "Iteration 6, loss = 24848301.06762850\n",
      "Iteration 7, loss = 3364833409171923664896.00000000\n",
      "Iteration 8, loss = 9715228841363144137282650128916707801398283075584.00000000\n",
      "Iteration 9, loss = 514334834780334881035671528213433331658129759533082765121609257508581497645059716278214315114935680499712.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.69320797\n",
      "Iteration 2, loss = 18.13988952\n",
      "Iteration 3, loss = 17.90440357\n",
      "Iteration 4, loss = 11.07700672\n",
      "Iteration 5, loss = 18.14016099\n",
      "Iteration 6, loss = 18.14015279\n",
      "Iteration 7, loss = 18.14009326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 17.90448430\n",
      "Iteration 9, loss = 17.90444960\n",
      "Iteration 10, loss = 17.90441984\n",
      "Iteration 11, loss = 18.14002026\n",
      "Iteration 12, loss = 18.14004337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69803140\n",
      "Iteration 2, loss = 17.90431182\n",
      "Iteration 3, loss = 18.13981030\n",
      "Iteration 4, loss = 18.13974214\n",
      "Iteration 5, loss = 17.90419645\n",
      "Iteration 6, loss = 17.90425226\n",
      "Iteration 7, loss = 18.13992524\n",
      "Iteration 8, loss = 18.13990094\n",
      "Iteration 9, loss = 18.13982662\n",
      "Iteration 10, loss = 17.90418320\n",
      "Iteration 11, loss = 17.90414936\n",
      "Iteration 12, loss = 17.90412633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69652269\n",
      "Iteration 2, loss = 18.02209801\n",
      "Iteration 3, loss = 18.02218011\n",
      "Iteration 4, loss = 18.02222912\n",
      "Iteration 5, loss = 18.02223890\n",
      "Iteration 6, loss = 18.02226244\n",
      "Iteration 7, loss = 18.02226042\n",
      "Iteration 8, loss = 18.02226928\n",
      "Iteration 9, loss = 17.96960108\n",
      "Iteration 10, loss = 18.02226347\n",
      "Iteration 11, loss = 18.02225703\n",
      "Iteration 12, loss = 8.25904865\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69820099\n",
      "Iteration 2, loss = 15.17023740\n",
      "Iteration 3, loss = 17.90406002\n",
      "Iteration 4, loss = 17.90570895\n",
      "Iteration 5, loss = 27.04138500\n",
      "Iteration 6, loss = 84538910.49207364\n",
      "Iteration 7, loss = 1239908352713393438720.00000000\n",
      "Iteration 8, loss = 840806703082082842664823997385167204933978554368.00000000\n",
      "Iteration 9, loss = 119661008692999717609958649994425254908020212355153260879623262295628182119706202357243421079212392448.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.70059838\n",
      "Iteration 2, loss = 17.53357832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 18.13963970\n",
      "Iteration 4, loss = 17.90646420\n",
      "Iteration 5, loss = 49.07273311\n",
      "Iteration 6, loss = 2635709349.79779816\n",
      "Iteration 7, loss = 25124753894095823524855808.00000000\n",
      "Iteration 8, loss = 1711597797430929390095105726135734955135424522881610022912.00000000\n",
      "Iteration 9, loss = 10593879247888707517287088509208917713378475957696535892633216110021618803458808516147949788476113548996345627106831499264.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.69963462\n",
      "Iteration 2, loss = 16.06201598\n",
      "Iteration 3, loss = 18.02184961\n",
      "Iteration 4, loss = 18.02375360\n",
      "Iteration 5, loss = 32.84830098\n",
      "Iteration 6, loss = 6643986.85621211\n",
      "Iteration 7, loss = 583053797580401737728.00000000\n",
      "Iteration 8, loss = 5837429114682569105641021829566801646354169856.00000000\n",
      "Iteration 9, loss = 450791786694865213632732208634877687953004259705651961313263174591969353475871040553116666649640960.00000000\n",
      "Iteration 10, loss = 3489444912680181367621512591829345422705629031188710751493044333987449043078835304359273711531273072788731903776943284348223200288903937577915658898618334851647680274028384997520580666257647286603481088.00000000\n",
      "Iteration 11, loss = inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.69820099\n",
      "Iteration 2, loss = 18.13989149\n",
      "Iteration 3, loss = 17.90416210\n",
      "Iteration 4, loss = 18.13968889\n",
      "Iteration 5, loss = 17.90412298\n",
      "Iteration 6, loss = 18.13973310\n",
      "Iteration 7, loss = 8.37413702\n",
      "Iteration 8, loss = 17.90416424\n",
      "Iteration 9, loss = 17.90415054\n",
      "Iteration 10, loss = 17.90412914\n",
      "Iteration 11, loss = 18.13971294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 18.13971916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70059838\n",
      "Iteration 2, loss = 17.90431205\n",
      "Iteration 3, loss = 11.14472376\n",
      "Iteration 4, loss = 18.13983991\n",
      "Iteration 5, loss = 17.90441786\n",
      "Iteration 6, loss = 18.14010763\n",
      "Iteration 7, loss = 17.90467541\n",
      "Iteration 8, loss = 17.90474476\n",
      "Iteration 9, loss = 18.14037329\n",
      "Iteration 10, loss = 18.14040100\n",
      "Iteration 11, loss = 18.14039125\n",
      "Iteration 12, loss = 18.14036543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963462\n",
      "Iteration 2, loss = 18.02209994\n",
      "Iteration 3, loss = 10.57724451\n",
      "Iteration 4, loss = 18.02189914\n",
      "Iteration 5, loss = 18.02191438\n",
      "Iteration 6, loss = 18.02194585\n",
      "Iteration 7, loss = 8.09204621\n",
      "Iteration 8, loss = 18.02196100\n",
      "Iteration 9, loss = 18.02194483\n",
      "Iteration 10, loss = 18.02191852\n",
      "Iteration 11, loss = 18.02191792\n",
      "Iteration 12, loss = 18.02192476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71479195\n",
      "Iteration 2, loss = 18.13962598\n",
      "Iteration 3, loss = 17.90406550\n",
      "Iteration 4, loss = 18.14605749\n",
      "Iteration 5, loss = 495.10102924\n",
      "Iteration 6, loss = 2810647374153.97851562\n",
      "Iteration 7, loss = 89177840005241028094433916092416.00000000\n",
      "Iteration 8, loss = 97906728681677996264656795401609702989394226988165377206318406274383872.00000000\n",
      "Iteration 9, loss = 108209528213734574993462883403520694477954553620441657905296178544751360136977472300888461174891059887202354492550388344973259727313674754031450324992.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.71290991\n",
      "Iteration 2, loss = 17.90404659\n",
      "Iteration 3, loss = 18.13964673\n",
      "Iteration 4, loss = 17.91138497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 743.81943000\n",
      "Iteration 6, loss = 6076319215826.79980469\n",
      "Iteration 7, loss = 491100704868016628652716318523392.00000000\n",
      "Iteration 8, loss = 2778042601358809835025765420509973167144128418682260930364210143219941376.00000000\n",
      "Iteration 9, loss = 102651364111416172017745888075265874119105262700284064358851387711658437405756807983721779716536404081557513062436384402006092213070714629668355205758976.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71448891\n",
      "Iteration 2, loss = 18.02183622\n",
      "Iteration 3, loss = 18.02185591\n",
      "Iteration 4, loss = 18.02866164\n",
      "Iteration 5, loss = 605.68386574\n",
      "Iteration 6, loss = 4146337131210.21582031\n",
      "Iteration 7, loss = 213237783325427130656112211132416.00000000\n",
      "Iteration 8, loss = 544793146556480856570531892347702818434708443519224816945162339118743552.00000000\n",
      "Iteration 9, loss = 3681248823405482265479657848033493433077622439818539878717704842498547132751898948623511080731891612575687173571879707039689842172290833701724395405312.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71479195\n",
      "Iteration 2, loss = 18.13989061\n",
      "Iteration 3, loss = 17.90438476\n",
      "Iteration 4, loss = 17.90426836\n",
      "Iteration 5, loss = 18.13974300\n",
      "Iteration 6, loss = 18.13969422\n",
      "Iteration 7, loss = 17.90411459\n",
      "Iteration 8, loss = 17.90412075\n",
      "Iteration 9, loss = 1.23101776\n",
      "Iteration 10, loss = 18.13970337\n",
      "Iteration 11, loss = 18.13969845\n",
      "Iteration 12, loss = 18.13968007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71290991\n",
      "Iteration 2, loss = 17.90431118\n",
      "Iteration 3, loss = 18.13996833\n",
      "Iteration 4, loss = 18.13985020\n",
      "Iteration 5, loss = 17.90416472\n",
      "Iteration 6, loss = 17.90411229\n",
      "Iteration 7, loss = 18.13968746\n",
      "Iteration 8, loss = 18.13969269\n",
      "Iteration 9, loss = 5.50341788\n",
      "Iteration 10, loss = 8.29704491\n",
      "Iteration 11, loss = 17.90411945\n",
      "Iteration 12, loss = 17.90412115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448891\n",
      "Iteration 2, loss = 18.02209911\n",
      "Iteration 3, loss = 18.02217425\n",
      "Iteration 4, loss = 18.02205775\n",
      "Iteration 5, loss = 18.02195297\n",
      "Iteration 6, loss = 18.02190269\n",
      "Iteration 7, loss = 18.02190050\n",
      "Iteration 8, loss = 18.02190617\n",
      "Iteration 9, loss = 1.96319198\n",
      "Iteration 10, loss = 8.77423302\n",
      "Iteration 11, loss = 18.02191207\n",
      "Iteration 12, loss = 18.02191353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94619291\n",
      "Iteration 2, loss = 1.97568186\n",
      "Iteration 3, loss = 1.08501316\n",
      "Iteration 4, loss = 1.51424259\n",
      "Iteration 5, loss = 1.58881560\n",
      "Iteration 6, loss = 1.32225324\n",
      "Iteration 7, loss = 1.45691648\n",
      "Iteration 8, loss = 1.38022908\n",
      "Iteration 9, loss = 1.29722618\n",
      "Iteration 10, loss = 1.31470151\n",
      "Iteration 11, loss = 1.23307598\n",
      "Iteration 12, loss = 1.20274019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94278501\n",
      "Iteration 2, loss = 1.95642929\n",
      "Iteration 3, loss = 1.15890143\n",
      "Iteration 4, loss = 1.53340400\n",
      "Iteration 5, loss = 1.59604761\n",
      "Iteration 6, loss = 1.29339509\n",
      "Iteration 7, loss = 1.48730641\n",
      "Iteration 8, loss = 1.34562344\n",
      "Iteration 9, loss = 1.32931969\n",
      "Iteration 10, loss = 1.29594237\n",
      "Iteration 11, loss = 1.25182138\n",
      "Iteration 12, loss = 1.19073998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94487116\n",
      "Iteration 2, loss = 1.96735748\n",
      "Iteration 3, loss = 1.11969451\n",
      "Iteration 4, loss = 1.52673990\n",
      "Iteration 5, loss = 1.59056020\n",
      "Iteration 6, loss = 1.30901463\n",
      "Iteration 7, loss = 1.47298804\n",
      "Iteration 8, loss = 1.36277512\n",
      "Iteration 9, loss = 1.31374826\n",
      "Iteration 10, loss = 1.30608665\n",
      "Iteration 11, loss = 1.24206336\n",
      "Iteration 12, loss = 1.19728782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94619291\n",
      "Iteration 2, loss = 14.75852101\n",
      "Iteration 3, loss = 10.54017484\n",
      "Iteration 4, loss = 1.88548224\n",
      "Iteration 5, loss = 8.61270881\n",
      "Iteration 6, loss = 12.34878138\n",
      "Iteration 7, loss = 11.88547722\n",
      "Iteration 8, loss = 8.65126001\n",
      "Iteration 9, loss = 3.52532370\n",
      "Iteration 10, loss = 2.87513508\n",
      "Iteration 11, loss = 6.10844725\n",
      "Iteration 12, loss = 6.89620595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94278501\n",
      "Iteration 2, loss = 14.94935263\n",
      "Iteration 3, loss = 10.47047926\n",
      "Iteration 4, loss = 1.58610965\n",
      "Iteration 5, loss = 8.88045831\n",
      "Iteration 6, loss = 12.63216828\n",
      "Iteration 7, loss = 12.24272274\n",
      "Iteration 8, loss = 9.11309901\n",
      "Iteration 9, loss = 4.10818483\n",
      "Iteration 10, loss = 2.24266349\n",
      "Iteration 11, loss = 5.45862612\n",
      "Iteration 12, loss = 6.17464456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94487116\n",
      "Iteration 2, loss = 14.84974672\n",
      "Iteration 3, loss = 10.50966901\n",
      "Iteration 4, loss = 1.74469239\n",
      "Iteration 5, loss = 8.74494201\n",
      "Iteration 6, loss = 12.49844131\n",
      "Iteration 7, loss = 12.08100261\n",
      "Iteration 8, loss = 8.90687443\n",
      "Iteration 9, loss = 3.84797203\n",
      "Iteration 10, loss = 2.52495526\n",
      "Iteration 11, loss = 5.74219828\n",
      "Iteration 12, loss = 6.49009200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69919076\n",
      "Iteration 2, loss = 0.69907553\n",
      "Iteration 3, loss = 0.69999869\n",
      "Iteration 4, loss = 0.71408266\n",
      "Iteration 5, loss = 0.86754784\n",
      "Iteration 6, loss = 1.77109808\n",
      "Iteration 7, loss = 1.99843061\n",
      "Iteration 8, loss = 1.57637395\n",
      "Iteration 9, loss = 1.89358381\n",
      "Iteration 10, loss = 1.67403589\n",
      "Iteration 11, loss = 1.65715376\n",
      "Iteration 12, loss = 1.71449814\n",
      "Iteration 13, loss = 1.51793890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69654036\n",
      "Iteration 2, loss = 0.69644440\n",
      "Iteration 3, loss = 0.69724310\n",
      "Iteration 4, loss = 0.70936833\n",
      "Iteration 5, loss = 0.84289465\n",
      "Iteration 6, loss = 1.68607217\n",
      "Iteration 7, loss = 2.06172178\n",
      "Iteration 8, loss = 1.54366955\n",
      "Iteration 9, loss = 1.88161206\n",
      "Iteration 10, loss = 1.71826057\n",
      "Iteration 11, loss = 1.60527723\n",
      "Iteration 12, loss = 1.75863816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69463165\n",
      "Iteration 2, loss = 0.69443610\n",
      "Iteration 3, loss = 0.69421493\n",
      "Iteration 4, loss = 0.69462437\n",
      "Iteration 5, loss = 0.70312649\n",
      "Iteration 6, loss = 0.80079628\n",
      "Iteration 7, loss = 1.50612056\n",
      "Iteration 8, loss = 2.21896248\n",
      "Iteration 9, loss = 1.38445101\n",
      "Iteration 10, loss = 1.92148257\n",
      "Iteration 11, loss = 1.72061391\n",
      "Iteration 12, loss = 1.59634411\n",
      "Iteration 13, loss = 1.77167583\n",
      "Iteration 14, loss = 1.50005089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69919076\n",
      "Iteration 2, loss = 6.75463186\n",
      "Iteration 3, loss = 4.80806656\n",
      "Iteration 4, loss = 4.01128804\n",
      "Iteration 5, loss = 1.56315038\n",
      "Iteration 6, loss = 1.87695799\n",
      "Iteration 7, loss = 1.22387875\n",
      "Iteration 8, loss = 1.11549888\n",
      "Iteration 9, loss = 1.33513273\n",
      "Iteration 10, loss = 1.06921297\n",
      "Iteration 11, loss = 1.20739355\n",
      "Iteration 12, loss = 1.11699482\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69654036\n",
      "Iteration 2, loss = 6.25360464\n",
      "Iteration 3, loss = 5.37508782\n",
      "Iteration 4, loss = 4.53223883\n",
      "Iteration 5, loss = 1.18862316\n",
      "Iteration 6, loss = 1.92248480\n",
      "Iteration 7, loss = 0.97529789\n",
      "Iteration 8, loss = 0.97719878\n",
      "Iteration 9, loss = 1.28397440\n",
      "Iteration 10, loss = 0.84540376\n",
      "Iteration 11, loss = 1.40178552\n",
      "Iteration 12, loss = 1.00760287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69463165\n",
      "Iteration 2, loss = 2.14775926\n",
      "Iteration 3, loss = 9.46291149\n",
      "Iteration 4, loss = 8.54821691\n",
      "Iteration 5, loss = 2.73736769\n",
      "Iteration 6, loss = 5.42518473\n",
      "Iteration 7, loss = 8.19791471\n",
      "Iteration 8, loss = 7.27232295\n",
      "Iteration 9, loss = 3.75926918\n",
      "Iteration 10, loss = 1.50782030\n",
      "Iteration 11, loss = 3.82783869\n",
      "Iteration 12, loss = 3.63979737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71496893\n",
      "Iteration 2, loss = 0.75596953\n",
      "Iteration 3, loss = 0.75160811\n",
      "Iteration 4, loss = 0.76380539\n",
      "Iteration 5, loss = 0.76900940\n",
      "Iteration 6, loss = 0.77314344\n",
      "Iteration 7, loss = 0.77552567\n",
      "Iteration 8, loss = 0.77518586\n",
      "Iteration 9, loss = 0.77435244\n",
      "Iteration 10, loss = 0.77135318\n",
      "Iteration 11, loss = 0.76850942\n",
      "Iteration 12, loss = 0.76412182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72019712\n",
      "Iteration 2, loss = 0.76634260\n",
      "Iteration 3, loss = 0.75841399\n",
      "Iteration 4, loss = 0.77110949\n",
      "Iteration 5, loss = 0.77408609\n",
      "Iteration 6, loss = 0.77841229\n",
      "Iteration 7, loss = 0.77860718\n",
      "Iteration 8, loss = 0.77882177\n",
      "Iteration 9, loss = 0.77615278\n",
      "Iteration 10, loss = 0.77397407\n",
      "Iteration 11, loss = 0.76967846\n",
      "Iteration 12, loss = 0.76617656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72306759\n",
      "Iteration 2, loss = 0.76684294\n",
      "Iteration 3, loss = 0.76080012\n",
      "Iteration 4, loss = 0.77327089\n",
      "Iteration 5, loss = 0.77737760\n",
      "Iteration 6, loss = 0.78148935\n",
      "Iteration 7, loss = 0.78270100\n",
      "Iteration 8, loss = 0.78246930\n",
      "Iteration 9, loss = 0.78060560\n",
      "Iteration 10, loss = 0.77784960\n",
      "Iteration 11, loss = 0.77416494\n",
      "Iteration 12, loss = 0.77007036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71496893\n",
      "Iteration 2, loss = 13.07916614\n",
      "Iteration 3, loss = 5.14022024\n",
      "Iteration 4, loss = 4.91247287\n",
      "Iteration 5, loss = 7.34990324\n",
      "Iteration 6, loss = 5.74615526\n",
      "Iteration 7, loss = 1.74084562\n",
      "Iteration 8, loss = 3.82515422\n",
      "Iteration 9, loss = 6.03295765\n",
      "Iteration 10, loss = 5.79489086\n",
      "Iteration 11, loss = 3.73443516\n",
      "Iteration 12, loss = 0.73453981\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72019712\n",
      "Iteration 2, loss = 12.91525190\n",
      "Iteration 3, loss = 5.23795338\n",
      "Iteration 4, loss = 4.72320044\n",
      "Iteration 5, loss = 7.01392952\n",
      "Iteration 6, loss = 5.21596323\n",
      "Iteration 7, loss = 1.10837727\n",
      "Iteration 8, loss = 4.33451711\n",
      "Iteration 9, loss = 6.33225491\n",
      "Iteration 10, loss = 5.92895038\n",
      "Iteration 11, loss = 3.74094186\n",
      "Iteration 12, loss = 0.71186438\n",
      "Iteration 13, loss = 3.27030311\n",
      "Iteration 14, loss = 4.60483656\n",
      "Iteration 15, loss = 4.21500693\n",
      "Iteration 16, loss = 2.44758297\n",
      "Iteration 17, loss = 0.77223383\n",
      "Iteration 18, loss = 2.40556441\n",
      "Iteration 19, loss = 2.88334103\n",
      "Iteration 20, loss = 2.11310850\n",
      "Iteration 21, loss = 0.72798478\n",
      "Iteration 22, loss = 1.69914478\n",
      "Iteration 23, loss = 2.31989651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72306759\n",
      "Iteration 2, loss = 13.00493882\n",
      "Iteration 3, loss = 5.19960483\n",
      "Iteration 4, loss = 4.81726098\n",
      "Iteration 5, loss = 7.18240713\n",
      "Iteration 6, loss = 5.48422672\n",
      "Iteration 7, loss = 1.41360714\n",
      "Iteration 8, loss = 4.14340020\n",
      "Iteration 9, loss = 6.29755172\n",
      "Iteration 10, loss = 6.02354506\n",
      "Iteration 11, loss = 3.94115849\n",
      "Iteration 12, loss = 0.81163257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94619291\n",
      "Iteration 2, loss = 17.90405504\n",
      "Iteration 3, loss = 1.48110818\n",
      "Iteration 4, loss = 1.82156759\n",
      "Iteration 5, loss = 0.73444467\n",
      "Iteration 6, loss = 1.52184962\n",
      "Iteration 7, loss = 6.27940204\n",
      "Iteration 8, loss = 1.74960725\n",
      "Iteration 9, loss = 1.69797347\n",
      "Iteration 10, loss = 1.40787573\n",
      "Iteration 11, loss = 0.72916664\n",
      "Iteration 12, loss = 0.70422902\n",
      "Iteration 13, loss = 0.71152227\n",
      "Iteration 14, loss = 0.69691338\n",
      "Iteration 15, loss = 0.69371941\n",
      "Iteration 16, loss = 0.69316278\n",
      "Iteration 17, loss = 0.69249526\n",
      "Iteration 18, loss = 0.69193016\n",
      "Iteration 19, loss = 0.69154183\n",
      "Iteration 20, loss = 0.69101688\n",
      "Iteration 21, loss = 0.69083970\n",
      "Iteration 22, loss = 0.69151968\n",
      "Iteration 23, loss = 0.70232356\n",
      "Iteration 24, loss = 0.79572997\n",
      "Iteration 25, loss = 1.42062379\n",
      "Iteration 26, loss = 2.00621982\n",
      "Iteration 27, loss = 0.86784638\n",
      "Iteration 28, loss = 0.72889791\n",
      "Iteration 29, loss = 0.70075201\n",
      "Iteration 30, loss = 0.68909483\n",
      "Iteration 31, loss = 0.68917064\n",
      "Iteration 32, loss = 0.68941805\n",
      "Iteration 33, loss = 0.68962855\n",
      "Iteration 34, loss = 0.68983205\n",
      "Iteration 35, loss = 0.69000936\n",
      "Iteration 36, loss = 0.69015140\n",
      "Iteration 37, loss = 0.69025762\n",
      "Iteration 38, loss = 0.69032598\n",
      "Iteration 39, loss = 0.69035493\n",
      "Iteration 40, loss = 0.69034431\n",
      "Iteration 41, loss = 0.69029456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94278501\n",
      "Iteration 2, loss = 18.13963447\n",
      "Iteration 3, loss = 1.81489092\n",
      "Iteration 4, loss = 2.01821209\n",
      "Iteration 5, loss = 0.73030755\n",
      "Iteration 6, loss = 1.40613406\n",
      "Iteration 7, loss = 6.47858915\n",
      "Iteration 8, loss = 1.66318514\n",
      "Iteration 9, loss = 1.96741235\n",
      "Iteration 10, loss = 1.40720366\n",
      "Iteration 11, loss = 0.71794596\n",
      "Iteration 12, loss = 0.70452117\n",
      "Iteration 13, loss = 0.71086277\n",
      "Iteration 14, loss = 0.69740753\n",
      "Iteration 15, loss = 0.69305455\n",
      "Iteration 16, loss = 0.69291902\n",
      "Iteration 17, loss = 0.69242628\n",
      "Iteration 18, loss = 0.69198269\n",
      "Iteration 19, loss = 0.69170534\n",
      "Iteration 20, loss = 0.69128786\n",
      "Iteration 21, loss = 0.69116898\n",
      "Iteration 22, loss = 0.69174658\n",
      "Iteration 23, loss = 0.70158556\n",
      "Iteration 24, loss = 0.79245587\n",
      "Iteration 25, loss = 1.45477300\n",
      "Iteration 26, loss = 2.07471489\n",
      "Iteration 27, loss = 0.93016244\n",
      "Iteration 28, loss = 0.75622870\n",
      "Iteration 29, loss = 0.70602808\n",
      "Iteration 30, loss = 0.68992814\n",
      "Iteration 31, loss = 0.69019515\n",
      "Iteration 32, loss = 0.69043769\n",
      "Iteration 33, loss = 0.69060866\n",
      "Iteration 34, loss = 0.69079401\n",
      "Iteration 35, loss = 0.69097129\n",
      "Iteration 36, loss = 0.69111951\n",
      "Iteration 37, loss = 0.69123387\n",
      "Iteration 38, loss = 0.69131242\n",
      "Iteration 39, loss = 0.69135440\n",
      "Iteration 40, loss = 0.69136027\n",
      "Iteration 41, loss = 0.69133071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94487116\n",
      "Iteration 2, loss = 18.02184463\n",
      "Iteration 3, loss = 1.63261029\n",
      "Iteration 4, loss = 1.91876074\n",
      "Iteration 5, loss = 0.73281294\n",
      "Iteration 6, loss = 1.47569361\n",
      "Iteration 7, loss = 6.40707462\n",
      "Iteration 8, loss = 1.67469594\n",
      "Iteration 9, loss = 1.83052663\n",
      "Iteration 10, loss = 1.41532872\n",
      "Iteration 11, loss = 0.72341340\n",
      "Iteration 12, loss = 0.70444003\n",
      "Iteration 13, loss = 0.71161852\n",
      "Iteration 14, loss = 0.69684846\n",
      "Iteration 15, loss = 0.69286114\n",
      "Iteration 16, loss = 0.69254735\n",
      "Iteration 17, loss = 0.69194366\n",
      "Iteration 18, loss = 0.69142825\n",
      "Iteration 19, loss = 0.69109121\n",
      "Iteration 20, loss = 0.69061434\n",
      "Iteration 21, loss = 0.69046586\n",
      "Iteration 22, loss = 0.69109287\n",
      "Iteration 23, loss = 0.70141774\n",
      "Iteration 24, loss = 0.79344621\n",
      "Iteration 25, loss = 1.43610847\n",
      "Iteration 26, loss = 2.03781191\n",
      "Iteration 27, loss = 0.90212371\n",
      "Iteration 28, loss = 0.74349179\n",
      "Iteration 29, loss = 0.70337190\n",
      "Iteration 30, loss = 0.68951854\n",
      "Iteration 31, loss = 0.68975478\n",
      "Iteration 32, loss = 0.69005541\n",
      "Iteration 33, loss = 0.69029175\n",
      "Iteration 34, loss = 0.69052595\n",
      "Iteration 35, loss = 0.69073862\n",
      "Iteration 36, loss = 0.69091528\n",
      "Iteration 37, loss = 0.69105385\n",
      "Iteration 38, loss = 0.69115304\n",
      "Iteration 39, loss = 0.69121234\n",
      "Iteration 40, loss = 0.69123242\n",
      "Iteration 41, loss = 0.69121424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94619291\n",
      "Iteration 2, loss = 17.90406018\n",
      "Iteration 3, loss = 17.90405791\n",
      "Iteration 4, loss = 17.90405533\n",
      "Iteration 5, loss = 18.13963528\n",
      "Iteration 6, loss = 18.13963728\n",
      "Iteration 7, loss = 18.13963942\n",
      "Iteration 8, loss = 10.15154775\n",
      "Iteration 9, loss = 17.90406725\n",
      "Iteration 10, loss = 17.90406904\n",
      "Iteration 11, loss = 17.90406818\n",
      "Iteration 12, loss = 17.90406567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94278501\n",
      "Iteration 2, loss = 18.13963961\n",
      "Iteration 3, loss = 18.13963723\n",
      "Iteration 4, loss = 18.13963469\n",
      "Iteration 5, loss = 17.90405595\n",
      "Iteration 6, loss = 17.90405812\n",
      "Iteration 7, loss = 17.90406044\n",
      "Iteration 8, loss = 12.68395868\n",
      "Iteration 9, loss = 18.13964745\n",
      "Iteration 10, loss = 18.13964926\n",
      "Iteration 11, loss = 18.13964836\n",
      "Iteration 12, loss = 18.13964580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94487116\n",
      "Iteration 2, loss = 18.02184975\n",
      "Iteration 3, loss = 18.02184744\n",
      "Iteration 4, loss = 18.02184489\n",
      "Iteration 5, loss = 18.02184549\n",
      "Iteration 6, loss = 18.02184755\n",
      "Iteration 7, loss = 18.02184976\n",
      "Iteration 8, loss = 11.35924795\n",
      "Iteration 9, loss = 18.02185712\n",
      "Iteration 10, loss = 18.02185891\n",
      "Iteration 11, loss = 18.02185803\n",
      "Iteration 12, loss = 18.02185551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69919076\n",
      "Iteration 2, loss = 0.71523460\n",
      "Iteration 3, loss = 9.03098938\n",
      "Iteration 4, loss = 17.90405489\n",
      "Iteration 5, loss = 1.21536112\n",
      "Iteration 6, loss = 8.31814260\n",
      "Iteration 7, loss = 1.84857918\n",
      "Iteration 8, loss = 2.26900019\n",
      "Iteration 9, loss = 3.23027787\n",
      "Iteration 10, loss = 0.76328745\n",
      "Iteration 11, loss = 1.50497570\n",
      "Iteration 12, loss = 0.96491255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69654036\n",
      "Iteration 2, loss = 0.71040642\n",
      "Iteration 3, loss = 8.43130499\n",
      "Iteration 4, loss = 17.90405490\n",
      "Iteration 5, loss = 1.84851702\n",
      "Iteration 6, loss = 8.87864993\n",
      "Iteration 7, loss = 2.39227669\n",
      "Iteration 8, loss = 2.22579443\n",
      "Iteration 9, loss = 3.20664507\n",
      "Iteration 10, loss = 0.83381841\n",
      "Iteration 11, loss = 1.79251710\n",
      "Iteration 12, loss = 0.90139649\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69463165\n",
      "Iteration 2, loss = 0.69359511\n",
      "Iteration 3, loss = 2.10892772\n",
      "Iteration 4, loss = 18.02184474\n",
      "Iteration 5, loss = 1.95758574\n",
      "Iteration 6, loss = 7.92552427\n",
      "Iteration 7, loss = 3.28099112\n",
      "Iteration 8, loss = 1.94217702\n",
      "Iteration 9, loss = 2.71436268\n",
      "Iteration 10, loss = 2.74317564\n",
      "Iteration 11, loss = 2.59147125\n",
      "Iteration 12, loss = 0.76910034\n",
      "Iteration 13, loss = 0.84206634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69919076\n",
      "Iteration 2, loss = 17.90406003\n",
      "Iteration 3, loss = 18.13963964\n",
      "Iteration 4, loss = 18.13964002\n",
      "Iteration 5, loss = 13.45873657\n",
      "Iteration 6, loss = 12.82851300\n",
      "Iteration 7, loss = 9.43580724\n",
      "Iteration 8, loss = 8.30379232\n",
      "Iteration 9, loss = 5.62902456\n",
      "Iteration 10, loss = 5.04739470\n",
      "Iteration 11, loss = 4.42657878\n",
      "Iteration 12, loss = 4.04972450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69654036\n",
      "Iteration 2, loss = 17.90406001\n",
      "Iteration 3, loss = 18.13964010\n",
      "Iteration 4, loss = 18.13964009\n",
      "Iteration 5, loss = 5.74983390\n",
      "Iteration 6, loss = 6.45747172\n",
      "Iteration 7, loss = 14.19713950\n",
      "Iteration 8, loss = 12.09450709\n",
      "Iteration 9, loss = 2.10714556\n",
      "Iteration 10, loss = 2.46329343\n",
      "Iteration 11, loss = 5.58407360\n",
      "Iteration 12, loss = 4.57022527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69463165\n",
      "Iteration 2, loss = 18.02184956\n",
      "Iteration 3, loss = 18.02185159\n",
      "Iteration 4, loss = 18.02185198\n",
      "Iteration 5, loss = 18.02185208\n",
      "Iteration 6, loss = 18.02185411\n",
      "Iteration 7, loss = 18.02185476\n",
      "Iteration 8, loss = 18.02185466\n",
      "Iteration 9, loss = 11.22301775\n",
      "Iteration 10, loss = 18.02185665\n",
      "Iteration 11, loss = 18.02185736\n",
      "Iteration 12, loss = 18.02185681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71496893\n",
      "Iteration 2, loss = 5.49259548\n",
      "Iteration 3, loss = 16.23667340\n",
      "Iteration 4, loss = 1.86633105\n",
      "Iteration 5, loss = 1.20279305\n",
      "Iteration 6, loss = 1.46694564\n",
      "Iteration 7, loss = 0.77302812\n",
      "Iteration 8, loss = 1.92319031\n",
      "Iteration 9, loss = 4.19494420\n",
      "Iteration 10, loss = 1.51590930\n",
      "Iteration 11, loss = 1.39002238\n",
      "Iteration 12, loss = 0.86134884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72019712\n",
      "Iteration 2, loss = 5.78715649\n",
      "Iteration 3, loss = 15.55076648\n",
      "Iteration 4, loss = 1.60165266\n",
      "Iteration 5, loss = 1.15420337\n",
      "Iteration 6, loss = 1.51060799\n",
      "Iteration 7, loss = 0.73396860\n",
      "Iteration 8, loss = 1.37948163\n",
      "Iteration 9, loss = 4.58889913\n",
      "Iteration 10, loss = 1.60651827\n",
      "Iteration 11, loss = 1.55845307\n",
      "Iteration 12, loss = 0.99761826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72306759\n",
      "Iteration 2, loss = 5.66028887\n",
      "Iteration 3, loss = 15.86458130\n",
      "Iteration 4, loss = 1.73518171\n",
      "Iteration 5, loss = 1.17288293\n",
      "Iteration 6, loss = 1.48815642\n",
      "Iteration 7, loss = 0.75512313\n",
      "Iteration 8, loss = 1.69871670\n",
      "Iteration 9, loss = 4.46107664\n",
      "Iteration 10, loss = 1.46754282\n",
      "Iteration 11, loss = 1.46815705\n",
      "Iteration 12, loss = 0.92983935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71496893\n",
      "Iteration 2, loss = 18.13963919\n",
      "Iteration 3, loss = 18.13963499\n",
      "Iteration 4, loss = 17.90405554\n",
      "Iteration 5, loss = 17.90405652\n",
      "Iteration 6, loss = 17.90405678\n",
      "Iteration 7, loss = 11.68667128\n",
      "Iteration 8, loss = 18.13963858\n",
      "Iteration 9, loss = 18.13963951\n",
      "Iteration 10, loss = 18.13963936\n",
      "Iteration 11, loss = 18.13963888\n",
      "Iteration 12, loss = 2.26716088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72019712\n",
      "Iteration 2, loss = 17.90405976\n",
      "Iteration 3, loss = 17.90405563\n",
      "Iteration 4, loss = 18.13963495\n",
      "Iteration 5, loss = 18.13963578\n",
      "Iteration 6, loss = 18.13963591\n",
      "Iteration 7, loss = 8.49444962\n",
      "Iteration 8, loss = 17.90405878\n",
      "Iteration 9, loss = 17.90405980\n",
      "Iteration 10, loss = 17.90405976\n",
      "Iteration 11, loss = 17.90405941\n",
      "Iteration 12, loss = 3.66227008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72306759\n",
      "Iteration 2, loss = 18.02184933\n",
      "Iteration 3, loss = 18.02184519\n",
      "Iteration 4, loss = 18.02184512\n",
      "Iteration 5, loss = 18.02184602\n",
      "Iteration 6, loss = 18.02184621\n",
      "Iteration 7, loss = 10.11231632\n",
      "Iteration 8, loss = 18.02184852\n",
      "Iteration 9, loss = 18.02184948\n",
      "Iteration 10, loss = 18.02184939\n",
      "Iteration 11, loss = 18.02184898\n",
      "Iteration 12, loss = 2.96476024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94619291\n",
      "Iteration 2, loss = 17.90406556\n",
      "Iteration 3, loss = 18.14082190\n",
      "Iteration 4, loss = 31.24748197\n",
      "Iteration 5, loss = 1593130.57345164\n",
      "Iteration 6, loss = 674721615120276864.00000000\n",
      "Iteration 7, loss = 71489964965532067981391872135778747285504.00000000\n",
      "Iteration 8, loss = 1241602841879716132657276261779287956465127563304486086842694342736668238032802620112896.00000000\n",
      "Iteration 9, loss = 241832731666217620232286471287732722241810615358817033490297854683021174534672569333708967354215670269484206720918789830993911364305659698957688300898626598955893342140778594435072.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.94278501\n",
      "Iteration 2, loss = 18.13964461\n",
      "Iteration 3, loss = 17.90522976\n",
      "Iteration 4, loss = 30.39010711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 2171218.85087102\n",
      "Iteration 6, loss = 65303708356746969088.00000000\n",
      "Iteration 7, loss = 9424634019463547196512502111189065866936320.00000000\n",
      "Iteration 8, loss = 1225514768270344477639165937235218828415725473912258899622314203058336067195221236349291986944.00000000\n",
      "Iteration 9, loss = 3319142352461977694587328487172214948657534776149198469662436153005099172548653532539806991603556673524282816034350325731140855361170533200200614073301552004056816002919748067575280091267072.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.94487116\n",
      "Iteration 2, loss = 18.02185491\n",
      "Iteration 3, loss = 18.02301940\n",
      "Iteration 4, loss = 30.76215187\n",
      "Iteration 5, loss = 1412543.08209396\n",
      "Iteration 6, loss = 30646372622300418048.00000000\n",
      "Iteration 7, loss = 54133549443799208652824764705391708733440.00000000\n",
      "Iteration 8, loss = 46863155720140887438110889375706351511906690911194915950206040445337290043078460559065088.00000000\n",
      "Iteration 9, loss = 126581619440749638723648059157077009613157807618219225139699410874497429054711424532168673083106628341036185791459074016631414870203568401169674956559440752484875994759679788974080.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.94619291\n",
      "Iteration 2, loss = 17.90458440\n",
      "Iteration 3, loss = 18.14066200\n",
      "Iteration 4, loss = 18.14047957\n",
      "Iteration 5, loss = 18.14019199\n",
      "Iteration 6, loss = 17.90447153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 17.90434664\n",
      "Iteration 8, loss = 17.90424227\n",
      "Iteration 9, loss = 18.13980454\n",
      "Iteration 10, loss = 18.13987270\n",
      "Iteration 11, loss = 17.90439081\n",
      "Iteration 12, loss = 17.90449656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94278501\n",
      "Iteration 2, loss = 18.14016383\n",
      "Iteration 3, loss = 17.90507679\n",
      "Iteration 4, loss = 17.90489594\n",
      "Iteration 5, loss = 17.90460549\n",
      "Iteration 6, loss = 18.14003540\n",
      "Iteration 7, loss = 18.13990868\n",
      "Iteration 8, loss = 18.13980598\n",
      "Iteration 9, loss = 17.90421102\n",
      "Iteration 10, loss = 17.90427854\n",
      "Iteration 11, loss = 18.13995489\n",
      "Iteration 12, loss = 18.14006076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94487116\n",
      "Iteration 2, loss = 18.02237056\n",
      "Iteration 3, loss = 18.02286284\n",
      "Iteration 4, loss = 18.02268242\n",
      "Iteration 5, loss = 18.02239534\n",
      "Iteration 6, loss = 18.02225112\n",
      "Iteration 7, loss = 18.02212606\n",
      "Iteration 8, loss = 18.02202304\n",
      "Iteration 9, loss = 18.02200664\n",
      "Iteration 10, loss = 18.02207398\n",
      "Iteration 11, loss = 18.02217057\n",
      "Iteration 12, loss = 18.02227567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69919076\n",
      "Iteration 2, loss = 1.95478497\n",
      "Iteration 3, loss = 18.13966130\n",
      "Iteration 4, loss = 18.14174491\n",
      "Iteration 5, loss = 60.86337171\n",
      "Iteration 6, loss = 9460961068.50806427\n",
      "Iteration 7, loss = 759818005853432902628409344.00000000\n",
      "Iteration 8, loss = 2952600935670656178260675052928266205336053531714095784394752.00000000\n",
      "Iteration 9, loss = 74002327384252279823475005249241784333244823596970642960383192519946910589824809996263099950675271016525807691371947161253052416.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69654036\n",
      "Iteration 2, loss = 1.82030715\n",
      "Iteration 3, loss = 18.13966068\n",
      "Iteration 4, loss = 18.14194859\n",
      "Iteration 5, loss = 79.05832590\n",
      "Iteration 6, loss = 9843986844.60318756\n",
      "Iteration 7, loss = 978042435145227150693498880.00000000\n",
      "Iteration 8, loss = 2508904659720252099843000496880678673929315800650921755541504.00000000\n",
      "Iteration 9, loss = 63530197705128598665382236065884585907601615155135329258173084184423085814318139504321801056368757914148115162687156870394675200.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.69463165\n",
      "Iteration 2, loss = 0.77289491\n",
      "Iteration 3, loss = 18.02184980\n",
      "Iteration 4, loss = 18.02237372\n",
      "Iteration 5, loss = 21.82390635\n",
      "Iteration 6, loss = 35268761.59892570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 14274540753485685587968.00000000\n",
      "Iteration 8, loss = 490746140594571938185167990235591655446716954443776.00000000\n",
      "Iteration 9, loss = 2763672034009789009268146919744029604461010512588226892016172396507341564275550714889394649971489086638129152.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.69919076\n",
      "Iteration 2, loss = 17.90456778\n",
      "Iteration 3, loss = 18.14044898\n",
      "Iteration 4, loss = 18.14037496\n",
      "Iteration 5, loss = 17.90490750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 17.90489827\n",
      "Iteration 7, loss = 17.90485528\n",
      "Iteration 8, loss = 18.14047516\n",
      "Iteration 9, loss = 18.14051778\n",
      "Iteration 10, loss = 18.14055030\n",
      "Iteration 11, loss = 17.90507430\n",
      "Iteration 12, loss = 17.90510066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69654036\n",
      "Iteration 2, loss = 17.90456550\n",
      "Iteration 3, loss = 18.14037787\n",
      "Iteration 4, loss = 18.14034296\n",
      "Iteration 5, loss = 17.90478414\n",
      "Iteration 6, loss = 17.90478341\n",
      "Iteration 7, loss = 18.14037420\n",
      "Iteration 8, loss = 18.14038946\n",
      "Iteration 9, loss = 18.14037428\n",
      "Iteration 10, loss = 11.70791497\n",
      "Iteration 11, loss = 17.90476214\n",
      "Iteration 12, loss = 17.90475248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69463165\n",
      "Iteration 2, loss = 18.02235108\n",
      "Iteration 3, loss = 18.02262275\n",
      "Iteration 4, loss = 18.02275066\n",
      "Iteration 5, loss = 18.02302748\n",
      "Iteration 6, loss = 18.02305681\n",
      "Iteration 7, loss = 18.02302252\n",
      "Iteration 8, loss = 18.02302207\n",
      "Iteration 9, loss = 2.23612404\n",
      "Iteration 10, loss = 18.02299475\n",
      "Iteration 11, loss = 18.02299256\n",
      "Iteration 12, loss = 18.02297542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71496893\n",
      "Iteration 2, loss = 18.13963463\n",
      "Iteration 3, loss = 17.90411402\n",
      "Iteration 4, loss = 18.19722495\n",
      "Iteration 5, loss = 72843.87320198\n",
      "Iteration 6, loss = 78041269768424544.00000000\n",
      "Iteration 7, loss = 133744259571044814457071605307855480029184.00000000\n",
      "Iteration 8, loss = 263198815057967406083671390615383996858978438407516081133341427946823648139214821190008832.00000000\n",
      "Iteration 9, loss = 1521228091090469277772496737727761916127334813720158963089371254881884404803590494451027085789069808802218310771920082460141290210633931776123052624086955906060801224823558236134212370432.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.72019712\n",
      "Iteration 2, loss = 17.90405529\n",
      "Iteration 3, loss = 18.13969879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 17.97088232\n",
      "Iteration 5, loss = 94208.87123022\n",
      "Iteration 6, loss = 126010535038260080.00000000\n",
      "Iteration 7, loss = 334772498671038551190152645481803034918912.00000000\n",
      "Iteration 8, loss = 1591732336424629216494146263177800927210036874089201807715073645344277273463505592682283008.00000000\n",
      "Iteration 9, loss = 53416585087105122801845389345170332766010974180405204690780431904063079325828126747956807453286204195493668536044455744761421969308280558898604574873874592624798717187652747837527786782720.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.72306759\n",
      "Iteration 2, loss = 18.02184484\n",
      "Iteration 3, loss = 18.02190609\n",
      "Iteration 4, loss = 18.08374384\n",
      "Iteration 5, loss = 83183.27664824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 100080908933539808.00000000\n",
      "Iteration 7, loss = 217372512776881584693193460865141198290944.00000000\n",
      "Iteration 8, loss = 683686850681653777686810852923833445633553907138317983699280176405746705345549438034116608.00000000\n",
      "Iteration 9, loss = 10144171376040893626581476598076980789671748154496028413924458679190947368589830073669792261629829888481832467218941077316378573348235405008739200593654754948246139190864637546291768852480.00000000\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.71496893\n",
      "Iteration 2, loss = 18.14016390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 17.90491924\n",
      "Iteration 4, loss = 17.90462568\n",
      "Iteration 5, loss = 18.13993194\n",
      "Iteration 6, loss = 18.13977257\n",
      "Iteration 7, loss = 17.43084708\n",
      "Iteration 8, loss = 17.90418006\n",
      "Iteration 9, loss = 17.90427893\n",
      "Iteration 10, loss = 17.90435074\n",
      "Iteration 11, loss = 17.90439891\n",
      "Iteration 12, loss = 18.14002552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72019712\n",
      "Iteration 2, loss = 17.90458447\n",
      "Iteration 3, loss = 18.14049876\n",
      "Iteration 4, loss = 18.14020553\n",
      "Iteration 5, loss = 17.90435490\n",
      "Iteration 6, loss = 17.90419320\n",
      "Iteration 7, loss = 17.90411483\n",
      "Iteration 8, loss = 18.13975017\n",
      "Iteration 9, loss = 18.13984556\n",
      "Iteration 10, loss = 18.13991410\n",
      "Iteration 11, loss = 18.13995901\n",
      "Iteration 12, loss = 17.90442371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72306759\n",
      "Iteration 2, loss = 18.02237063\n",
      "Iteration 3, loss = 18.02270330\n",
      "Iteration 4, loss = 18.02241203\n",
      "Iteration 5, loss = 18.02214157\n",
      "Iteration 6, loss = 18.02198195\n",
      "Iteration 7, loss = 18.02190603\n",
      "Iteration 8, loss = 18.02196381\n",
      "Iteration 9, loss = 18.02206018\n",
      "Iteration 10, loss = 18.02212995\n",
      "Iteration 11, loss = 18.02217631\n",
      "Iteration 12, loss = 18.02222176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74404879\n",
      "Iteration 2, loss = 1.76514532\n",
      "Iteration 3, loss = 4.16288160\n",
      "Iteration 4, loss = 1.09780692\n",
      "Iteration 5, loss = 2.90031993\n",
      "Iteration 6, loss = 2.62725031\n",
      "Iteration 7, loss = 1.76408775\n",
      "Iteration 8, loss = 2.85656337\n",
      "Iteration 9, loss = 1.29077721\n",
      "Iteration 10, loss = 2.15543320\n",
      "Iteration 11, loss = 1.68896631\n",
      "Iteration 12, loss = 1.51911956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73757900\n",
      "Iteration 2, loss = 1.67127675\n",
      "Iteration 3, loss = 4.26258010\n",
      "Iteration 4, loss = 0.92557322\n",
      "Iteration 5, loss = 2.31338666\n",
      "Iteration 6, loss = 3.37940695\n",
      "Iteration 7, loss = 1.11265089\n",
      "Iteration 8, loss = 2.61810566\n",
      "Iteration 9, loss = 1.96135165\n",
      "Iteration 10, loss = 1.80121418\n",
      "Iteration 11, loss = 2.05940215\n",
      "Iteration 12, loss = 1.36101457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73736508\n",
      "Iteration 2, loss = 1.71297313\n",
      "Iteration 3, loss = 4.21472143\n",
      "Iteration 4, loss = 1.00307959\n",
      "Iteration 5, loss = 2.64127563\n",
      "Iteration 6, loss = 2.96307990\n",
      "Iteration 7, loss = 1.45596085\n",
      "Iteration 8, loss = 2.91233586\n",
      "Iteration 9, loss = 1.39145496\n",
      "Iteration 10, loss = 2.12685447\n",
      "Iteration 11, loss = 1.78938656\n",
      "Iteration 12, loss = 1.46366274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74404879\n",
      "Iteration 2, loss = 17.90413441\n",
      "Iteration 3, loss = 17.90413418\n",
      "Iteration 4, loss = 10.61123860\n",
      "Iteration 5, loss = 16.54845456\n",
      "Iteration 6, loss = 11.34314867\n",
      "Iteration 7, loss = 0.78413055\n",
      "Iteration 8, loss = 10.82080301\n",
      "Iteration 9, loss = 13.62732770\n",
      "Iteration 10, loss = 11.14933434\n",
      "Iteration 11, loss = 5.55154052\n",
      "Iteration 12, loss = 1.73983018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73757900\n",
      "Iteration 2, loss = 18.13971384\n",
      "Iteration 3, loss = 18.07845683\n",
      "Iteration 4, loss = 11.44682212\n",
      "Iteration 5, loss = 17.80634621\n",
      "Iteration 6, loss = 12.96383099\n",
      "Iteration 7, loss = 2.44086709\n",
      "Iteration 8, loss = 11.27236014\n",
      "Iteration 9, loss = 16.19696703\n",
      "Iteration 10, loss = 15.29161785\n",
      "Iteration 11, loss = 10.79615498\n",
      "Iteration 12, loss = 4.31655046\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73736508\n",
      "Iteration 2, loss = 18.02192349\n",
      "Iteration 3, loss = 18.02132075\n",
      "Iteration 4, loss = 11.05072628\n",
      "Iteration 5, loss = 17.17625681\n",
      "Iteration 6, loss = 12.16772762\n",
      "Iteration 7, loss = 1.49305548\n",
      "Iteration 8, loss = 11.97716850\n",
      "Iteration 9, loss = 16.64761671\n",
      "Iteration 10, loss = 15.55289038\n",
      "Iteration 11, loss = 10.92257335\n",
      "Iteration 12, loss = 4.35078451\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93894923\n",
      "Iteration 2, loss = 2.94888699\n",
      "Iteration 3, loss = 1.34198992\n",
      "Iteration 4, loss = 2.54351080\n",
      "Iteration 5, loss = 1.80357707\n",
      "Iteration 6, loss = 1.99070425\n",
      "Iteration 7, loss = 1.94754666\n",
      "Iteration 8, loss = 1.61074105\n",
      "Iteration 9, loss = 1.74393002\n",
      "Iteration 10, loss = 1.49213870\n",
      "Iteration 11, loss = 1.40165474\n",
      "Iteration 12, loss = 1.36157623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93954706\n",
      "Iteration 2, loss = 2.92939430\n",
      "Iteration 3, loss = 1.46647336\n",
      "Iteration 4, loss = 2.51553471\n",
      "Iteration 5, loss = 1.88596088\n",
      "Iteration 6, loss = 1.89074016\n",
      "Iteration 7, loss = 2.06630201\n",
      "Iteration 8, loss = 1.47380139\n",
      "Iteration 9, loss = 1.80971817\n",
      "Iteration 10, loss = 1.43925207\n",
      "Iteration 11, loss = 1.42033061\n",
      "Iteration 12, loss = 1.35010480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93574500\n",
      "Iteration 2, loss = 2.93097172\n",
      "Iteration 3, loss = 1.40773955\n",
      "Iteration 4, loss = 2.53544891\n",
      "Iteration 5, loss = 1.83020296\n",
      "Iteration 6, loss = 1.95126452\n",
      "Iteration 7, loss = 1.99734053\n",
      "Iteration 8, loss = 1.54675241\n",
      "Iteration 9, loss = 1.78012853\n",
      "Iteration 10, loss = 1.45879304\n",
      "Iteration 11, loss = 1.41325915\n",
      "Iteration 12, loss = 1.35437400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93894923\n",
      "Iteration 2, loss = 17.90413358\n",
      "Iteration 3, loss = 17.90413343\n",
      "Iteration 4, loss = 4.60390550\n",
      "Iteration 5, loss = 18.13971280\n",
      "Iteration 6, loss = 18.13971288\n",
      "Iteration 7, loss = 18.13971288\n",
      "Iteration 8, loss = 17.51570515\n",
      "Iteration 9, loss = 7.94120604\n",
      "Iteration 10, loss = 3.11373818\n",
      "Iteration 11, loss = 8.20637049\n",
      "Iteration 12, loss = 9.12264839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93954706\n",
      "Iteration 2, loss = 18.13971302\n",
      "Iteration 3, loss = 18.13971286\n",
      "Iteration 4, loss = 3.97665780\n",
      "Iteration 5, loss = 17.90413337\n",
      "Iteration 6, loss = 17.90413346\n",
      "Iteration 7, loss = 17.90413347\n",
      "Iteration 8, loss = 17.90413344\n",
      "Iteration 9, loss = 9.51773323\n",
      "Iteration 10, loss = 1.46458338\n",
      "Iteration 11, loss = 6.90142039\n",
      "Iteration 12, loss = 7.99123026\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93574500\n",
      "Iteration 2, loss = 18.02192267\n",
      "Iteration 3, loss = 18.02192252\n",
      "Iteration 4, loss = 4.25358059\n",
      "Iteration 5, loss = 18.02192246\n",
      "Iteration 6, loss = 18.02192254\n",
      "Iteration 7, loss = 18.02192255\n",
      "Iteration 8, loss = 18.02192252\n",
      "Iteration 9, loss = 8.74489780\n",
      "Iteration 10, loss = 2.26939751\n",
      "Iteration 11, loss = 7.40778641\n",
      "Iteration 12, loss = 8.31998422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75536437\n",
      "Iteration 2, loss = 1.07985304\n",
      "Iteration 3, loss = 1.46972308\n",
      "Iteration 4, loss = 1.39319583\n",
      "Iteration 5, loss = 1.27882131\n",
      "Iteration 6, loss = 1.31754875\n",
      "Iteration 7, loss = 1.22845903\n",
      "Iteration 8, loss = 1.17594902\n",
      "Iteration 9, loss = 1.12891268\n",
      "Iteration 10, loss = 1.06788596\n",
      "Iteration 11, loss = 1.01036034\n",
      "Iteration 12, loss = 0.95785499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74908130\n",
      "Iteration 2, loss = 1.05100716\n",
      "Iteration 3, loss = 1.47130663\n",
      "Iteration 4, loss = 1.36475580\n",
      "Iteration 5, loss = 1.31291536\n",
      "Iteration 6, loss = 1.29370390\n",
      "Iteration 7, loss = 1.24783200\n",
      "Iteration 8, loss = 1.16460963\n",
      "Iteration 9, loss = 1.13885723\n",
      "Iteration 10, loss = 1.06183376\n",
      "Iteration 11, loss = 1.01799297\n",
      "Iteration 12, loss = 0.95567038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75064350\n",
      "Iteration 2, loss = 1.06522649\n",
      "Iteration 3, loss = 1.47029903\n",
      "Iteration 4, loss = 1.37689790\n",
      "Iteration 5, loss = 1.29419357\n",
      "Iteration 6, loss = 1.30541093\n",
      "Iteration 7, loss = 1.23607471\n",
      "Iteration 8, loss = 1.16933756\n",
      "Iteration 9, loss = 1.13247924\n",
      "Iteration 10, loss = 1.06367185\n",
      "Iteration 11, loss = 1.01280043\n",
      "Iteration 12, loss = 0.95566621\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75536437\n",
      "Iteration 2, loss = 17.90413327\n",
      "Iteration 3, loss = 15.06951420\n",
      "Iteration 4, loss = 6.34406412\n",
      "Iteration 5, loss = 10.64834254\n",
      "Iteration 6, loss = 6.87054751\n",
      "Iteration 7, loss = 1.07820111\n",
      "Iteration 8, loss = 3.71638719\n",
      "Iteration 9, loss = 2.25541107\n",
      "Iteration 10, loss = 1.79931343\n",
      "Iteration 11, loss = 2.47644171\n",
      "Iteration 12, loss = 0.96884245\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74908130\n",
      "Iteration 2, loss = 18.13971270\n",
      "Iteration 3, loss = 14.83445195\n",
      "Iteration 4, loss = 6.88805697\n",
      "Iteration 5, loss = 11.44983501\n",
      "Iteration 6, loss = 7.96558098\n",
      "Iteration 7, loss = 0.77995488\n",
      "Iteration 8, loss = 7.44752483\n",
      "Iteration 9, loss = 9.21456450\n",
      "Iteration 10, loss = 7.19460684\n",
      "Iteration 11, loss = 2.96413222\n",
      "Iteration 12, loss = 2.39672841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75064350\n",
      "Iteration 2, loss = 18.02192236\n",
      "Iteration 3, loss = 14.95612100\n",
      "Iteration 4, loss = 6.56699246\n",
      "Iteration 5, loss = 10.98575867\n",
      "Iteration 6, loss = 7.35428309\n",
      "Iteration 7, loss = 0.74333463\n",
      "Iteration 8, loss = 5.04446869\n",
      "Iteration 9, loss = 4.72667661\n",
      "Iteration 10, loss = 1.42641796\n",
      "Iteration 11, loss = 3.50519426\n",
      "Iteration 12, loss = 4.98991562\n",
      "Iteration 13, loss = 4.12785564\n",
      "Iteration 14, loss = 1.71849632\n",
      "Iteration 15, loss = 1.73482499\n",
      "Iteration 16, loss = 3.13612702\n",
      "Iteration 17, loss = 2.99011846\n",
      "Iteration 18, loss = 1.73215133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74404879\n",
      "Iteration 2, loss = 17.90413417\n",
      "Iteration 3, loss = 13.68538688\n",
      "Iteration 4, loss = 17.90413328\n",
      "Iteration 5, loss = 3.22630965\n",
      "Iteration 6, loss = 10.73993039\n",
      "Iteration 7, loss = 3.74847770\n",
      "Iteration 8, loss = 3.04937465\n",
      "Iteration 9, loss = 0.71553949\n",
      "Iteration 10, loss = 2.17652539\n",
      "Iteration 11, loss = 3.42793303\n",
      "Iteration 12, loss = 4.34667630\n",
      "Iteration 13, loss = 1.93033318\n",
      "Iteration 14, loss = 1.35309584\n",
      "Iteration 15, loss = 1.33048851\n",
      "Iteration 16, loss = 1.04001699\n",
      "Iteration 17, loss = 0.70787315\n",
      "Iteration 18, loss = 0.69336320\n",
      "Iteration 19, loss = 0.69343383\n",
      "Iteration 20, loss = 0.69311356\n",
      "Iteration 21, loss = 0.69317098\n",
      "Iteration 22, loss = 0.69394566\n",
      "Iteration 23, loss = 0.72623670\n",
      "Iteration 24, loss = 1.39245484\n",
      "Iteration 25, loss = 3.76175749\n",
      "Iteration 26, loss = 0.77879525\n",
      "Iteration 27, loss = 1.42805893\n",
      "Iteration 28, loss = 1.08410522\n",
      "Iteration 29, loss = 0.83672404\n",
      "Iteration 30, loss = 0.72334427\n",
      "Iteration 31, loss = 0.70070260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73757900\n",
      "Iteration 2, loss = 18.13971360\n",
      "Iteration 3, loss = 15.71522141\n",
      "Iteration 4, loss = 18.13971272\n",
      "Iteration 5, loss = 4.51000318\n",
      "Iteration 6, loss = 11.41852413\n",
      "Iteration 7, loss = 3.96347042\n",
      "Iteration 8, loss = 4.20392801\n",
      "Iteration 9, loss = 0.72074930\n",
      "Iteration 10, loss = 0.69195517\n",
      "Iteration 11, loss = 0.74864900\n",
      "Iteration 12, loss = 5.59462799\n",
      "Iteration 13, loss = 17.03465014\n",
      "Iteration 14, loss = 0.99710318\n",
      "Iteration 15, loss = 2.62007539\n",
      "Iteration 16, loss = 3.55668547\n",
      "Iteration 17, loss = 2.27975009\n",
      "Iteration 18, loss = 0.77590761\n",
      "Iteration 19, loss = 0.69707766\n",
      "Iteration 20, loss = 0.70871422\n",
      "Iteration 21, loss = 0.69304889\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73736508\n",
      "Iteration 2, loss = 18.02192325\n",
      "Iteration 3, loss = 14.74496075\n",
      "Iteration 4, loss = 18.02192237\n",
      "Iteration 5, loss = 3.89309528\n",
      "Iteration 6, loss = 11.05193076\n",
      "Iteration 7, loss = 3.86537667\n",
      "Iteration 8, loss = 3.58220314\n",
      "Iteration 9, loss = 0.69248910\n",
      "Iteration 10, loss = 1.20046587\n",
      "Iteration 11, loss = 4.91302506\n",
      "Iteration 12, loss = 5.37276824\n",
      "Iteration 13, loss = 2.06912249\n",
      "Iteration 14, loss = 1.36756659\n",
      "Iteration 15, loss = 1.23893106\n",
      "Iteration 16, loss = 1.15160109\n",
      "Iteration 17, loss = 0.80110812\n",
      "Iteration 18, loss = 0.71202103\n",
      "Iteration 19, loss = 0.70248829\n",
      "Iteration 20, loss = 0.70736100\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74404879\n",
      "Iteration 2, loss = 17.90416079\n",
      "Iteration 3, loss = 18.13974203\n",
      "Iteration 4, loss = 18.13973292\n",
      "Iteration 5, loss = 17.90414719\n",
      "Iteration 6, loss = 17.90414315\n",
      "Iteration 7, loss = 16.81613023\n",
      "Iteration 8, loss = 18.13972389\n",
      "Iteration 9, loss = 18.13972563\n",
      "Iteration 10, loss = 18.13972471\n",
      "Iteration 11, loss = 16.48595091\n",
      "Iteration 12, loss = 17.90414439\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73757900\n",
      "Iteration 2, loss = 18.13974023\n",
      "Iteration 3, loss = 17.90416211\n",
      "Iteration 4, loss = 17.90415294\n",
      "Iteration 5, loss = 18.13972592\n",
      "Iteration 6, loss = 18.13972207\n",
      "Iteration 7, loss = 7.75729766\n",
      "Iteration 8, loss = 17.90414472\n",
      "Iteration 9, loss = 17.90414666\n",
      "Iteration 10, loss = 17.90414584\n",
      "Iteration 11, loss = 16.88691336\n",
      "Iteration 12, loss = 18.13972446\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73736508\n",
      "Iteration 2, loss = 18.02194971\n",
      "Iteration 3, loss = 18.02195125\n",
      "Iteration 4, loss = 18.02194216\n",
      "Iteration 5, loss = 18.02193582\n",
      "Iteration 6, loss = 18.02193190\n",
      "Iteration 7, loss = 12.20213939\n",
      "Iteration 8, loss = 18.02193360\n",
      "Iteration 9, loss = 18.02193543\n",
      "Iteration 10, loss = 18.02193456\n",
      "Iteration 11, loss = 16.60618489\n",
      "Iteration 12, loss = 18.02193371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93894923\n",
      "Iteration 2, loss = 17.90413344\n",
      "Iteration 3, loss = 0.89011171\n",
      "Iteration 4, loss = 2.07873447\n",
      "Iteration 5, loss = 10.97859461\n",
      "Iteration 6, loss = 2.21660420\n",
      "Iteration 7, loss = 0.84976366\n",
      "Iteration 8, loss = 2.31888857\n",
      "Iteration 9, loss = 5.00301192\n",
      "Iteration 10, loss = 0.73633665\n",
      "Iteration 11, loss = 1.71043149\n",
      "Iteration 12, loss = 1.80441657\n",
      "Iteration 13, loss = 4.60039620\n",
      "Iteration 14, loss = 0.69899607\n",
      "Iteration 15, loss = 0.74701863\n",
      "Iteration 16, loss = 0.70458912\n",
      "Iteration 17, loss = 0.69692544\n",
      "Iteration 18, loss = 0.69615455\n",
      "Iteration 19, loss = 0.69319800\n",
      "Iteration 20, loss = 0.69393082\n",
      "Iteration 21, loss = 0.69351997\n",
      "Iteration 22, loss = 0.69619853\n",
      "Iteration 23, loss = 0.70744498\n",
      "Iteration 24, loss = 0.79616063\n",
      "Iteration 25, loss = 1.29462532\n",
      "Iteration 26, loss = 1.60820354\n",
      "Iteration 27, loss = 0.69267035\n",
      "Iteration 28, loss = 0.69442319\n",
      "Iteration 29, loss = 0.69571064\n",
      "Iteration 30, loss = 0.70224335\n",
      "Iteration 31, loss = 0.69389680\n",
      "Iteration 32, loss = 0.69397843\n",
      "Iteration 33, loss = 0.69412426\n",
      "Iteration 34, loss = 0.69420304\n",
      "Iteration 35, loss = 0.69496327\n",
      "Iteration 36, loss = 0.70431383\n",
      "Iteration 37, loss = 0.85563536\n",
      "Iteration 38, loss = 2.20625617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93954706\n",
      "Iteration 2, loss = 18.13971287\n",
      "Iteration 3, loss = 0.92787684\n",
      "Iteration 4, loss = 2.17525198\n",
      "Iteration 5, loss = 10.43480007\n",
      "Iteration 6, loss = 2.35988531\n",
      "Iteration 7, loss = 0.86182332\n",
      "Iteration 8, loss = 2.39516711\n",
      "Iteration 9, loss = 4.94049825\n",
      "Iteration 10, loss = 0.76266794\n",
      "Iteration 11, loss = 2.06334519\n",
      "Iteration 12, loss = 1.74563690\n",
      "Iteration 13, loss = 4.41127032\n",
      "Iteration 14, loss = 0.72828186\n",
      "Iteration 15, loss = 0.70173867\n",
      "Iteration 16, loss = 0.69351188\n",
      "Iteration 17, loss = 0.69307365\n",
      "Iteration 18, loss = 0.69320530\n",
      "Iteration 19, loss = 0.69298290\n",
      "Iteration 20, loss = 0.69284708\n",
      "Iteration 21, loss = 0.69273833\n",
      "Iteration 22, loss = 0.69263085\n",
      "Iteration 23, loss = 0.69250790\n",
      "Iteration 24, loss = 0.69236865\n",
      "Iteration 25, loss = 0.69220490\n",
      "Iteration 26, loss = 0.69204853\n",
      "Iteration 27, loss = 0.69217538\n",
      "Iteration 28, loss = 0.69868471\n",
      "Iteration 29, loss = 0.84066508\n",
      "Iteration 30, loss = 2.38366936\n",
      "Iteration 31, loss = 1.45465618\n",
      "Iteration 32, loss = 0.89730192\n",
      "Iteration 33, loss = 0.74498671\n",
      "Iteration 34, loss = 0.69419104\n",
      "Iteration 35, loss = 0.70302173\n",
      "Iteration 36, loss = 0.69465807\n",
      "Iteration 37, loss = 0.69501425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93574500\n",
      "Iteration 2, loss = 18.02192252\n",
      "Iteration 3, loss = 0.90522833\n",
      "Iteration 4, loss = 2.07196698\n",
      "Iteration 5, loss = 10.80456364\n",
      "Iteration 6, loss = 2.25645217\n",
      "Iteration 7, loss = 0.87295556\n",
      "Iteration 8, loss = 2.45815114\n",
      "Iteration 9, loss = 4.74952896\n",
      "Iteration 10, loss = 0.75776095\n",
      "Iteration 11, loss = 1.94771718\n",
      "Iteration 12, loss = 1.69993764\n",
      "Iteration 13, loss = 4.44388114\n",
      "Iteration 14, loss = 0.71583568\n",
      "Iteration 15, loss = 0.71112339\n",
      "Iteration 16, loss = 0.69383848\n",
      "Iteration 17, loss = 0.69358406\n",
      "Iteration 18, loss = 0.69388782\n",
      "Iteration 19, loss = 0.69326698\n",
      "Iteration 20, loss = 0.69326470\n",
      "Iteration 21, loss = 0.69303026\n",
      "Iteration 22, loss = 0.69303869\n",
      "Iteration 23, loss = 0.69303722\n",
      "Iteration 24, loss = 0.69485102\n",
      "Iteration 25, loss = 0.71065103\n",
      "Iteration 26, loss = 0.88139794\n",
      "Iteration 27, loss = 1.84263792\n",
      "Iteration 28, loss = 1.35064786\n",
      "Iteration 29, loss = 0.70640724\n",
      "Iteration 30, loss = 0.70280693\n",
      "Iteration 31, loss = 0.70533818\n",
      "Iteration 32, loss = 0.69634618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93894923\n",
      "Iteration 2, loss = 17.90415994\n",
      "Iteration 3, loss = 11.29610563\n",
      "Iteration 4, loss = 18.13975991\n",
      "Iteration 5, loss = 18.13976044\n",
      "Iteration 6, loss = 18.13975995\n",
      "Iteration 7, loss = 17.90419087\n",
      "Iteration 8, loss = 17.90419498\n",
      "Iteration 9, loss = 17.90419299\n",
      "Iteration 10, loss = 18.13977129\n",
      "Iteration 11, loss = 18.13976747\n",
      "Iteration 12, loss = 18.13975948\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93954706\n",
      "Iteration 2, loss = 18.13973939\n",
      "Iteration 3, loss = 3.44790586\n",
      "Iteration 4, loss = 17.90418242\n",
      "Iteration 5, loss = 17.90418295\n",
      "Iteration 6, loss = 17.90418195\n",
      "Iteration 7, loss = 18.13977086\n",
      "Iteration 8, loss = 18.13977485\n",
      "Iteration 9, loss = 18.13977335\n",
      "Iteration 10, loss = 17.90419376\n",
      "Iteration 11, loss = 17.90419037\n",
      "Iteration 12, loss = 17.90418227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93574500\n",
      "Iteration 2, loss = 18.02194887\n",
      "Iteration 3, loss = 4.26707180\n",
      "Iteration 4, loss = 18.02197129\n",
      "Iteration 5, loss = 18.02197161\n",
      "Iteration 6, loss = 18.02197044\n",
      "Iteration 7, loss = 18.02197972\n",
      "Iteration 8, loss = 18.02198369\n",
      "Iteration 9, loss = 18.02198227\n",
      "Iteration 10, loss = 18.02198224\n",
      "Iteration 11, loss = 18.02197895\n",
      "Iteration 12, loss = 18.02197096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75536437\n",
      "Iteration 2, loss = 12.23097458\n",
      "Iteration 3, loss = 8.31619352\n",
      "Iteration 4, loss = 13.17067179\n",
      "Iteration 5, loss = 1.63061882\n",
      "Iteration 6, loss = 2.09016102\n",
      "Iteration 7, loss = 1.02411871\n",
      "Iteration 8, loss = 0.85794874\n",
      "Iteration 9, loss = 0.86296582\n",
      "Iteration 10, loss = 1.02739202\n",
      "Iteration 11, loss = 1.23470813\n",
      "Iteration 12, loss = 1.16025775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74908130\n",
      "Iteration 2, loss = 11.83535295\n",
      "Iteration 3, loss = 9.04032558\n",
      "Iteration 4, loss = 13.83414520\n",
      "Iteration 5, loss = 1.76273962\n",
      "Iteration 6, loss = 2.25958376\n",
      "Iteration 7, loss = 1.09003077\n",
      "Iteration 8, loss = 0.87802811\n",
      "Iteration 9, loss = 0.87647019\n",
      "Iteration 10, loss = 1.02535197\n",
      "Iteration 11, loss = 1.17506071\n",
      "Iteration 12, loss = 1.12571421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75064350\n",
      "Iteration 2, loss = 12.05972593\n",
      "Iteration 3, loss = 8.63294514\n",
      "Iteration 4, loss = 13.49722056\n",
      "Iteration 5, loss = 1.68903773\n",
      "Iteration 6, loss = 2.16544013\n",
      "Iteration 7, loss = 1.05242867\n",
      "Iteration 8, loss = 0.86701880\n",
      "Iteration 9, loss = 0.87066343\n",
      "Iteration 10, loss = 1.02767534\n",
      "Iteration 11, loss = 1.20536158\n",
      "Iteration 12, loss = 1.13931418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75536437\n",
      "Iteration 2, loss = 17.90415950\n",
      "Iteration 3, loss = 18.13974478\n",
      "Iteration 4, loss = 18.13973697\n",
      "Iteration 5, loss = 17.90415215\n",
      "Iteration 6, loss = 17.90414593\n",
      "Iteration 7, loss = 17.90414134\n",
      "Iteration 8, loss = 18.13972240\n",
      "Iteration 9, loss = 18.13972446\n",
      "Iteration 10, loss = 18.13972472\n",
      "Iteration 11, loss = 17.90414575\n",
      "Iteration 12, loss = 17.90414584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74908130\n",
      "Iteration 2, loss = 18.13973893\n",
      "Iteration 3, loss = 17.90416493\n",
      "Iteration 4, loss = 17.90415703\n",
      "Iteration 5, loss = 18.13973081\n",
      "Iteration 6, loss = 18.13972469\n",
      "Iteration 7, loss = 17.56979699\n",
      "Iteration 8, loss = 17.90414313\n",
      "Iteration 9, loss = 17.90414553\n",
      "Iteration 10, loss = 17.90414611\n",
      "Iteration 11, loss = 18.13972631\n",
      "Iteration 12, loss = 18.13972653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75064350\n",
      "Iteration 2, loss = 18.02194841\n",
      "Iteration 3, loss = 18.02195414\n",
      "Iteration 4, loss = 18.02194612\n",
      "Iteration 5, loss = 18.02194032\n",
      "Iteration 6, loss = 18.02193431\n",
      "Iteration 7, loss = 17.97671900\n",
      "Iteration 8, loss = 18.02193256\n",
      "Iteration 9, loss = 18.02193478\n",
      "Iteration 10, loss = 18.02193503\n",
      "Iteration 11, loss = 15.79830513\n",
      "Iteration 12, loss = 18.02193555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74404879\n",
      "Iteration 2, loss = 17.90413757\n",
      "Iteration 3, loss = 17.90510362\n",
      "Iteration 4, loss = 2297.59612127\n",
      "Iteration 5, loss = 15683270896626842992640.00000000\n",
      "Iteration 6, loss = 9968823446630294846205471621419805074050818595720199105489867097294239761956864.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.73757900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 18.13971658\n",
      "Iteration 3, loss = 18.14058760\n",
      "Iteration 4, loss = 1879.73783867\n",
      "Iteration 5, loss = 12854165207337492545536.00000000\n",
      "Iteration 6, loss = 6079472559353238248799989268723955289945835825752607737229475268772437713485824.00000000\n",
      "Iteration 7, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73736508\n",
      "Iteration 2, loss = 18.02192642\n",
      "Iteration 3, loss = 18.02283732\n",
      "Iteration 4, loss = 2081.73376942\n",
      "Iteration 5, loss = 14793604730025022586880.00000000\n",
      "Iteration 6, loss = 8999343117195840299538893640471805859933909355657799417797323653971675371798528.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74404879\n",
      "Iteration 2, loss = 17.90679975\n",
      "Iteration 3, loss = 18.14428714\n",
      "Iteration 4, loss = 17.90729537\n",
      "Iteration 5, loss = 18.14242061\n",
      "Iteration 6, loss = 17.90662147\n",
      "Iteration 7, loss = 18.14224548\n",
      "Iteration 8, loss = 18.14219786\n",
      "Iteration 9, loss = 17.90644496\n",
      "Iteration 10, loss = 17.90630743\n",
      "Iteration 11, loss = 17.90618655\n",
      "Iteration 12, loss = 18.14172305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73757900\n",
      "Iteration 2, loss = 18.14237911\n",
      "Iteration 3, loss = 17.90869377\n",
      "Iteration 4, loss = 18.14289329\n",
      "Iteration 5, loss = 17.90685481\n",
      "Iteration 6, loss = 18.14219608\n",
      "Iteration 7, loss = 17.90664702\n",
      "Iteration 8, loss = 17.90658413\n",
      "Iteration 9, loss = 18.14197710\n",
      "Iteration 10, loss = 18.14184440\n",
      "Iteration 11, loss = 2.94337163\n",
      "Iteration 12, loss = 18.14168629\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73736508\n",
      "Iteration 2, loss = 18.02457149\n",
      "Iteration 3, loss = 18.02645990\n",
      "Iteration 4, loss = 18.02507235\n",
      "Iteration 5, loss = 18.02461862\n",
      "Iteration 6, loss = 18.02439166\n",
      "Iteration 7, loss = 18.02442905\n",
      "Iteration 8, loss = 18.02437444\n",
      "Iteration 9, loss = 18.02419579\n",
      "Iteration 10, loss = 18.02406140\n",
      "Iteration 11, loss = 18.02394878\n",
      "Iteration 12, loss = 18.02391613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93894923\n",
      "Iteration 2, loss = 17.90414743\n",
      "Iteration 3, loss = 18.14321206\n",
      "Iteration 4, loss = 72168.02352382\n",
      "Iteration 5, loss = 180671161390381045645312.00000000\n",
      "Iteration 6, loss = 8401460235291701247957250406928644957044832729800826407526371197165251402492018688.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.93954706\n",
      "Iteration 2, loss = 18.13972642\n",
      "Iteration 3, loss = 17.90758805\n",
      "Iteration 4, loss = 68668.13497454\n",
      "Iteration 5, loss = 61043720202887633043456.00000000\n",
      "Iteration 6, loss = 302287032268100541207322368433163142093621701936325086204530976536895193320783872.00000000\n",
      "Iteration 7, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.93574500\n",
      "Iteration 2, loss = 18.02193615\n",
      "Iteration 3, loss = 18.02536779\n",
      "Iteration 4, loss = 69694.54047529\n",
      "Iteration 5, loss = 91835421208839857373184.00000000\n",
      "Iteration 6, loss = 1079085798734565919492085474699099973702423753468427108267528038463936871671005184.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.93894923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 17.90679935\n",
      "Iteration 3, loss = 18.14371303\n",
      "Iteration 4, loss = 17.90807699\n",
      "Iteration 5, loss = 17.90754300\n",
      "Iteration 6, loss = 18.14272216\n",
      "Iteration 7, loss = 18.14191231\n",
      "Iteration 8, loss = 17.90588755\n",
      "Iteration 9, loss = 17.90565392\n",
      "Iteration 10, loss = 17.90529530\n",
      "Iteration 11, loss = 18.14068144\n",
      "Iteration 12, loss = 18.14054423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93954706\n",
      "Iteration 2, loss = 18.14237889\n",
      "Iteration 3, loss = 17.90820853\n",
      "Iteration 4, loss = 18.14377121\n",
      "Iteration 5, loss = 18.14321498\n",
      "Iteration 6, loss = 17.90723524\n",
      "Iteration 7, loss = 17.90637122\n",
      "Iteration 8, loss = 18.14140461\n",
      "Iteration 9, loss = 18.14116110\n",
      "Iteration 10, loss = 18.14085210\n",
      "Iteration 11, loss = 17.90513666\n",
      "Iteration 12, loss = 17.90501030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93574500\n",
      "Iteration 2, loss = 18.02457124\n",
      "Iteration 3, loss = 18.02597385\n",
      "Iteration 4, loss = 18.02593438\n",
      "Iteration 5, loss = 18.02538452\n",
      "Iteration 6, loss = 18.02498874\n",
      "Iteration 7, loss = 18.02414386\n",
      "Iteration 8, loss = 18.02363425\n",
      "Iteration 9, loss = 18.02340028\n",
      "Iteration 10, loss = 18.02308217\n",
      "Iteration 11, loss = 18.02293338\n",
      "Iteration 12, loss = 18.02278919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75536437\n",
      "Iteration 2, loss = 17.90413546\n",
      "Iteration 3, loss = 17.90468226\n",
      "Iteration 4, loss = 518.83002134\n",
      "Iteration 5, loss = 241927413534506647552.00000000\n",
      "Iteration 6, loss = 42657529384098513008299229223401679643393966212424689529743144776529608704.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n",
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.74908130\n",
      "Iteration 2, loss = 18.13971469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 18.14022692\n",
      "Iteration 4, loss = 454.84410895\n",
      "Iteration 5, loss = 205361791004305358848.00000000\n",
      "Iteration 6, loss = 27556926110557683566532167323545580510183554553260570118133604690065096704.00000000\n",
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n",
      "Iteration 1, loss = 0.75064350\n",
      "Iteration 2, loss = 18.02192445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 18.02245316\n",
      "Iteration 4, loss = 489.38944777\n",
      "Iteration 5, loss = 229500652045243088896.00000000\n",
      "Iteration 6, loss = 37840127645428976715392691146672503760613870469047304126058462537154822144.00000000\n",
      "Iteration 7, loss = nan\n",
      "Iteration 8, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\extmath.py:192: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = nan\n",
      "Iteration 10, loss = nan\n",
      "Iteration 11, loss = nan\n",
      "Iteration 12, loss = nan\n",
      "Iteration 13, loss = nan\n",
      "Iteration 14, loss = nan\n",
      "Iteration 15, loss = nan\n",
      "Iteration 16, loss = nan\n",
      "Iteration 17, loss = nan\n",
      "Iteration 18, loss = nan\n",
      "Iteration 19, loss = nan\n",
      "Iteration 20, loss = nan\n",
      "Iteration 21, loss = nan\n",
      "Iteration 22, loss = nan\n",
      "Iteration 23, loss = nan\n",
      "Iteration 24, loss = nan\n",
      "Iteration 25, loss = nan\n",
      "Iteration 26, loss = nan\n",
      "Iteration 27, loss = nan\n",
      "Iteration 28, loss = nan\n",
      "Iteration 29, loss = nan\n",
      "Iteration 30, loss = nan\n",
      "Iteration 31, loss = nan\n",
      "Iteration 32, loss = nan\n",
      "Iteration 33, loss = nan\n",
      "Iteration 34, loss = nan\n",
      "Iteration 35, loss = nan\n",
      "Iteration 36, loss = nan\n",
      "Iteration 37, loss = nan\n",
      "Iteration 38, loss = nan\n",
      "Iteration 39, loss = nan\n",
      "Iteration 40, loss = nan\n",
      "Iteration 41, loss = nan\n",
      "Iteration 42, loss = nan\n",
      "Iteration 43, loss = nan\n",
      "Iteration 44, loss = nan\n",
      "Iteration 45, loss = nan\n",
      "Iteration 46, loss = nan\n",
      "Iteration 47, loss = nan\n",
      "Iteration 48, loss = nan\n",
      "Iteration 49, loss = nan\n",
      "Iteration 50, loss = nan\n",
      "Iteration 51, loss = nan\n",
      "Iteration 52, loss = nan\n",
      "Iteration 53, loss = nan\n",
      "Iteration 54, loss = nan\n",
      "Iteration 55, loss = nan\n",
      "Iteration 56, loss = nan\n",
      "Iteration 57, loss = nan\n",
      "Iteration 58, loss = nan\n",
      "Iteration 59, loss = nan\n",
      "Iteration 60, loss = nan\n",
      "Iteration 61, loss = nan\n",
      "Iteration 62, loss = nan\n",
      "Iteration 63, loss = nan\n",
      "Iteration 64, loss = nan\n",
      "Iteration 65, loss = nan\n",
      "Iteration 66, loss = nan\n",
      "Iteration 67, loss = nan\n",
      "Iteration 68, loss = nan\n",
      "Iteration 69, loss = nan\n",
      "Iteration 70, loss = nan\n",
      "Iteration 71, loss = nan\n",
      "Iteration 72, loss = nan\n",
      "Iteration 73, loss = nan\n",
      "Iteration 74, loss = nan\n",
      "Iteration 75, loss = nan\n",
      "Iteration 76, loss = nan\n",
      "Iteration 77, loss = nan\n",
      "Iteration 78, loss = nan\n",
      "Iteration 79, loss = nan\n",
      "Iteration 80, loss = nan\n",
      "Iteration 81, loss = nan\n",
      "Iteration 82, loss = nan\n",
      "Iteration 83, loss = nan\n",
      "Iteration 84, loss = nan\n",
      "Iteration 85, loss = nan\n",
      "Iteration 86, loss = nan\n",
      "Iteration 87, loss = nan\n",
      "Iteration 88, loss = nan\n",
      "Iteration 89, loss = nan\n",
      "Iteration 90, loss = nan\n",
      "Iteration 91, loss = nan\n",
      "Iteration 92, loss = nan\n",
      "Iteration 93, loss = nan\n",
      "Iteration 94, loss = nan\n",
      "Iteration 95, loss = nan\n",
      "Iteration 96, loss = nan\n",
      "Iteration 97, loss = nan\n",
      "Iteration 98, loss = nan\n",
      "Iteration 99, loss = nan\n",
      "Iteration 100, loss = nan\n",
      "Iteration 101, loss = nan\n",
      "Iteration 102, loss = nan\n",
      "Iteration 103, loss = nan\n",
      "Iteration 104, loss = nan\n",
      "Iteration 105, loss = nan\n",
      "Iteration 106, loss = nan\n",
      "Iteration 107, loss = nan\n",
      "Iteration 108, loss = nan\n",
      "Iteration 109, loss = nan\n",
      "Iteration 110, loss = nan\n",
      "Iteration 111, loss = nan\n",
      "Iteration 112, loss = nan\n",
      "Iteration 113, loss = nan\n",
      "Iteration 114, loss = nan\n",
      "Iteration 115, loss = nan\n",
      "Iteration 116, loss = nan\n",
      "Iteration 117, loss = nan\n",
      "Iteration 118, loss = nan\n",
      "Iteration 119, loss = nan\n",
      "Iteration 120, loss = nan\n",
      "Iteration 121, loss = nan\n",
      "Iteration 122, loss = nan\n",
      "Iteration 123, loss = nan\n",
      "Iteration 124, loss = nan\n",
      "Iteration 125, loss = nan\n",
      "Iteration 126, loss = nan\n",
      "Iteration 127, loss = nan\n",
      "Iteration 128, loss = nan\n",
      "Iteration 129, loss = nan\n",
      "Iteration 130, loss = nan\n",
      "Iteration 131, loss = nan\n",
      "Iteration 132, loss = nan\n",
      "Iteration 133, loss = nan\n",
      "Iteration 134, loss = nan\n",
      "Iteration 135, loss = nan\n",
      "Iteration 136, loss = nan\n",
      "Iteration 137, loss = nan\n",
      "Iteration 138, loss = nan\n",
      "Iteration 139, loss = nan\n",
      "Iteration 140, loss = nan\n",
      "Iteration 141, loss = nan\n",
      "Iteration 142, loss = nan\n",
      "Iteration 143, loss = nan\n",
      "Iteration 144, loss = nan\n",
      "Iteration 145, loss = nan\n",
      "Iteration 146, loss = nan\n",
      "Iteration 147, loss = nan\n",
      "Iteration 148, loss = nan\n",
      "Iteration 149, loss = nan\n",
      "Iteration 150, loss = nan\n",
      "Iteration 151, loss = nan\n",
      "Iteration 152, loss = nan\n",
      "Iteration 153, loss = nan\n",
      "Iteration 154, loss = nan\n",
      "Iteration 155, loss = nan\n",
      "Iteration 156, loss = nan\n",
      "Iteration 157, loss = nan\n",
      "Iteration 158, loss = nan\n",
      "Iteration 159, loss = nan\n",
      "Iteration 160, loss = nan\n",
      "Iteration 161, loss = nan\n",
      "Iteration 162, loss = nan\n",
      "Iteration 163, loss = nan\n",
      "Iteration 164, loss = nan\n",
      "Iteration 165, loss = nan\n",
      "Iteration 166, loss = nan\n",
      "Iteration 167, loss = nan\n",
      "Iteration 168, loss = nan\n",
      "Iteration 169, loss = nan\n",
      "Iteration 170, loss = nan\n",
      "Iteration 171, loss = nan\n",
      "Iteration 172, loss = nan\n",
      "Iteration 173, loss = nan\n",
      "Iteration 174, loss = nan\n",
      "Iteration 175, loss = nan\n",
      "Iteration 176, loss = nan\n",
      "Iteration 177, loss = nan\n",
      "Iteration 178, loss = nan\n",
      "Iteration 179, loss = nan\n",
      "Iteration 180, loss = nan\n",
      "Iteration 181, loss = nan\n",
      "Iteration 182, loss = nan\n",
      "Iteration 183, loss = nan\n",
      "Iteration 184, loss = nan\n",
      "Iteration 185, loss = nan\n",
      "Iteration 186, loss = nan\n",
      "Iteration 187, loss = nan\n",
      "Iteration 188, loss = nan\n",
      "Iteration 189, loss = nan\n",
      "Iteration 190, loss = nan\n",
      "Iteration 191, loss = nan\n",
      "Iteration 192, loss = nan\n",
      "Iteration 193, loss = nan\n",
      "Iteration 194, loss = nan\n",
      "Iteration 195, loss = nan\n",
      "Iteration 196, loss = nan\n",
      "Iteration 197, loss = nan\n",
      "Iteration 198, loss = nan\n",
      "Iteration 199, loss = nan\n",
      "Iteration 200, loss = nan\n",
      "Iteration 201, loss = nan\n",
      "Iteration 202, loss = nan\n",
      "Iteration 203, loss = nan\n",
      "Iteration 204, loss = nan\n",
      "Iteration 205, loss = nan\n",
      "Iteration 206, loss = nan\n",
      "Iteration 207, loss = nan\n",
      "Iteration 208, loss = nan\n",
      "Iteration 209, loss = nan\n",
      "Iteration 210, loss = nan\n",
      "Iteration 211, loss = nan\n",
      "Iteration 212, loss = nan\n",
      "Iteration 213, loss = nan\n",
      "Iteration 214, loss = nan\n",
      "Iteration 215, loss = nan\n",
      "Iteration 216, loss = nan\n",
      "Iteration 217, loss = nan\n",
      "Iteration 218, loss = nan\n",
      "Iteration 219, loss = nan\n",
      "Iteration 220, loss = nan\n",
      "Iteration 221, loss = nan\n",
      "Iteration 222, loss = nan\n",
      "Iteration 223, loss = nan\n",
      "Iteration 224, loss = nan\n",
      "Iteration 225, loss = nan\n",
      "Iteration 226, loss = nan\n",
      "Iteration 227, loss = nan\n",
      "Iteration 228, loss = nan\n",
      "Iteration 229, loss = nan\n",
      "Iteration 230, loss = nan\n",
      "Iteration 231, loss = nan\n",
      "Iteration 232, loss = nan\n",
      "Iteration 233, loss = nan\n",
      "Iteration 234, loss = nan\n",
      "Iteration 235, loss = nan\n",
      "Iteration 236, loss = nan\n",
      "Iteration 237, loss = nan\n",
      "Iteration 238, loss = nan\n",
      "Iteration 239, loss = nan\n",
      "Iteration 240, loss = nan\n",
      "Iteration 241, loss = nan\n",
      "Iteration 242, loss = nan\n",
      "Iteration 243, loss = nan\n",
      "Iteration 244, loss = nan\n",
      "Iteration 245, loss = nan\n",
      "Iteration 246, loss = nan\n",
      "Iteration 247, loss = nan\n",
      "Iteration 248, loss = nan\n",
      "Iteration 249, loss = nan\n",
      "Iteration 250, loss = nan\n",
      "Iteration 251, loss = nan\n",
      "Iteration 252, loss = nan\n",
      "Iteration 253, loss = nan\n",
      "Iteration 254, loss = nan\n",
      "Iteration 255, loss = nan\n",
      "Iteration 256, loss = nan\n",
      "Iteration 257, loss = nan\n",
      "Iteration 258, loss = nan\n",
      "Iteration 259, loss = nan\n",
      "Iteration 260, loss = nan\n",
      "Iteration 261, loss = nan\n",
      "Iteration 262, loss = nan\n",
      "Iteration 263, loss = nan\n",
      "Iteration 264, loss = nan\n",
      "Iteration 265, loss = nan\n",
      "Iteration 266, loss = nan\n",
      "Iteration 267, loss = nan\n",
      "Iteration 268, loss = nan\n",
      "Iteration 269, loss = nan\n",
      "Iteration 270, loss = nan\n",
      "Iteration 271, loss = nan\n",
      "Iteration 272, loss = nan\n",
      "Iteration 273, loss = nan\n",
      "Iteration 274, loss = nan\n",
      "Iteration 275, loss = nan\n",
      "Iteration 276, loss = nan\n",
      "Iteration 277, loss = nan\n",
      "Iteration 278, loss = nan\n",
      "Iteration 279, loss = nan\n",
      "Iteration 280, loss = nan\n",
      "Iteration 281, loss = nan\n",
      "Iteration 282, loss = nan\n",
      "Iteration 283, loss = nan\n",
      "Iteration 284, loss = nan\n",
      "Iteration 285, loss = nan\n",
      "Iteration 286, loss = nan\n",
      "Iteration 287, loss = nan\n",
      "Iteration 288, loss = nan\n",
      "Iteration 289, loss = nan\n",
      "Iteration 290, loss = nan\n",
      "Iteration 291, loss = nan\n",
      "Iteration 292, loss = nan\n",
      "Iteration 293, loss = nan\n",
      "Iteration 294, loss = nan\n",
      "Iteration 295, loss = nan\n",
      "Iteration 296, loss = nan\n",
      "Iteration 297, loss = nan\n",
      "Iteration 298, loss = nan\n",
      "Iteration 299, loss = nan\n",
      "Iteration 300, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75536437\n",
      "Iteration 2, loss = 17.90679753\n",
      "Iteration 3, loss = 18.14460602\n",
      "Iteration 4, loss = 18.14256775\n",
      "Iteration 5, loss = 17.90687568\n",
      "Iteration 6, loss = 18.14343684\n",
      "Iteration 7, loss = 18.14317219\n",
      "Iteration 8, loss = 17.90690718\n",
      "Iteration 9, loss = 17.90624936\n",
      "Iteration 10, loss = 18.14150227\n",
      "Iteration 11, loss = 18.14093988\n",
      "Iteration 12, loss = 18.14044493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74908130\n",
      "Iteration 2, loss = 18.14237695\n",
      "Iteration 3, loss = 17.90900921\n",
      "Iteration 4, loss = 17.90699804\n",
      "Iteration 5, loss = 18.14249599\n",
      "Iteration 6, loss = 17.90792232\n",
      "Iteration 7, loss = 17.90766483\n",
      "Iteration 8, loss = 18.14254917\n",
      "Iteration 9, loss = 18.14186304\n",
      "Iteration 10, loss = 17.90590916\n",
      "Iteration 11, loss = 17.90535860\n",
      "Iteration 12, loss = 17.90486768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75064350\n",
      "Iteration 2, loss = 18.02456922\n",
      "Iteration 3, loss = 18.02686092\n",
      "Iteration 4, loss = 18.02475470\n",
      "Iteration 5, loss = 18.02431723\n",
      "Iteration 6, loss = 18.02498753\n",
      "Iteration 7, loss = 18.02462277\n",
      "Iteration 8, loss = 18.02393830\n",
      "Iteration 9, loss = 18.02341521\n",
      "Iteration 10, loss = 18.02314525\n",
      "Iteration 11, loss = 18.02295192\n",
      "Iteration 12, loss = 18.02275498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72388585\n",
      "Iteration 2, loss = 0.69322866\n",
      "Iteration 3, loss = 0.69903735\n",
      "Iteration 4, loss = 0.69688729\n",
      "Iteration 5, loss = 0.69339579\n",
      "Iteration 6, loss = 0.69285560\n",
      "Iteration 7, loss = 0.69287537\n",
      "Iteration 8, loss = 0.69260844\n",
      "Iteration 9, loss = 0.69237334\n",
      "Iteration 10, loss = 0.69220677\n",
      "Iteration 11, loss = 0.69202552\n",
      "Iteration 12, loss = 0.69183579\n",
      "Iteration 13, loss = 0.69164367\n",
      "Iteration 14, loss = 0.69143616\n",
      "Iteration 15, loss = 0.69122824\n",
      "Iteration 16, loss = 0.69101647\n",
      "Iteration 17, loss = 0.69079812\n",
      "Iteration 18, loss = 0.69055792\n",
      "Iteration 19, loss = 0.69031531\n",
      "Iteration 20, loss = 0.69003452\n",
      "Iteration 21, loss = 0.68971507\n",
      "Iteration 22, loss = 0.68935152\n",
      "Iteration 23, loss = 0.68907150\n",
      "Iteration 24, loss = 0.68884809\n",
      "Iteration 25, loss = 0.68863956\n",
      "Iteration 26, loss = 0.68841445\n",
      "Iteration 27, loss = 0.68817882\n",
      "Iteration 28, loss = 0.68794723\n",
      "Iteration 29, loss = 0.68771270\n",
      "Iteration 30, loss = 0.68747492\n",
      "Iteration 31, loss = 0.68725232\n",
      "Iteration 32, loss = 0.68700693\n",
      "Iteration 33, loss = 0.68674676\n",
      "Iteration 34, loss = 0.68650922\n",
      "Iteration 35, loss = 0.68626411\n",
      "Iteration 36, loss = 0.68599781\n",
      "Iteration 37, loss = 0.68575216\n",
      "Iteration 38, loss = 0.68551583\n",
      "Iteration 39, loss = 0.68525175\n",
      "Iteration 40, loss = 0.68499988\n",
      "Iteration 41, loss = 0.68475092\n",
      "Iteration 42, loss = 0.68446598\n",
      "Iteration 43, loss = 0.68419616\n",
      "Iteration 44, loss = 0.68392143\n",
      "Iteration 45, loss = 0.68368015\n",
      "Iteration 46, loss = 0.68343569\n",
      "Iteration 47, loss = 0.68318895\n",
      "Iteration 48, loss = 0.68294123\n",
      "Iteration 49, loss = 0.68268688\n",
      "Iteration 50, loss = 0.68243604\n",
      "Iteration 51, loss = 0.68220513\n",
      "Iteration 52, loss = 0.68195152\n",
      "Iteration 53, loss = 0.68168510\n",
      "Iteration 54, loss = 0.68144152\n",
      "Iteration 55, loss = 0.68119240\n",
      "Iteration 56, loss = 0.68093001\n",
      "Iteration 57, loss = 0.68067840\n",
      "Iteration 58, loss = 0.68042019\n",
      "Iteration 59, loss = 0.68016939\n",
      "Iteration 60, loss = 0.67991919\n",
      "Iteration 61, loss = 0.67968795\n",
      "Iteration 62, loss = 0.67943773\n",
      "Iteration 63, loss = 0.67917486\n",
      "Iteration 64, loss = 0.67892543\n",
      "Iteration 65, loss = 0.67866688\n",
      "Iteration 66, loss = 0.67841692\n",
      "Iteration 67, loss = 0.67818341\n",
      "Iteration 68, loss = 0.67793616\n",
      "Iteration 69, loss = 0.67767409\n",
      "Iteration 70, loss = 0.67741922\n",
      "Iteration 71, loss = 0.67716626\n",
      "Iteration 72, loss = 0.67692679\n",
      "Iteration 73, loss = 0.67668666\n",
      "Iteration 74, loss = 0.67642521\n",
      "Iteration 75, loss = 0.67616282\n",
      "Iteration 76, loss = 0.67591841\n",
      "Iteration 77, loss = 0.67566695\n",
      "Iteration 78, loss = 0.67541312\n",
      "Iteration 79, loss = 0.67516987\n",
      "Iteration 80, loss = 0.67493570\n",
      "Iteration 81, loss = 0.67467540\n",
      "Iteration 82, loss = 0.67440894\n",
      "Iteration 83, loss = 0.67416112\n",
      "Iteration 84, loss = 0.67391302\n",
      "Iteration 85, loss = 0.67366096\n",
      "Iteration 86, loss = 0.67341132\n",
      "Iteration 87, loss = 0.67315952\n",
      "Iteration 88, loss = 0.67291001\n",
      "Iteration 89, loss = 0.67265674\n",
      "Iteration 90, loss = 0.67240954\n",
      "Iteration 91, loss = 0.67215340\n",
      "Iteration 92, loss = 0.67190891\n",
      "Iteration 93, loss = 0.67165004\n",
      "Iteration 94, loss = 0.67140204\n",
      "Iteration 95, loss = 0.67115480\n",
      "Iteration 96, loss = 0.67089689\n",
      "Iteration 97, loss = 0.67064763\n",
      "Iteration 98, loss = 0.67040242\n",
      "Iteration 99, loss = 0.67014296\n",
      "Iteration 100, loss = 0.66989134\n",
      "Iteration 101, loss = 0.66964033\n",
      "Iteration 102, loss = 0.66939847\n",
      "Iteration 103, loss = 0.66913588\n",
      "Iteration 104, loss = 0.66888411\n",
      "Iteration 105, loss = 0.66863245\n",
      "Iteration 106, loss = 0.66838044\n",
      "Iteration 107, loss = 0.66812829\n",
      "Iteration 108, loss = 0.66787596\n",
      "Iteration 109, loss = 0.66762404\n",
      "Iteration 110, loss = 0.66737141\n",
      "Iteration 111, loss = 0.66711845\n",
      "Iteration 112, loss = 0.66686576\n",
      "Iteration 113, loss = 0.66661301\n",
      "Iteration 114, loss = 0.66638659\n",
      "Iteration 115, loss = 0.66612712\n",
      "Iteration 116, loss = 0.66586343\n",
      "Iteration 117, loss = 0.66560890\n",
      "Iteration 118, loss = 0.66537744\n",
      "Iteration 119, loss = 0.66511372\n",
      "Iteration 120, loss = 0.66484175\n",
      "Iteration 121, loss = 0.66459055\n",
      "Iteration 122, loss = 0.66434248\n",
      "Iteration 123, loss = 0.66408460\n",
      "Iteration 124, loss = 0.66386046\n",
      "Iteration 125, loss = 0.66359835\n",
      "Iteration 126, loss = 0.66332429\n",
      "Iteration 127, loss = 0.66309727\n",
      "Iteration 128, loss = 0.66282029\n",
      "Iteration 129, loss = 0.66255581\n",
      "Iteration 130, loss = 0.66230203\n",
      "Iteration 131, loss = 0.66206004\n",
      "Iteration 132, loss = 0.66179099\n",
      "Iteration 133, loss = 0.66153602\n",
      "Iteration 134, loss = 0.66128073\n",
      "Iteration 135, loss = 0.66102523\n",
      "Iteration 136, loss = 0.66076949\n",
      "Iteration 137, loss = 0.66051355\n",
      "Iteration 138, loss = 0.66025760\n",
      "Iteration 139, loss = 0.66000282\n",
      "Iteration 140, loss = 0.65977307\n",
      "Iteration 141, loss = 0.65950959\n",
      "Iteration 142, loss = 0.65924296\n",
      "Iteration 143, loss = 0.65898180\n",
      "Iteration 144, loss = 0.65875023\n",
      "Iteration 145, loss = 0.65848364\n",
      "Iteration 146, loss = 0.65820561\n",
      "Iteration 147, loss = 0.65800396\n",
      "Iteration 148, loss = 0.65771825\n",
      "Iteration 149, loss = 0.65745393\n",
      "Iteration 150, loss = 0.65717800\n",
      "Iteration 151, loss = 0.65695467\n",
      "Iteration 152, loss = 0.65669464\n",
      "Iteration 153, loss = 0.65643144\n",
      "Iteration 154, loss = 0.65615561\n",
      "Iteration 155, loss = 0.65588721\n",
      "Iteration 156, loss = 0.65564652\n",
      "Iteration 157, loss = 0.65537337\n",
      "Iteration 158, loss = 0.65511123\n",
      "Iteration 159, loss = 0.65486943\n",
      "Iteration 160, loss = 0.65459677\n",
      "Iteration 161, loss = 0.65432431\n",
      "Iteration 162, loss = 0.65407075\n",
      "Iteration 163, loss = 0.65381107\n",
      "Iteration 164, loss = 0.65356933\n",
      "Iteration 165, loss = 0.65329949\n",
      "Iteration 166, loss = 0.65301865\n",
      "Iteration 167, loss = 0.65280979\n",
      "Iteration 168, loss = 0.65252357\n",
      "Iteration 169, loss = 0.65225586\n",
      "Iteration 170, loss = 0.65197580\n",
      "Iteration 171, loss = 0.65173385\n",
      "Iteration 172, loss = 0.65148435\n",
      "Iteration 173, loss = 0.65121698\n",
      "Iteration 174, loss = 0.65093772\n",
      "Iteration 175, loss = 0.65065411\n",
      "Iteration 176, loss = 0.65040506\n",
      "Iteration 177, loss = 0.65014935\n",
      "Iteration 178, loss = 0.64987404\n",
      "Iteration 179, loss = 0.64960179\n",
      "Iteration 180, loss = 0.64933983\n",
      "Iteration 181, loss = 0.64907504\n",
      "Iteration 182, loss = 0.64880937\n",
      "Iteration 183, loss = 0.64854489\n",
      "Iteration 184, loss = 0.64827947\n",
      "Iteration 185, loss = 0.64800832\n",
      "Iteration 186, loss = 0.64775068\n",
      "Iteration 187, loss = 0.64747270\n",
      "Iteration 188, loss = 0.64720600\n",
      "Iteration 189, loss = 0.64693985\n",
      "Iteration 190, loss = 0.64667265\n",
      "Iteration 191, loss = 0.64640436\n",
      "Iteration 192, loss = 0.64613534\n",
      "Iteration 193, loss = 0.64586863\n",
      "Iteration 194, loss = 0.64562343\n",
      "Iteration 195, loss = 0.64537679\n",
      "Iteration 196, loss = 0.64510884\n",
      "Iteration 197, loss = 0.64482988\n",
      "Iteration 198, loss = 0.64454007\n",
      "Iteration 199, loss = 0.64426322\n",
      "Iteration 200, loss = 0.64399759\n",
      "Iteration 201, loss = 0.64370701\n",
      "Iteration 202, loss = 0.64347004\n",
      "Iteration 203, loss = 0.64319886\n",
      "Iteration 204, loss = 0.64292282\n",
      "Iteration 205, loss = 0.64263807\n",
      "Iteration 206, loss = 0.64234656\n",
      "Iteration 207, loss = 0.64208461\n",
      "Iteration 208, loss = 0.64182698\n",
      "Iteration 209, loss = 0.64154423\n",
      "Iteration 210, loss = 0.64125348\n",
      "Iteration 211, loss = 0.64098499\n",
      "Iteration 212, loss = 0.64071580\n",
      "Iteration 213, loss = 0.64043193\n",
      "Iteration 214, loss = 0.64017068\n",
      "Iteration 215, loss = 0.63988084\n",
      "Iteration 216, loss = 0.63960528\n",
      "Iteration 217, loss = 0.63932962\n",
      "Iteration 218, loss = 0.63905337\n",
      "Iteration 219, loss = 0.63881302\n",
      "Iteration 220, loss = 0.63854140\n",
      "Iteration 221, loss = 0.63826515\n",
      "Iteration 222, loss = 0.63797725\n",
      "Iteration 223, loss = 0.63767895\n",
      "Iteration 224, loss = 0.63741839\n",
      "Iteration 225, loss = 0.63714709\n",
      "Iteration 226, loss = 0.63686233\n",
      "Iteration 227, loss = 0.63656702\n",
      "Iteration 228, loss = 0.63628541\n",
      "Iteration 229, loss = 0.63601663\n",
      "Iteration 230, loss = 0.63572414\n",
      "Iteration 231, loss = 0.63544522\n",
      "Iteration 232, loss = 0.63517865\n",
      "Iteration 233, loss = 0.63488807\n",
      "Iteration 234, loss = 0.63459156\n",
      "Iteration 235, loss = 0.63431616\n",
      "Iteration 236, loss = 0.63403611\n",
      "Iteration 237, loss = 0.63374903\n",
      "Iteration 238, loss = 0.63347528\n",
      "Iteration 239, loss = 0.63317866\n",
      "Iteration 240, loss = 0.63289587\n",
      "Iteration 241, loss = 0.63261111\n",
      "Iteration 242, loss = 0.63232830\n",
      "Iteration 243, loss = 0.63207150\n",
      "Iteration 244, loss = 0.63179982\n",
      "Iteration 245, loss = 0.63151682\n",
      "Iteration 246, loss = 0.63122095\n",
      "Iteration 247, loss = 0.63091609\n",
      "Iteration 248, loss = 0.63062375\n",
      "Iteration 249, loss = 0.63032705\n",
      "Iteration 250, loss = 0.63003986\n",
      "Iteration 251, loss = 0.62975146\n",
      "Iteration 252, loss = 0.62946407\n",
      "Iteration 253, loss = 0.62917568\n",
      "Iteration 254, loss = 0.62891593\n",
      "Iteration 255, loss = 0.62864050\n",
      "Iteration 256, loss = 0.62835356\n",
      "Iteration 257, loss = 0.62805502\n",
      "Iteration 258, loss = 0.62774645\n",
      "Iteration 259, loss = 0.62744167\n",
      "Iteration 260, loss = 0.62714756\n",
      "Iteration 261, loss = 0.62689582\n",
      "Iteration 262, loss = 0.62659889\n",
      "Iteration 263, loss = 0.62630557\n",
      "Iteration 264, loss = 0.62600235\n",
      "Iteration 265, loss = 0.62568817\n",
      "Iteration 266, loss = 0.62544670\n",
      "Iteration 267, loss = 0.62512540\n",
      "Iteration 268, loss = 0.62482490\n",
      "Iteration 269, loss = 0.62451414\n",
      "Iteration 270, loss = 0.62424704\n",
      "Iteration 271, loss = 0.62395499\n",
      "Iteration 272, loss = 0.62365809\n",
      "Iteration 273, loss = 0.62334968\n",
      "Iteration 274, loss = 0.62303130\n",
      "Iteration 275, loss = 0.62279969\n",
      "Iteration 276, loss = 0.62245892\n",
      "Iteration 277, loss = 0.62215429\n",
      "Iteration 278, loss = 0.62184062\n",
      "Iteration 279, loss = 0.62159091\n",
      "Iteration 280, loss = 0.62127340\n",
      "Iteration 281, loss = 0.62097191\n",
      "Iteration 282, loss = 0.62066006\n",
      "Iteration 283, loss = 0.62034547\n",
      "Iteration 284, loss = 0.62005299\n",
      "Iteration 285, loss = 0.61974837\n",
      "Iteration 286, loss = 0.61944982\n",
      "Iteration 287, loss = 0.61914295\n",
      "Iteration 288, loss = 0.61884975\n",
      "Iteration 289, loss = 0.61853407\n",
      "Iteration 290, loss = 0.61827205\n",
      "Iteration 291, loss = 0.61796194\n",
      "Iteration 292, loss = 0.61765802\n",
      "Iteration 293, loss = 0.61734327\n",
      "Iteration 294, loss = 0.61701948\n",
      "Iteration 295, loss = 0.61675235\n",
      "Iteration 296, loss = 0.61643239\n",
      "Iteration 297, loss = 0.61612179\n",
      "Iteration 298, loss = 0.61580156\n",
      "Iteration 299, loss = 0.61549644\n",
      "Iteration 300, loss = 0.61517780\n",
      "Iteration 1, loss = 0.72598529\n",
      "Iteration 2, loss = 0.69199506\n",
      "Iteration 3, loss = 0.69856445\n",
      "Iteration 4, loss = 0.69599414\n",
      "Iteration 5, loss = 0.69215897\n",
      "Iteration 6, loss = 0.69163766\n",
      "Iteration 7, loss = 0.69168759\n",
      "Iteration 8, loss = 0.69142795\n",
      "Iteration 9, loss = 0.69120865\n",
      "Iteration 10, loss = 0.69106610\n",
      "Iteration 11, loss = 0.69091707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.69075391\n",
      "Iteration 13, loss = 0.69058691\n",
      "Iteration 14, loss = 0.69041582\n",
      "Iteration 15, loss = 0.69023931\n",
      "Iteration 16, loss = 0.69005801\n",
      "Iteration 17, loss = 0.68986848\n",
      "Iteration 18, loss = 0.68966586\n",
      "Iteration 19, loss = 0.68945315\n",
      "Iteration 20, loss = 0.68922300\n",
      "Iteration 21, loss = 0.68895841\n",
      "Iteration 22, loss = 0.68863804\n",
      "Iteration 23, loss = 0.68837416\n",
      "Iteration 24, loss = 0.68815701\n",
      "Iteration 25, loss = 0.68798778\n",
      "Iteration 26, loss = 0.68779848\n",
      "Iteration 27, loss = 0.68760737\n",
      "Iteration 28, loss = 0.68740654\n",
      "Iteration 29, loss = 0.68719289\n",
      "Iteration 30, loss = 0.68697704\n",
      "Iteration 31, loss = 0.68676324\n",
      "Iteration 32, loss = 0.68653571\n",
      "Iteration 33, loss = 0.68631268\n",
      "Iteration 34, loss = 0.68611497\n",
      "Iteration 35, loss = 0.68590730\n",
      "Iteration 36, loss = 0.68569868\n",
      "Iteration 37, loss = 0.68548848\n",
      "Iteration 38, loss = 0.68527714\n",
      "Iteration 39, loss = 0.68506463\n",
      "Iteration 40, loss = 0.68484933\n",
      "Iteration 41, loss = 0.68462765\n",
      "Iteration 42, loss = 0.68442167\n",
      "Iteration 43, loss = 0.68420616\n",
      "Iteration 44, loss = 0.68399104\n",
      "Iteration 45, loss = 0.68377874\n",
      "Iteration 46, loss = 0.68356621\n",
      "Iteration 47, loss = 0.68335380\n",
      "Iteration 48, loss = 0.68314120\n",
      "Iteration 49, loss = 0.68292844\n",
      "Iteration 50, loss = 0.68271584\n",
      "Iteration 51, loss = 0.68250356\n",
      "Iteration 52, loss = 0.68229095\n",
      "Iteration 53, loss = 0.68207754\n",
      "Iteration 54, loss = 0.68186415\n",
      "Iteration 55, loss = 0.68165102\n",
      "Iteration 56, loss = 0.68143824\n",
      "Iteration 57, loss = 0.68122538\n",
      "Iteration 58, loss = 0.68101188\n",
      "Iteration 59, loss = 0.68079869\n",
      "Iteration 60, loss = 0.68058585\n",
      "Iteration 61, loss = 0.68037290\n",
      "Iteration 62, loss = 0.68015931\n",
      "Iteration 63, loss = 0.67994612\n",
      "Iteration 64, loss = 0.67973311\n",
      "Iteration 65, loss = 0.67952033\n",
      "Iteration 66, loss = 0.67930652\n",
      "Iteration 67, loss = 0.67909343\n",
      "Iteration 68, loss = 0.67888016\n",
      "Iteration 69, loss = 0.67866697\n",
      "Iteration 70, loss = 0.67845379\n",
      "Iteration 71, loss = 0.67824127\n",
      "Iteration 72, loss = 0.67802727\n",
      "Iteration 73, loss = 0.67781438\n",
      "Iteration 74, loss = 0.67760092\n",
      "Iteration 75, loss = 0.67738783\n",
      "Iteration 76, loss = 0.67717439\n",
      "Iteration 77, loss = 0.67696117\n",
      "Iteration 78, loss = 0.67674785\n",
      "Iteration 79, loss = 0.67653440\n",
      "Iteration 80, loss = 0.67632132\n",
      "Iteration 81, loss = 0.67610909\n",
      "Iteration 82, loss = 0.67589422\n",
      "Iteration 83, loss = 0.67568094\n",
      "Iteration 84, loss = 0.67546768\n",
      "Iteration 85, loss = 0.67525376\n",
      "Iteration 86, loss = 0.67504030\n",
      "Iteration 87, loss = 0.67482696\n",
      "Iteration 88, loss = 0.67461354\n",
      "Iteration 89, loss = 0.67439972\n",
      "Iteration 90, loss = 0.67418634\n",
      "Iteration 91, loss = 0.67397245\n",
      "Iteration 92, loss = 0.67375891\n",
      "Iteration 93, loss = 0.67354511\n",
      "Iteration 94, loss = 0.67333124\n",
      "Iteration 95, loss = 0.67311757\n",
      "Iteration 96, loss = 0.67290365\n",
      "Iteration 97, loss = 0.67268951\n",
      "Iteration 98, loss = 0.67247537\n",
      "Iteration 99, loss = 0.67226142\n",
      "Iteration 100, loss = 0.67204729\n",
      "Iteration 101, loss = 0.67183316\n",
      "Iteration 102, loss = 0.67161894\n",
      "Iteration 103, loss = 0.67140471\n",
      "Iteration 104, loss = 0.67119030\n",
      "Iteration 105, loss = 0.67097603\n",
      "Iteration 106, loss = 0.67076173\n",
      "Iteration 107, loss = 0.67054717\n",
      "Iteration 108, loss = 0.67033245\n",
      "Iteration 109, loss = 0.67011777\n",
      "Iteration 110, loss = 0.66990341\n",
      "Iteration 111, loss = 0.66968819\n",
      "Iteration 112, loss = 0.66947330\n",
      "Iteration 113, loss = 0.66925846\n",
      "Iteration 114, loss = 0.66904335\n",
      "Iteration 115, loss = 0.66882810\n",
      "Iteration 116, loss = 0.66861294\n",
      "Iteration 117, loss = 0.66839756\n",
      "Iteration 118, loss = 0.66818223\n",
      "Iteration 119, loss = 0.66796667\n",
      "Iteration 120, loss = 0.66775109\n",
      "Iteration 121, loss = 0.66753540\n",
      "Iteration 122, loss = 0.66731960\n",
      "Iteration 123, loss = 0.66710381\n",
      "Iteration 124, loss = 0.66688828\n",
      "Iteration 125, loss = 0.66667175\n",
      "Iteration 126, loss = 0.66645554\n",
      "Iteration 127, loss = 0.66623935\n",
      "Iteration 128, loss = 0.66602294\n",
      "Iteration 129, loss = 0.66580642\n",
      "Iteration 130, loss = 0.66559019\n",
      "Iteration 131, loss = 0.66537357\n",
      "Iteration 132, loss = 0.66515665\n",
      "Iteration 133, loss = 0.66493978\n",
      "Iteration 134, loss = 0.66472284\n",
      "Iteration 135, loss = 0.66450544\n",
      "Iteration 136, loss = 0.66428804\n",
      "Iteration 137, loss = 0.66407091\n",
      "Iteration 138, loss = 0.66385345\n",
      "Iteration 139, loss = 0.66363584\n",
      "Iteration 140, loss = 0.66341795\n",
      "Iteration 141, loss = 0.66319994\n",
      "Iteration 142, loss = 0.66298202\n",
      "Iteration 143, loss = 0.66276423\n",
      "Iteration 144, loss = 0.66254914\n",
      "Iteration 145, loss = 0.66233633\n",
      "Iteration 146, loss = 0.66211536\n",
      "Iteration 147, loss = 0.66189532\n",
      "Iteration 148, loss = 0.66167209\n",
      "Iteration 149, loss = 0.66145300\n",
      "Iteration 150, loss = 0.66124472\n",
      "Iteration 151, loss = 0.66102012\n",
      "Iteration 152, loss = 0.66079897\n",
      "Iteration 153, loss = 0.66058104\n",
      "Iteration 154, loss = 0.66035675\n",
      "Iteration 155, loss = 0.66014488\n",
      "Iteration 156, loss = 0.65992328\n",
      "Iteration 157, loss = 0.65970355\n",
      "Iteration 158, loss = 0.65948367\n",
      "Iteration 159, loss = 0.65926233\n",
      "Iteration 160, loss = 0.65903845\n",
      "Iteration 161, loss = 0.65881444\n",
      "Iteration 162, loss = 0.65859537\n",
      "Iteration 163, loss = 0.65837205\n",
      "Iteration 164, loss = 0.65815017\n",
      "Iteration 165, loss = 0.65792830\n",
      "Iteration 166, loss = 0.65770626\n",
      "Iteration 167, loss = 0.65748413\n",
      "Iteration 168, loss = 0.65726173\n",
      "Iteration 169, loss = 0.65703915\n",
      "Iteration 170, loss = 0.65681651\n",
      "Iteration 171, loss = 0.65659351\n",
      "Iteration 172, loss = 0.65637038\n",
      "Iteration 173, loss = 0.65614706\n",
      "Iteration 174, loss = 0.65592353\n",
      "Iteration 175, loss = 0.65569983\n",
      "Iteration 176, loss = 0.65547602\n",
      "Iteration 177, loss = 0.65525195\n",
      "Iteration 178, loss = 0.65502768\n",
      "Iteration 179, loss = 0.65480321\n",
      "Iteration 180, loss = 0.65457854\n",
      "Iteration 181, loss = 0.65435368\n",
      "Iteration 182, loss = 0.65412868\n",
      "Iteration 183, loss = 0.65390345\n",
      "Iteration 184, loss = 0.65367802\n",
      "Iteration 185, loss = 0.65345238\n",
      "Iteration 186, loss = 0.65322655\n",
      "Iteration 187, loss = 0.65300050\n",
      "Iteration 188, loss = 0.65277430\n",
      "Iteration 189, loss = 0.65254790\n",
      "Iteration 190, loss = 0.65232127\n",
      "Iteration 191, loss = 0.65209443\n",
      "Iteration 192, loss = 0.65186738\n",
      "Iteration 193, loss = 0.65164012\n",
      "Iteration 194, loss = 0.65141265\n",
      "Iteration 195, loss = 0.65118496\n",
      "Iteration 196, loss = 0.65095723\n",
      "Iteration 197, loss = 0.65072903\n",
      "Iteration 198, loss = 0.65050073\n",
      "Iteration 199, loss = 0.65027221\n",
      "Iteration 200, loss = 0.65004347\n",
      "Iteration 201, loss = 0.64981451\n",
      "Iteration 202, loss = 0.64958532\n",
      "Iteration 203, loss = 0.64935603\n",
      "Iteration 204, loss = 0.64912658\n",
      "Iteration 205, loss = 0.64889674\n",
      "Iteration 206, loss = 0.64866646\n",
      "Iteration 207, loss = 0.64843618\n",
      "Iteration 208, loss = 0.64820628\n",
      "Iteration 209, loss = 0.64797529\n",
      "Iteration 210, loss = 0.64774419\n",
      "Iteration 211, loss = 0.64751317\n",
      "Iteration 212, loss = 0.64728148\n",
      "Iteration 213, loss = 0.64705031\n",
      "Iteration 214, loss = 0.64681808\n",
      "Iteration 215, loss = 0.64658610\n",
      "Iteration 216, loss = 0.64635376\n",
      "Iteration 217, loss = 0.64612132\n",
      "Iteration 218, loss = 0.64588829\n",
      "Iteration 219, loss = 0.64565512\n",
      "Iteration 220, loss = 0.64542189\n",
      "Iteration 221, loss = 0.64518831\n",
      "Iteration 222, loss = 0.64495450\n",
      "Iteration 223, loss = 0.64472044\n",
      "Iteration 224, loss = 0.64448619\n",
      "Iteration 225, loss = 0.64425163\n",
      "Iteration 226, loss = 0.64401691\n",
      "Iteration 227, loss = 0.64378172\n",
      "Iteration 228, loss = 0.64354654\n",
      "Iteration 229, loss = 0.64331086\n",
      "Iteration 230, loss = 0.64307504\n",
      "Iteration 231, loss = 0.64283897\n",
      "Iteration 232, loss = 0.64260263\n",
      "Iteration 233, loss = 0.64236603\n",
      "Iteration 234, loss = 0.64212917\n",
      "Iteration 235, loss = 0.64189205\n",
      "Iteration 236, loss = 0.64165476\n",
      "Iteration 237, loss = 0.64141707\n",
      "Iteration 238, loss = 0.64117930\n",
      "Iteration 239, loss = 0.64094175\n",
      "Iteration 240, loss = 0.64070255\n",
      "Iteration 241, loss = 0.64046385\n",
      "Iteration 242, loss = 0.64022488\n",
      "Iteration 243, loss = 0.63998564\n",
      "Iteration 244, loss = 0.63974613\n",
      "Iteration 245, loss = 0.63950635\n",
      "Iteration 246, loss = 0.63926629\n",
      "Iteration 247, loss = 0.63902595\n",
      "Iteration 248, loss = 0.63878534\n",
      "Iteration 249, loss = 0.63854445\n",
      "Iteration 250, loss = 0.63830330\n",
      "Iteration 251, loss = 0.63806230\n",
      "Iteration 252, loss = 0.63782078\n",
      "Iteration 253, loss = 0.63757857\n",
      "Iteration 254, loss = 0.63733587\n",
      "Iteration 255, loss = 0.63709331\n",
      "Iteration 256, loss = 0.63685053\n",
      "Iteration 257, loss = 0.63660831\n",
      "Iteration 258, loss = 0.63636403\n",
      "Iteration 259, loss = 0.63612118\n",
      "Iteration 260, loss = 0.63587756\n",
      "Iteration 261, loss = 0.63563243\n",
      "Iteration 262, loss = 0.63538792\n",
      "Iteration 263, loss = 0.63514266\n",
      "Iteration 264, loss = 0.63489818\n",
      "Iteration 265, loss = 0.63465362\n",
      "Iteration 266, loss = 0.63440635\n",
      "Iteration 267, loss = 0.63416092\n",
      "Iteration 268, loss = 0.63391422\n",
      "Iteration 269, loss = 0.63366742\n",
      "Iteration 270, loss = 0.63342108\n",
      "Iteration 271, loss = 0.63317352\n",
      "Iteration 272, loss = 0.63292560\n",
      "Iteration 273, loss = 0.63267796\n",
      "Iteration 274, loss = 0.63242999\n",
      "Iteration 275, loss = 0.63218194\n",
      "Iteration 276, loss = 0.63193292\n",
      "Iteration 277, loss = 0.63168338\n",
      "Iteration 278, loss = 0.63143441\n",
      "Iteration 279, loss = 0.63118449\n",
      "Iteration 280, loss = 0.63093539\n",
      "Iteration 281, loss = 0.63068485\n",
      "Iteration 282, loss = 0.63043370\n",
      "Iteration 283, loss = 0.63018442\n",
      "Iteration 284, loss = 0.62993240\n",
      "Iteration 285, loss = 0.62968066\n",
      "Iteration 286, loss = 0.62942831\n",
      "Iteration 287, loss = 0.62917744\n",
      "Iteration 288, loss = 0.62892470\n",
      "Iteration 289, loss = 0.62867093\n",
      "Iteration 290, loss = 0.62841908\n",
      "Iteration 291, loss = 0.62816551\n",
      "Iteration 292, loss = 0.62791076\n",
      "Iteration 293, loss = 0.62765768\n",
      "Iteration 294, loss = 0.62740354\n",
      "Iteration 295, loss = 0.62714774\n",
      "Iteration 296, loss = 0.62689333\n",
      "Iteration 297, loss = 0.62663871\n",
      "Iteration 298, loss = 0.62638182\n",
      "Iteration 299, loss = 0.62612607\n",
      "Iteration 300, loss = 0.62587094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72621091\n",
      "Iteration 2, loss = 0.69381846\n",
      "Iteration 3, loss = 0.70002038\n",
      "Iteration 4, loss = 0.69768191\n",
      "Iteration 5, loss = 0.69400136\n",
      "Iteration 6, loss = 0.69347640\n",
      "Iteration 7, loss = 0.69351547\n",
      "Iteration 8, loss = 0.69326124\n",
      "Iteration 9, loss = 0.69304048\n",
      "Iteration 10, loss = 0.69289194\n",
      "Iteration 11, loss = 0.69273727\n",
      "Iteration 12, loss = 0.69256915\n",
      "Iteration 13, loss = 0.69239749\n",
      "Iteration 14, loss = 0.69222178\n",
      "Iteration 15, loss = 0.69204068\n",
      "Iteration 16, loss = 0.69185486\n",
      "Iteration 17, loss = 0.69166498\n",
      "Iteration 18, loss = 0.69147140\n",
      "Iteration 19, loss = 0.69127446\n",
      "Iteration 20, loss = 0.69107453\n",
      "Iteration 21, loss = 0.69087191\n",
      "Iteration 22, loss = 0.69066689\n",
      "Iteration 23, loss = 0.69045971\n",
      "Iteration 24, loss = 0.69025061\n",
      "Iteration 25, loss = 0.69003979\n",
      "Iteration 26, loss = 0.68982744\n",
      "Iteration 27, loss = 0.68961371\n",
      "Iteration 28, loss = 0.68939876\n",
      "Iteration 29, loss = 0.68918272\n",
      "Iteration 30, loss = 0.68896570\n",
      "Iteration 31, loss = 0.68875064\n",
      "Iteration 32, loss = 0.68853019\n",
      "Iteration 33, loss = 0.68831133\n",
      "Iteration 34, loss = 0.68809183\n",
      "Iteration 35, loss = 0.68787281\n",
      "Iteration 36, loss = 0.68764997\n",
      "Iteration 37, loss = 0.68742844\n",
      "Iteration 38, loss = 0.68721205\n",
      "Iteration 39, loss = 0.68698546\n",
      "Iteration 40, loss = 0.68676347\n",
      "Iteration 41, loss = 0.68654119\n",
      "Iteration 42, loss = 0.68631865\n",
      "Iteration 43, loss = 0.68609589\n",
      "Iteration 44, loss = 0.68587293\n",
      "Iteration 45, loss = 0.68564980\n",
      "Iteration 46, loss = 0.68542652\n",
      "Iteration 47, loss = 0.68520311\n",
      "Iteration 48, loss = 0.68497959\n",
      "Iteration 49, loss = 0.68475597\n",
      "Iteration 50, loss = 0.68453228\n",
      "Iteration 51, loss = 0.68430851\n",
      "Iteration 52, loss = 0.68408470\n",
      "Iteration 53, loss = 0.68386390\n",
      "Iteration 54, loss = 0.68363829\n",
      "Iteration 55, loss = 0.68341498\n",
      "Iteration 56, loss = 0.68319667\n",
      "Iteration 57, loss = 0.68296664\n",
      "Iteration 58, loss = 0.68274306\n",
      "Iteration 59, loss = 0.68251979\n",
      "Iteration 60, loss = 0.68229627\n",
      "Iteration 61, loss = 0.68207269\n",
      "Iteration 62, loss = 0.68184906\n",
      "Iteration 63, loss = 0.68162539\n",
      "Iteration 64, loss = 0.68140167\n",
      "Iteration 65, loss = 0.68117791\n",
      "Iteration 66, loss = 0.68095412\n",
      "Iteration 67, loss = 0.68073030\n",
      "Iteration 68, loss = 0.68050645\n",
      "Iteration 69, loss = 0.68028259\n",
      "Iteration 70, loss = 0.68005871\n",
      "Iteration 71, loss = 0.67983537\n",
      "Iteration 72, loss = 0.67961344\n",
      "Iteration 73, loss = 0.67938775\n",
      "Iteration 74, loss = 0.67916541\n",
      "Iteration 75, loss = 0.67894685\n",
      "Iteration 76, loss = 0.67871779\n",
      "Iteration 77, loss = 0.67849466\n",
      "Iteration 78, loss = 0.67827124\n",
      "Iteration 79, loss = 0.67804774\n",
      "Iteration 80, loss = 0.67782417\n",
      "Iteration 81, loss = 0.67760053\n",
      "Iteration 82, loss = 0.67737682\n",
      "Iteration 83, loss = 0.67715304\n",
      "Iteration 84, loss = 0.67692921\n",
      "Iteration 85, loss = 0.67670533\n",
      "Iteration 86, loss = 0.67648192\n",
      "Iteration 87, loss = 0.67626550\n",
      "Iteration 88, loss = 0.67603437\n",
      "Iteration 89, loss = 0.67581655\n",
      "Iteration 90, loss = 0.67559109\n",
      "Iteration 91, loss = 0.67536429\n",
      "Iteration 92, loss = 0.67514207\n",
      "Iteration 93, loss = 0.67492646\n",
      "Iteration 94, loss = 0.67469257\n",
      "Iteration 95, loss = 0.67446880\n",
      "Iteration 96, loss = 0.67424490\n",
      "Iteration 97, loss = 0.67402090\n",
      "Iteration 98, loss = 0.67379790\n",
      "Iteration 99, loss = 0.67358468\n",
      "Iteration 100, loss = 0.67334917\n",
      "Iteration 101, loss = 0.67312894\n",
      "Iteration 102, loss = 0.67290622\n",
      "Iteration 103, loss = 0.67268347\n",
      "Iteration 104, loss = 0.67245367\n",
      "Iteration 105, loss = 0.67223167\n",
      "Iteration 106, loss = 0.67201637\n",
      "Iteration 107, loss = 0.67178034\n",
      "Iteration 108, loss = 0.67155597\n",
      "Iteration 109, loss = 0.67133145\n",
      "Iteration 110, loss = 0.67110696\n",
      "Iteration 111, loss = 0.67089777\n",
      "Iteration 112, loss = 0.67065797\n",
      "Iteration 113, loss = 0.67043509\n",
      "Iteration 114, loss = 0.67021784\n",
      "Iteration 115, loss = 0.66998811\n",
      "Iteration 116, loss = 0.66976262\n",
      "Iteration 117, loss = 0.66954512\n",
      "Iteration 118, loss = 0.66930905\n",
      "Iteration 119, loss = 0.66908400\n",
      "Iteration 120, loss = 0.66885874\n",
      "Iteration 121, loss = 0.66863329\n",
      "Iteration 122, loss = 0.66840765\n",
      "Iteration 123, loss = 0.66818507\n",
      "Iteration 124, loss = 0.66796919\n",
      "Iteration 125, loss = 0.66773256\n",
      "Iteration 126, loss = 0.66751630\n",
      "Iteration 127, loss = 0.66728500\n",
      "Iteration 128, loss = 0.66705722\n",
      "Iteration 129, loss = 0.66684128\n",
      "Iteration 130, loss = 0.66660149\n",
      "Iteration 131, loss = 0.66637526\n",
      "Iteration 132, loss = 0.66614879\n",
      "Iteration 133, loss = 0.66592457\n",
      "Iteration 134, loss = 0.66570682\n",
      "Iteration 135, loss = 0.66548146\n",
      "Iteration 136, loss = 0.66524279\n",
      "Iteration 137, loss = 0.66501858\n",
      "Iteration 138, loss = 0.66479634\n",
      "Iteration 139, loss = 0.66456850\n",
      "Iteration 140, loss = 0.66434006\n",
      "Iteration 141, loss = 0.66411033\n",
      "Iteration 142, loss = 0.66389838\n",
      "Iteration 143, loss = 0.66366034\n",
      "Iteration 144, loss = 0.66342612\n",
      "Iteration 145, loss = 0.66319788\n",
      "Iteration 146, loss = 0.66296899\n",
      "Iteration 147, loss = 0.66273946\n",
      "Iteration 148, loss = 0.66250935\n",
      "Iteration 149, loss = 0.66227872\n",
      "Iteration 150, loss = 0.66205048\n",
      "Iteration 151, loss = 0.66183254\n",
      "Iteration 152, loss = 0.66161560\n",
      "Iteration 153, loss = 0.66137938\n",
      "Iteration 154, loss = 0.66113042\n",
      "Iteration 155, loss = 0.66093410\n",
      "Iteration 156, loss = 0.66070619\n",
      "Iteration 157, loss = 0.66047247\n",
      "Iteration 158, loss = 0.66022682\n",
      "Iteration 159, loss = 0.65998022\n",
      "Iteration 160, loss = 0.65974823\n",
      "Iteration 161, loss = 0.65953514\n",
      "Iteration 162, loss = 0.65931465\n",
      "Iteration 163, loss = 0.65907013\n",
      "Iteration 164, loss = 0.65888021\n",
      "Iteration 165, loss = 0.65862520\n",
      "Iteration 166, loss = 0.65838495\n",
      "Iteration 167, loss = 0.65816278\n",
      "Iteration 168, loss = 0.65794031\n",
      "Iteration 169, loss = 0.65770321\n",
      "Iteration 170, loss = 0.65744142\n",
      "Iteration 171, loss = 0.65728606\n",
      "Iteration 172, loss = 0.65697432\n",
      "Iteration 173, loss = 0.65677358\n",
      "Iteration 174, loss = 0.65655240\n",
      "Iteration 175, loss = 0.65631910\n",
      "Iteration 176, loss = 0.65606934\n",
      "Iteration 177, loss = 0.65583126\n",
      "Iteration 178, loss = 0.65561376\n",
      "Iteration 179, loss = 0.65536026\n",
      "Iteration 180, loss = 0.65513104\n",
      "Iteration 181, loss = 0.65491496\n",
      "Iteration 182, loss = 0.65462466\n",
      "Iteration 183, loss = 0.65443778\n",
      "Iteration 184, loss = 0.65420962\n",
      "Iteration 185, loss = 0.65392185\n",
      "Iteration 186, loss = 0.65371266\n",
      "Iteration 187, loss = 0.65351665\n",
      "Iteration 188, loss = 0.65327841\n",
      "Iteration 189, loss = 0.65297451\n",
      "Iteration 190, loss = 0.65280852\n",
      "Iteration 191, loss = 0.65253208\n",
      "Iteration 192, loss = 0.65226999\n",
      "Iteration 193, loss = 0.65207742\n",
      "Iteration 194, loss = 0.65178755\n",
      "Iteration 195, loss = 0.65161209\n",
      "Iteration 196, loss = 0.65136066\n",
      "Iteration 197, loss = 0.65107533\n",
      "Iteration 198, loss = 0.65088253\n",
      "Iteration 199, loss = 0.65066022\n",
      "Iteration 200, loss = 0.65036841\n",
      "Iteration 201, loss = 0.65012894\n",
      "Iteration 202, loss = 0.64992164\n",
      "Iteration 203, loss = 0.64963964\n",
      "Iteration 204, loss = 0.64940986\n",
      "Iteration 205, loss = 0.64915947\n",
      "Iteration 206, loss = 0.64893191\n",
      "Iteration 207, loss = 0.64867686\n",
      "Iteration 208, loss = 0.64849753\n",
      "Iteration 209, loss = 0.64825395\n",
      "Iteration 210, loss = 0.64796336\n",
      "Iteration 211, loss = 0.64773333\n",
      "Iteration 212, loss = 0.64755839\n",
      "Iteration 213, loss = 0.64726489\n",
      "Iteration 214, loss = 0.64703585\n",
      "Iteration 215, loss = 0.64679427\n",
      "Iteration 216, loss = 0.64650036\n",
      "Iteration 217, loss = 0.64633538\n",
      "Iteration 218, loss = 0.64605678\n",
      "Iteration 219, loss = 0.64577039\n",
      "Iteration 220, loss = 0.64559503\n",
      "Iteration 221, loss = 0.64533805\n",
      "Iteration 222, loss = 0.64504654\n",
      "Iteration 223, loss = 0.64482634\n",
      "Iteration 224, loss = 0.64463116\n",
      "Iteration 225, loss = 0.64433467\n",
      "Iteration 226, loss = 0.64412152\n",
      "Iteration 227, loss = 0.64385735\n",
      "Iteration 228, loss = 0.64356753\n",
      "Iteration 229, loss = 0.64333785\n",
      "Iteration 230, loss = 0.64314357\n",
      "Iteration 231, loss = 0.64287848\n",
      "Iteration 232, loss = 0.64258566\n",
      "Iteration 233, loss = 0.64238973\n",
      "Iteration 234, loss = 0.64215122\n",
      "Iteration 235, loss = 0.64185223\n",
      "Iteration 236, loss = 0.64160888\n",
      "Iteration 237, loss = 0.64143484\n",
      "Iteration 238, loss = 0.64113554\n",
      "Iteration 239, loss = 0.64089227\n",
      "Iteration 240, loss = 0.64065111\n",
      "Iteration 241, loss = 0.64035015\n",
      "Iteration 242, loss = 0.64017123\n",
      "Iteration 243, loss = 0.63989408\n",
      "Iteration 244, loss = 0.63960044\n",
      "Iteration 245, loss = 0.63940960\n",
      "Iteration 246, loss = 0.63915572\n",
      "Iteration 247, loss = 0.63885650\n",
      "Iteration 248, loss = 0.63861913\n",
      "Iteration 249, loss = 0.63842902\n",
      "Iteration 250, loss = 0.63812691\n",
      "Iteration 251, loss = 0.63789133\n",
      "Iteration 252, loss = 0.63763583\n",
      "Iteration 253, loss = 0.63733615\n",
      "Iteration 254, loss = 0.63708725\n",
      "Iteration 255, loss = 0.63688841\n",
      "Iteration 256, loss = 0.63658207\n",
      "Iteration 257, loss = 0.63636121\n",
      "Iteration 258, loss = 0.63613399\n",
      "Iteration 259, loss = 0.63582515\n",
      "Iteration 260, loss = 0.63556781\n",
      "Iteration 261, loss = 0.63539215\n",
      "Iteration 262, loss = 0.63508571\n",
      "Iteration 263, loss = 0.63483461\n",
      "Iteration 264, loss = 0.63458515\n",
      "Iteration 265, loss = 0.63428023\n",
      "Iteration 266, loss = 0.63402560\n",
      "Iteration 267, loss = 0.63377579\n",
      "Iteration 268, loss = 0.63350932\n",
      "Iteration 269, loss = 0.63325210\n",
      "Iteration 270, loss = 0.63300951\n",
      "Iteration 271, loss = 0.63278168\n",
      "Iteration 272, loss = 0.63253776\n",
      "Iteration 273, loss = 0.63222761\n",
      "Iteration 274, loss = 0.63196564\n",
      "Iteration 275, loss = 0.63178830\n",
      "Iteration 276, loss = 0.63147955\n",
      "Iteration 277, loss = 0.63121155\n",
      "Iteration 278, loss = 0.63097245\n",
      "Iteration 279, loss = 0.63065842\n",
      "Iteration 280, loss = 0.63045225\n",
      "Iteration 281, loss = 0.63018256\n",
      "Iteration 282, loss = 0.62987444\n",
      "Iteration 283, loss = 0.62965250\n",
      "Iteration 284, loss = 0.62941057\n",
      "Iteration 285, loss = 0.62909604\n",
      "Iteration 286, loss = 0.62891260\n",
      "Iteration 287, loss = 0.62867472\n",
      "Iteration 288, loss = 0.62838428\n",
      "Iteration 289, loss = 0.62804857\n",
      "Iteration 290, loss = 0.62788661\n",
      "Iteration 291, loss = 0.62759064\n",
      "Iteration 292, loss = 0.62728490\n",
      "Iteration 293, loss = 0.62697669\n",
      "Iteration 294, loss = 0.62677980\n",
      "Iteration 295, loss = 0.62645451\n",
      "Iteration 296, loss = 0.62627077\n",
      "Iteration 297, loss = 0.62601247\n",
      "Iteration 298, loss = 0.62571334\n",
      "Iteration 299, loss = 0.62537636\n",
      "Iteration 300, loss = 0.62515528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72388585\n",
      "Iteration 2, loss = 1.89700893\n",
      "Iteration 3, loss = 1.05003359\n",
      "Iteration 4, loss = 0.69755333\n",
      "Iteration 5, loss = 0.69752352\n",
      "Iteration 6, loss = 0.69749654\n",
      "Iteration 7, loss = 0.69747157\n",
      "Iteration 8, loss = 0.69744811\n",
      "Iteration 9, loss = 0.69742582\n",
      "Iteration 10, loss = 0.69740445\n",
      "Iteration 11, loss = 0.69738382\n",
      "Iteration 12, loss = 0.69736377\n",
      "Iteration 13, loss = 0.69734422\n",
      "Iteration 14, loss = 0.69732505\n",
      "Iteration 15, loss = 0.69730621\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72598529\n",
      "Iteration 2, loss = 1.87252815\n",
      "Iteration 3, loss = 1.06536728\n",
      "Iteration 4, loss = 0.69640982\n",
      "Iteration 5, loss = 0.69638459\n",
      "Iteration 6, loss = 0.69636194\n",
      "Iteration 7, loss = 0.69634116\n",
      "Iteration 8, loss = 0.69632179\n",
      "Iteration 9, loss = 0.69630351\n",
      "Iteration 10, loss = 0.69628611\n",
      "Iteration 11, loss = 0.69626941\n",
      "Iteration 12, loss = 0.69625329\n",
      "Iteration 13, loss = 0.69623766\n",
      "Iteration 14, loss = 0.69622242\n",
      "Iteration 15, loss = 0.69620750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72621091\n",
      "Iteration 2, loss = 1.88352242\n",
      "Iteration 3, loss = 1.05585208\n",
      "Iteration 4, loss = 0.69698162\n",
      "Iteration 5, loss = 0.69695413\n",
      "Iteration 6, loss = 0.69692936\n",
      "Iteration 7, loss = 0.69690652\n",
      "Iteration 8, loss = 0.69688515\n",
      "Iteration 9, loss = 0.69686490\n",
      "Iteration 10, loss = 0.69684556\n",
      "Iteration 11, loss = 0.69682694\n",
      "Iteration 12, loss = 0.69680890\n",
      "Iteration 13, loss = 0.69679135\n",
      "Iteration 14, loss = 0.69677419\n",
      "Iteration 15, loss = 0.69675736\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71967970\n",
      "Iteration 2, loss = 0.74891383\n",
      "Iteration 3, loss = 0.69309195\n",
      "Iteration 4, loss = 0.69303644\n",
      "Iteration 5, loss = 0.69173750\n",
      "Iteration 6, loss = 0.69153652\n",
      "Iteration 7, loss = 0.69131460\n",
      "Iteration 8, loss = 0.69107715\n",
      "Iteration 9, loss = 0.69082240\n",
      "Iteration 10, loss = 0.69055168\n",
      "Iteration 11, loss = 0.69026665\n",
      "Iteration 12, loss = 0.68996876\n",
      "Iteration 13, loss = 0.68965934\n",
      "Iteration 14, loss = 0.68933959\n",
      "Iteration 15, loss = 0.68901059\n",
      "Iteration 16, loss = 0.68867330\n",
      "Iteration 17, loss = 0.68832859\n",
      "Iteration 18, loss = 0.68797724\n",
      "Iteration 19, loss = 0.68761996\n",
      "Iteration 20, loss = 0.68725738\n",
      "Iteration 21, loss = 0.68689006\n",
      "Iteration 22, loss = 0.68651853\n",
      "Iteration 23, loss = 0.68614324\n",
      "Iteration 24, loss = 0.68576460\n",
      "Iteration 25, loss = 0.68538299\n",
      "Iteration 26, loss = 0.68499873\n",
      "Iteration 27, loss = 0.68461214\n",
      "Iteration 28, loss = 0.68422348\n",
      "Iteration 29, loss = 0.68383299\n",
      "Iteration 30, loss = 0.68344088\n",
      "Iteration 31, loss = 0.68304737\n",
      "Iteration 32, loss = 0.68265261\n",
      "Iteration 33, loss = 0.68225676\n",
      "Iteration 34, loss = 0.68185998\n",
      "Iteration 35, loss = 0.68146237\n",
      "Iteration 36, loss = 0.68106407\n",
      "Iteration 37, loss = 0.68066516\n",
      "Iteration 38, loss = 0.68026573\n",
      "Iteration 39, loss = 0.67986588\n",
      "Iteration 40, loss = 0.67946567\n",
      "Iteration 41, loss = 0.67906516\n",
      "Iteration 42, loss = 0.67866442\n",
      "Iteration 43, loss = 0.67826349\n",
      "Iteration 44, loss = 0.67786242\n",
      "Iteration 45, loss = 0.67746126\n",
      "Iteration 46, loss = 0.67706003\n",
      "Iteration 47, loss = 0.67665877\n",
      "Iteration 48, loss = 0.67625751\n",
      "Iteration 49, loss = 0.67585627\n",
      "Iteration 50, loss = 0.67545508\n",
      "Iteration 51, loss = 0.67505395\n",
      "Iteration 52, loss = 0.67465290\n",
      "Iteration 53, loss = 0.67425195\n",
      "Iteration 54, loss = 0.67385110\n",
      "Iteration 55, loss = 0.67345038\n",
      "Iteration 56, loss = 0.67304979\n",
      "Iteration 57, loss = 0.67264933\n",
      "Iteration 58, loss = 0.67224902\n",
      "Iteration 59, loss = 0.67184885\n",
      "Iteration 60, loss = 0.67144885\n",
      "Iteration 61, loss = 0.67104900\n",
      "Iteration 62, loss = 0.67064931\n",
      "Iteration 63, loss = 0.67024978\n",
      "Iteration 64, loss = 0.66985042\n",
      "Iteration 65, loss = 0.66945123\n",
      "Iteration 66, loss = 0.66905220\n",
      "Iteration 67, loss = 0.66865334\n",
      "Iteration 68, loss = 0.66825464\n",
      "Iteration 69, loss = 0.66785611\n",
      "Iteration 70, loss = 0.66745774\n",
      "Iteration 71, loss = 0.66705954\n",
      "Iteration 72, loss = 0.66666149\n",
      "Iteration 73, loss = 0.66626360\n",
      "Iteration 74, loss = 0.66586587\n",
      "Iteration 75, loss = 0.66546829\n",
      "Iteration 76, loss = 0.66507087\n",
      "Iteration 77, loss = 0.66467359\n",
      "Iteration 78, loss = 0.66427645\n",
      "Iteration 79, loss = 0.66387946\n",
      "Iteration 80, loss = 0.66348261\n",
      "Iteration 81, loss = 0.66308589\n",
      "Iteration 82, loss = 0.66268931\n",
      "Iteration 83, loss = 0.66229285\n",
      "Iteration 84, loss = 0.66189652\n",
      "Iteration 85, loss = 0.66150032\n",
      "Iteration 86, loss = 0.66110423\n",
      "Iteration 87, loss = 0.66070825\n",
      "Iteration 88, loss = 0.66031239\n",
      "Iteration 89, loss = 0.65991664\n",
      "Iteration 90, loss = 0.65952099\n",
      "Iteration 91, loss = 0.65912544\n",
      "Iteration 92, loss = 0.65872999\n",
      "Iteration 93, loss = 0.65833464\n",
      "Iteration 94, loss = 0.65793937\n",
      "Iteration 95, loss = 0.65754419\n",
      "Iteration 96, loss = 0.65714909\n",
      "Iteration 97, loss = 0.65675407\n",
      "Iteration 98, loss = 0.65635913\n",
      "Iteration 99, loss = 0.65596425\n",
      "Iteration 100, loss = 0.65556945\n",
      "Iteration 101, loss = 0.65517471\n",
      "Iteration 102, loss = 0.65478003\n",
      "Iteration 103, loss = 0.65438541\n",
      "Iteration 104, loss = 0.65399084\n",
      "Iteration 105, loss = 0.65359632\n",
      "Iteration 106, loss = 0.65320185\n",
      "Iteration 107, loss = 0.65280742\n",
      "Iteration 108, loss = 0.65241303\n",
      "Iteration 109, loss = 0.65201868\n",
      "Iteration 110, loss = 0.65162436\n",
      "Iteration 111, loss = 0.65123006\n",
      "Iteration 112, loss = 0.65083580\n",
      "Iteration 113, loss = 0.65044155\n",
      "Iteration 114, loss = 0.65004733\n",
      "Iteration 115, loss = 0.64965311\n",
      "Iteration 116, loss = 0.64925891\n",
      "Iteration 117, loss = 0.64886472\n",
      "Iteration 118, loss = 0.64847053\n",
      "Iteration 119, loss = 0.64807635\n",
      "Iteration 120, loss = 0.64768216\n",
      "Iteration 121, loss = 0.64728797\n",
      "Iteration 122, loss = 0.64689377\n",
      "Iteration 123, loss = 0.64649956\n",
      "Iteration 124, loss = 0.64610533\n",
      "Iteration 125, loss = 0.64571109\n",
      "Iteration 126, loss = 0.64531682\n",
      "Iteration 127, loss = 0.64492253\n",
      "Iteration 128, loss = 0.64452822\n",
      "Iteration 129, loss = 0.64413387\n",
      "Iteration 130, loss = 0.64373949\n",
      "Iteration 131, loss = 0.64334507\n",
      "Iteration 132, loss = 0.64295061\n",
      "Iteration 133, loss = 0.64255611\n",
      "Iteration 134, loss = 0.64216156\n",
      "Iteration 135, loss = 0.64176697\n",
      "Iteration 136, loss = 0.64137232\n",
      "Iteration 137, loss = 0.64097762\n",
      "Iteration 138, loss = 0.64058286\n",
      "Iteration 139, loss = 0.64018803\n",
      "Iteration 140, loss = 0.63979315\n",
      "Iteration 141, loss = 0.63939820\n",
      "Iteration 142, loss = 0.63900318\n",
      "Iteration 143, loss = 0.63860808\n",
      "Iteration 144, loss = 0.63821291\n",
      "Iteration 145, loss = 0.63781767\n",
      "Iteration 146, loss = 0.63742234\n",
      "Iteration 147, loss = 0.63702693\n",
      "Iteration 148, loss = 0.63663143\n",
      "Iteration 149, loss = 0.63623584\n",
      "Iteration 150, loss = 0.63584017\n",
      "Iteration 151, loss = 0.63544439\n",
      "Iteration 152, loss = 0.63504852\n",
      "Iteration 153, loss = 0.63465256\n",
      "Iteration 154, loss = 0.63425648\n",
      "Iteration 155, loss = 0.63386031\n",
      "Iteration 156, loss = 0.63346402\n",
      "Iteration 157, loss = 0.63306763\n",
      "Iteration 158, loss = 0.63267112\n",
      "Iteration 159, loss = 0.63227450\n",
      "Iteration 160, loss = 0.63187776\n",
      "Iteration 161, loss = 0.63148090\n",
      "Iteration 162, loss = 0.63108392\n",
      "Iteration 163, loss = 0.63068681\n",
      "Iteration 164, loss = 0.63028957\n",
      "Iteration 165, loss = 0.62989221\n",
      "Iteration 166, loss = 0.62949471\n",
      "Iteration 167, loss = 0.62909708\n",
      "Iteration 168, loss = 0.62869931\n",
      "Iteration 169, loss = 0.62830140\n",
      "Iteration 170, loss = 0.62790335\n",
      "Iteration 171, loss = 0.62750515\n",
      "Iteration 172, loss = 0.62710681\n",
      "Iteration 173, loss = 0.62670832\n",
      "Iteration 174, loss = 0.62630968\n",
      "Iteration 175, loss = 0.62591089\n",
      "Iteration 176, loss = 0.62551194\n",
      "Iteration 177, loss = 0.62511283\n",
      "Iteration 178, loss = 0.62471356\n",
      "Iteration 179, loss = 0.62431413\n",
      "Iteration 180, loss = 0.62391454\n",
      "Iteration 181, loss = 0.62351478\n",
      "Iteration 182, loss = 0.62311485\n",
      "Iteration 183, loss = 0.62271475\n",
      "Iteration 184, loss = 0.62231448\n",
      "Iteration 185, loss = 0.62191404\n",
      "Iteration 186, loss = 0.62151342\n",
      "Iteration 187, loss = 0.62111262\n",
      "Iteration 188, loss = 0.62071164\n",
      "Iteration 189, loss = 0.62031048\n",
      "Iteration 190, loss = 0.61990913\n",
      "Iteration 191, loss = 0.61950760\n",
      "Iteration 192, loss = 0.61910587\n",
      "Iteration 193, loss = 0.61870396\n",
      "Iteration 194, loss = 0.61830186\n",
      "Iteration 195, loss = 0.61789956\n",
      "Iteration 196, loss = 0.61749707\n",
      "Iteration 197, loss = 0.61709438\n",
      "Iteration 198, loss = 0.61669149\n",
      "Iteration 199, loss = 0.61628839\n",
      "Iteration 200, loss = 0.61588510\n",
      "Iteration 201, loss = 0.61548160\n",
      "Iteration 202, loss = 0.61507790\n",
      "Iteration 203, loss = 0.61467398\n",
      "Iteration 204, loss = 0.61426986\n",
      "Iteration 205, loss = 0.61386552\n",
      "Iteration 206, loss = 0.61346097\n",
      "Iteration 207, loss = 0.61305621\n",
      "Iteration 208, loss = 0.61265123\n",
      "Iteration 209, loss = 0.61224603\n",
      "Iteration 210, loss = 0.61184062\n",
      "Iteration 211, loss = 0.61143498\n",
      "Iteration 212, loss = 0.61102912\n",
      "Iteration 213, loss = 0.61062303\n",
      "Iteration 214, loss = 0.61021672\n",
      "Iteration 215, loss = 0.60981018\n",
      "Iteration 216, loss = 0.60940341\n",
      "Iteration 217, loss = 0.60899641\n",
      "Iteration 218, loss = 0.60858918\n",
      "Iteration 219, loss = 0.60818172\n",
      "Iteration 220, loss = 0.60777402\n",
      "Iteration 221, loss = 0.60736609\n",
      "Iteration 222, loss = 0.60695792\n",
      "Iteration 223, loss = 0.60654951\n",
      "Iteration 224, loss = 0.60614086\n",
      "Iteration 225, loss = 0.60573196\n",
      "Iteration 226, loss = 0.60532283\n",
      "Iteration 227, loss = 0.60491345\n",
      "Iteration 228, loss = 0.60450382\n",
      "Iteration 229, loss = 0.60409395\n",
      "Iteration 230, loss = 0.60368383\n",
      "Iteration 231, loss = 0.60327346\n",
      "Iteration 232, loss = 0.60286285\n",
      "Iteration 233, loss = 0.60245198\n",
      "Iteration 234, loss = 0.60204085\n",
      "Iteration 235, loss = 0.60162948\n",
      "Iteration 236, loss = 0.60121784\n",
      "Iteration 237, loss = 0.60080596\n",
      "Iteration 238, loss = 0.60039381\n",
      "Iteration 239, loss = 0.59998141\n",
      "Iteration 240, loss = 0.59956874\n",
      "Iteration 241, loss = 0.59915582\n",
      "Iteration 242, loss = 0.59874263\n",
      "Iteration 243, loss = 0.59832918\n",
      "Iteration 244, loss = 0.59791547\n",
      "Iteration 245, loss = 0.59750150\n",
      "Iteration 246, loss = 0.59708725\n",
      "Iteration 247, loss = 0.59667274\n",
      "Iteration 248, loss = 0.59625797\n",
      "Iteration 249, loss = 0.59584292\n",
      "Iteration 250, loss = 0.59542761\n",
      "Iteration 251, loss = 0.59501203\n",
      "Iteration 252, loss = 0.59459617\n",
      "Iteration 253, loss = 0.59418004\n",
      "Iteration 254, loss = 0.59376364\n",
      "Iteration 255, loss = 0.59334697\n",
      "Iteration 256, loss = 0.59293002\n",
      "Iteration 257, loss = 0.59251280\n",
      "Iteration 258, loss = 0.59209530\n",
      "Iteration 259, loss = 0.59167752\n",
      "Iteration 260, loss = 0.59125947\n",
      "Iteration 261, loss = 0.59084113\n",
      "Iteration 262, loss = 0.59042252\n",
      "Iteration 263, loss = 0.59000363\n",
      "Iteration 264, loss = 0.58958446\n",
      "Iteration 265, loss = 0.58916501\n",
      "Iteration 266, loss = 0.58874528\n",
      "Iteration 267, loss = 0.58832526\n",
      "Iteration 268, loss = 0.58790496\n",
      "Iteration 269, loss = 0.58748438\n",
      "Iteration 270, loss = 0.58706351\n",
      "Iteration 271, loss = 0.58664236\n",
      "Iteration 272, loss = 0.58622092\n",
      "Iteration 273, loss = 0.58579920\n",
      "Iteration 274, loss = 0.58537719\n",
      "Iteration 275, loss = 0.58495490\n",
      "Iteration 276, loss = 0.58453231\n",
      "Iteration 277, loss = 0.58410944\n",
      "Iteration 278, loss = 0.58368628\n",
      "Iteration 279, loss = 0.58326284\n",
      "Iteration 280, loss = 0.58283910\n",
      "Iteration 281, loss = 0.58241507\n",
      "Iteration 282, loss = 0.58199076\n",
      "Iteration 283, loss = 0.58156615\n",
      "Iteration 284, loss = 0.58114126\n",
      "Iteration 285, loss = 0.58071607\n",
      "Iteration 286, loss = 0.58029059\n",
      "Iteration 287, loss = 0.57986482\n",
      "Iteration 288, loss = 0.57943876\n",
      "Iteration 289, loss = 0.57901240\n",
      "Iteration 290, loss = 0.57858576\n",
      "Iteration 291, loss = 0.57815882\n",
      "Iteration 292, loss = 0.57773159\n",
      "Iteration 293, loss = 0.57730406\n",
      "Iteration 294, loss = 0.57687624\n",
      "Iteration 295, loss = 0.57644813\n",
      "Iteration 296, loss = 0.57601973\n",
      "Iteration 297, loss = 0.57559103\n",
      "Iteration 298, loss = 0.57516203\n",
      "Iteration 299, loss = 0.57473274\n",
      "Iteration 300, loss = 0.57430316\n",
      "Iteration 1, loss = 0.71598122\n",
      "Iteration 2, loss = 0.74399904\n",
      "Iteration 3, loss = 0.69207240\n",
      "Iteration 4, loss = 0.69163155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.69043419\n",
      "Iteration 6, loss = 0.69026525\n",
      "Iteration 7, loss = 0.69007452\n",
      "Iteration 8, loss = 0.68987133\n",
      "Iteration 9, loss = 0.68965331\n",
      "Iteration 10, loss = 0.68942161\n",
      "Iteration 11, loss = 0.68917762\n",
      "Iteration 12, loss = 0.68892261\n",
      "Iteration 13, loss = 0.68865772\n",
      "Iteration 14, loss = 0.68838395\n",
      "Iteration 15, loss = 0.68810223\n",
      "Iteration 16, loss = 0.68781340\n",
      "Iteration 17, loss = 0.68751818\n",
      "Iteration 18, loss = 0.68721725\n",
      "Iteration 19, loss = 0.68691121\n",
      "Iteration 20, loss = 0.68660061\n",
      "Iteration 21, loss = 0.68628592\n",
      "Iteration 22, loss = 0.68596760\n",
      "Iteration 23, loss = 0.68564602\n",
      "Iteration 24, loss = 0.68532155\n",
      "Iteration 25, loss = 0.68499451\n",
      "Iteration 26, loss = 0.68466517\n",
      "Iteration 27, loss = 0.68433380\n",
      "Iteration 28, loss = 0.68400063\n",
      "Iteration 29, loss = 0.68366586\n",
      "Iteration 30, loss = 0.68332968\n",
      "Iteration 31, loss = 0.68299226\n",
      "Iteration 32, loss = 0.68265375\n",
      "Iteration 33, loss = 0.68231428\n",
      "Iteration 34, loss = 0.68197398\n",
      "Iteration 35, loss = 0.68163295\n",
      "Iteration 36, loss = 0.68129129\n",
      "Iteration 37, loss = 0.68094908\n",
      "Iteration 38, loss = 0.68060641\n",
      "Iteration 39, loss = 0.68026334\n",
      "Iteration 40, loss = 0.67991994\n",
      "Iteration 41, loss = 0.67957627\n",
      "Iteration 42, loss = 0.67923236\n",
      "Iteration 43, loss = 0.67888828\n",
      "Iteration 44, loss = 0.67854405\n",
      "Iteration 45, loss = 0.67819971\n",
      "Iteration 46, loss = 0.67785530\n",
      "Iteration 47, loss = 0.67751083\n",
      "Iteration 48, loss = 0.67716635\n",
      "Iteration 49, loss = 0.67682186\n",
      "Iteration 50, loss = 0.67647739\n",
      "Iteration 51, loss = 0.67613295\n",
      "Iteration 52, loss = 0.67578857\n",
      "Iteration 53, loss = 0.67544424\n",
      "Iteration 54, loss = 0.67509999\n",
      "Iteration 55, loss = 0.67475583\n",
      "Iteration 56, loss = 0.67441176\n",
      "Iteration 57, loss = 0.67406779\n",
      "Iteration 58, loss = 0.67372393\n",
      "Iteration 59, loss = 0.67338017\n",
      "Iteration 60, loss = 0.67303654\n",
      "Iteration 61, loss = 0.67269303\n",
      "Iteration 62, loss = 0.67234963\n",
      "Iteration 63, loss = 0.67200637\n",
      "Iteration 64, loss = 0.67166323\n",
      "Iteration 65, loss = 0.67132022\n",
      "Iteration 66, loss = 0.67097735\n",
      "Iteration 67, loss = 0.67063460\n",
      "Iteration 68, loss = 0.67029198\n",
      "Iteration 69, loss = 0.66994949\n",
      "Iteration 70, loss = 0.66960713\n",
      "Iteration 71, loss = 0.66926490\n",
      "Iteration 72, loss = 0.66892280\n",
      "Iteration 73, loss = 0.66858083\n",
      "Iteration 74, loss = 0.66823897\n",
      "Iteration 75, loss = 0.66789725\n",
      "Iteration 76, loss = 0.66755564\n",
      "Iteration 77, loss = 0.66721415\n",
      "Iteration 78, loss = 0.66687278\n",
      "Iteration 79, loss = 0.66653153\n",
      "Iteration 80, loss = 0.66619039\n",
      "Iteration 81, loss = 0.66584936\n",
      "Iteration 82, loss = 0.66550843\n",
      "Iteration 83, loss = 0.66516762\n",
      "Iteration 84, loss = 0.66482690\n",
      "Iteration 85, loss = 0.66448629\n",
      "Iteration 86, loss = 0.66414578\n",
      "Iteration 87, loss = 0.66380536\n",
      "Iteration 88, loss = 0.66346504\n",
      "Iteration 89, loss = 0.66312480\n",
      "Iteration 90, loss = 0.66278466\n",
      "Iteration 91, loss = 0.66244459\n",
      "Iteration 92, loss = 0.66210461\n",
      "Iteration 93, loss = 0.66176472\n",
      "Iteration 94, loss = 0.66142489\n",
      "Iteration 95, loss = 0.66108514\n",
      "Iteration 96, loss = 0.66074547\n",
      "Iteration 97, loss = 0.66040586\n",
      "Iteration 98, loss = 0.66006632\n",
      "Iteration 99, loss = 0.65972684\n",
      "Iteration 100, loss = 0.65938742\n",
      "Iteration 101, loss = 0.65904805\n",
      "Iteration 102, loss = 0.65870875\n",
      "Iteration 103, loss = 0.65836949\n",
      "Iteration 104, loss = 0.65803029\n",
      "Iteration 105, loss = 0.65769113\n",
      "Iteration 106, loss = 0.65735201\n",
      "Iteration 107, loss = 0.65701294\n",
      "Iteration 108, loss = 0.65667391\n",
      "Iteration 109, loss = 0.65633491\n",
      "Iteration 110, loss = 0.65599594\n",
      "Iteration 111, loss = 0.65565701\n",
      "Iteration 112, loss = 0.65531810\n",
      "Iteration 113, loss = 0.65497922\n",
      "Iteration 114, loss = 0.65464036\n",
      "Iteration 115, loss = 0.65430152\n",
      "Iteration 116, loss = 0.65396270\n",
      "Iteration 117, loss = 0.65362389\n",
      "Iteration 118, loss = 0.65328510\n",
      "Iteration 119, loss = 0.65294631\n",
      "Iteration 120, loss = 0.65260753\n",
      "Iteration 121, loss = 0.65226876\n",
      "Iteration 122, loss = 0.65192999\n",
      "Iteration 123, loss = 0.65159122\n",
      "Iteration 124, loss = 0.65125244\n",
      "Iteration 125, loss = 0.65091366\n",
      "Iteration 126, loss = 0.65057488\n",
      "Iteration 127, loss = 0.65023608\n",
      "Iteration 128, loss = 0.64989727\n",
      "Iteration 129, loss = 0.64955844\n",
      "Iteration 130, loss = 0.64921960\n",
      "Iteration 131, loss = 0.64888074\n",
      "Iteration 132, loss = 0.64854185\n",
      "Iteration 133, loss = 0.64820295\n",
      "Iteration 134, loss = 0.64786401\n",
      "Iteration 135, loss = 0.64752504\n",
      "Iteration 136, loss = 0.64718605\n",
      "Iteration 137, loss = 0.64684702\n",
      "Iteration 138, loss = 0.64650795\n",
      "Iteration 139, loss = 0.64616884\n",
      "Iteration 140, loss = 0.64582969\n",
      "Iteration 141, loss = 0.64549050\n",
      "Iteration 142, loss = 0.64515127\n",
      "Iteration 143, loss = 0.64481199\n",
      "Iteration 144, loss = 0.64447265\n",
      "Iteration 145, loss = 0.64413327\n",
      "Iteration 146, loss = 0.64379383\n",
      "Iteration 147, loss = 0.64345434\n",
      "Iteration 148, loss = 0.64311478\n",
      "Iteration 149, loss = 0.64277517\n",
      "Iteration 150, loss = 0.64243549\n",
      "Iteration 151, loss = 0.64209575\n",
      "Iteration 152, loss = 0.64175595\n",
      "Iteration 153, loss = 0.64141607\n",
      "Iteration 154, loss = 0.64107612\n",
      "Iteration 155, loss = 0.64073610\n",
      "Iteration 156, loss = 0.64039600\n",
      "Iteration 157, loss = 0.64005583\n",
      "Iteration 158, loss = 0.63971557\n",
      "Iteration 159, loss = 0.63937524\n",
      "Iteration 160, loss = 0.63903482\n",
      "Iteration 161, loss = 0.63869432\n",
      "Iteration 162, loss = 0.63835373\n",
      "Iteration 163, loss = 0.63801305\n",
      "Iteration 164, loss = 0.63767228\n",
      "Iteration 165, loss = 0.63733142\n",
      "Iteration 166, loss = 0.63699046\n",
      "Iteration 167, loss = 0.63664940\n",
      "Iteration 168, loss = 0.63630825\n",
      "Iteration 169, loss = 0.63596699\n",
      "Iteration 170, loss = 0.63562564\n",
      "Iteration 171, loss = 0.63528417\n",
      "Iteration 172, loss = 0.63494261\n",
      "Iteration 173, loss = 0.63460093\n",
      "Iteration 174, loss = 0.63425914\n",
      "Iteration 175, loss = 0.63391725\n",
      "Iteration 176, loss = 0.63357524\n",
      "Iteration 177, loss = 0.63323311\n",
      "Iteration 178, loss = 0.63289087\n",
      "Iteration 179, loss = 0.63254851\n",
      "Iteration 180, loss = 0.63220602\n",
      "Iteration 181, loss = 0.63186342\n",
      "Iteration 182, loss = 0.63152069\n",
      "Iteration 183, loss = 0.63117784\n",
      "Iteration 184, loss = 0.63083486\n",
      "Iteration 185, loss = 0.63049175\n",
      "Iteration 186, loss = 0.63014851\n",
      "Iteration 187, loss = 0.62980513\n",
      "Iteration 188, loss = 0.62946163\n",
      "Iteration 189, loss = 0.62911799\n",
      "Iteration 190, loss = 0.62877421\n",
      "Iteration 191, loss = 0.62843029\n",
      "Iteration 192, loss = 0.62808623\n",
      "Iteration 193, loss = 0.62774203\n",
      "Iteration 194, loss = 0.62739769\n",
      "Iteration 195, loss = 0.62705320\n",
      "Iteration 196, loss = 0.62670856\n",
      "Iteration 197, loss = 0.62636378\n",
      "Iteration 198, loss = 0.62601885\n",
      "Iteration 199, loss = 0.62567376\n",
      "Iteration 200, loss = 0.62532853\n",
      "Iteration 201, loss = 0.62498314\n",
      "Iteration 202, loss = 0.62463759\n",
      "Iteration 203, loss = 0.62429189\n",
      "Iteration 204, loss = 0.62394603\n",
      "Iteration 205, loss = 0.62360001\n",
      "Iteration 206, loss = 0.62325383\n",
      "Iteration 207, loss = 0.62290748\n",
      "Iteration 208, loss = 0.62256097\n",
      "Iteration 209, loss = 0.62221430\n",
      "Iteration 210, loss = 0.62186746\n",
      "Iteration 211, loss = 0.62152045\n",
      "Iteration 212, loss = 0.62117328\n",
      "Iteration 213, loss = 0.62082593\n",
      "Iteration 214, loss = 0.62047841\n",
      "Iteration 215, loss = 0.62013071\n",
      "Iteration 216, loss = 0.61978284\n",
      "Iteration 217, loss = 0.61943480\n",
      "Iteration 218, loss = 0.61908658\n",
      "Iteration 219, loss = 0.61873818\n",
      "Iteration 220, loss = 0.61838960\n",
      "Iteration 221, loss = 0.61804084\n",
      "Iteration 222, loss = 0.61769189\n",
      "Iteration 223, loss = 0.61734277\n",
      "Iteration 224, loss = 0.61699346\n",
      "Iteration 225, loss = 0.61664396\n",
      "Iteration 226, loss = 0.61629428\n",
      "Iteration 227, loss = 0.61594441\n",
      "Iteration 228, loss = 0.61559435\n",
      "Iteration 229, loss = 0.61524410\n",
      "Iteration 230, loss = 0.61489365\n",
      "Iteration 231, loss = 0.61454302\n",
      "Iteration 232, loss = 0.61419219\n",
      "Iteration 233, loss = 0.61384117\n",
      "Iteration 234, loss = 0.61348995\n",
      "Iteration 235, loss = 0.61313853\n",
      "Iteration 236, loss = 0.61278692\n",
      "Iteration 237, loss = 0.61243510\n",
      "Iteration 238, loss = 0.61208309\n",
      "Iteration 239, loss = 0.61173087\n",
      "Iteration 240, loss = 0.61137846\n",
      "Iteration 241, loss = 0.61102584\n",
      "Iteration 242, loss = 0.61067301\n",
      "Iteration 243, loss = 0.61031999\n",
      "Iteration 244, loss = 0.60996675\n",
      "Iteration 245, loss = 0.60961331\n",
      "Iteration 246, loss = 0.60925966\n",
      "Iteration 247, loss = 0.60890580\n",
      "Iteration 248, loss = 0.60855173\n",
      "Iteration 249, loss = 0.60819745\n",
      "Iteration 250, loss = 0.60784296\n",
      "Iteration 251, loss = 0.60748826\n",
      "Iteration 252, loss = 0.60713334\n",
      "Iteration 253, loss = 0.60677821\n",
      "Iteration 254, loss = 0.60642286\n",
      "Iteration 255, loss = 0.60606730\n",
      "Iteration 256, loss = 0.60571152\n",
      "Iteration 257, loss = 0.60535553\n",
      "Iteration 258, loss = 0.60499931\n",
      "Iteration 259, loss = 0.60464288\n",
      "Iteration 260, loss = 0.60428623\n",
      "Iteration 261, loss = 0.60392935\n",
      "Iteration 262, loss = 0.60357226\n",
      "Iteration 263, loss = 0.60321494\n",
      "Iteration 264, loss = 0.60285740\n",
      "Iteration 265, loss = 0.60249963\n",
      "Iteration 266, loss = 0.60214165\n",
      "Iteration 267, loss = 0.60178343\n",
      "Iteration 268, loss = 0.60142499\n",
      "Iteration 269, loss = 0.60106633\n",
      "Iteration 270, loss = 0.60070743\n",
      "Iteration 271, loss = 0.60034831\n",
      "Iteration 272, loss = 0.59998896\n",
      "Iteration 273, loss = 0.59962938\n",
      "Iteration 274, loss = 0.59926958\n",
      "Iteration 275, loss = 0.59890954\n",
      "Iteration 276, loss = 0.59854927\n",
      "Iteration 277, loss = 0.59818877\n",
      "Iteration 278, loss = 0.59782803\n",
      "Iteration 279, loss = 0.59746707\n",
      "Iteration 280, loss = 0.59710587\n",
      "Iteration 281, loss = 0.59674444\n",
      "Iteration 282, loss = 0.59638277\n",
      "Iteration 283, loss = 0.59602087\n",
      "Iteration 284, loss = 0.59565873\n",
      "Iteration 285, loss = 0.59529636\n",
      "Iteration 286, loss = 0.59493375\n",
      "Iteration 287, loss = 0.59457090\n",
      "Iteration 288, loss = 0.59420782\n",
      "Iteration 289, loss = 0.59384449\n",
      "Iteration 290, loss = 0.59348093\n",
      "Iteration 291, loss = 0.59311714\n",
      "Iteration 292, loss = 0.59275310\n",
      "Iteration 293, loss = 0.59238882\n",
      "Iteration 294, loss = 0.59202430\n",
      "Iteration 295, loss = 0.59165955\n",
      "Iteration 296, loss = 0.59129455\n",
      "Iteration 297, loss = 0.59092931\n",
      "Iteration 298, loss = 0.59056383\n",
      "Iteration 299, loss = 0.59019810\n",
      "Iteration 300, loss = 0.58983214\n",
      "Iteration 1, loss = 0.71448156\n",
      "Iteration 2, loss = 0.74271691\n",
      "Iteration 3, loss = 0.68834543\n",
      "Iteration 4, loss = 0.68811939\n",
      "Iteration 5, loss = 0.68686855\n",
      "Iteration 6, loss = 0.68669506\n",
      "Iteration 7, loss = 0.68650143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.68629496\n",
      "Iteration 9, loss = 0.68607341\n",
      "Iteration 10, loss = 0.68583792\n",
      "Iteration 11, loss = 0.68558990\n",
      "Iteration 12, loss = 0.68533064\n",
      "Iteration 13, loss = 0.68506129\n",
      "Iteration 14, loss = 0.68478289\n",
      "Iteration 15, loss = 0.68449636\n",
      "Iteration 16, loss = 0.68420254\n",
      "Iteration 17, loss = 0.68390220\n",
      "Iteration 18, loss = 0.68359602\n",
      "Iteration 19, loss = 0.68328459\n",
      "Iteration 20, loss = 0.68296848\n",
      "Iteration 21, loss = 0.68264817\n",
      "Iteration 22, loss = 0.68232411\n",
      "Iteration 23, loss = 0.68199671\n",
      "Iteration 24, loss = 0.68166631\n",
      "Iteration 25, loss = 0.68133325\n",
      "Iteration 26, loss = 0.68099781\n",
      "Iteration 27, loss = 0.68066026\n",
      "Iteration 28, loss = 0.68032082\n",
      "Iteration 29, loss = 0.67997972\n",
      "Iteration 30, loss = 0.67963713\n",
      "Iteration 31, loss = 0.67929323\n",
      "Iteration 32, loss = 0.67894817\n",
      "Iteration 33, loss = 0.67860209\n",
      "Iteration 34, loss = 0.67825511\n",
      "Iteration 35, loss = 0.67790734\n",
      "Iteration 36, loss = 0.67755888\n",
      "Iteration 37, loss = 0.67720982\n",
      "Iteration 38, loss = 0.67686024\n",
      "Iteration 39, loss = 0.67651020\n",
      "Iteration 40, loss = 0.67615977\n",
      "Iteration 41, loss = 0.67580901\n",
      "Iteration 42, loss = 0.67545797\n",
      "Iteration 43, loss = 0.67510670\n",
      "Iteration 44, loss = 0.67475523\n",
      "Iteration 45, loss = 0.67440360\n",
      "Iteration 46, loss = 0.67405184\n",
      "Iteration 47, loss = 0.67369998\n",
      "Iteration 48, loss = 0.67334805\n",
      "Iteration 49, loss = 0.67299607\n",
      "Iteration 50, loss = 0.67264405\n",
      "Iteration 51, loss = 0.67229202\n",
      "Iteration 52, loss = 0.67193999\n",
      "Iteration 53, loss = 0.67158798\n",
      "Iteration 54, loss = 0.67123599\n",
      "Iteration 55, loss = 0.67088404\n",
      "Iteration 56, loss = 0.67053213\n",
      "Iteration 57, loss = 0.67018028\n",
      "Iteration 58, loss = 0.66982849\n",
      "Iteration 59, loss = 0.66947676\n",
      "Iteration 60, loss = 0.66912510\n",
      "Iteration 61, loss = 0.66877351\n",
      "Iteration 62, loss = 0.66842200\n",
      "Iteration 63, loss = 0.66807057\n",
      "Iteration 64, loss = 0.66771922\n",
      "Iteration 65, loss = 0.66736795\n",
      "Iteration 66, loss = 0.66701676\n",
      "Iteration 67, loss = 0.66666566\n",
      "Iteration 68, loss = 0.66631465\n",
      "Iteration 69, loss = 0.66596371\n",
      "Iteration 70, loss = 0.66561286\n",
      "Iteration 71, loss = 0.66526209\n",
      "Iteration 72, loss = 0.66491140\n",
      "Iteration 73, loss = 0.66456080\n",
      "Iteration 74, loss = 0.66421027\n",
      "Iteration 75, loss = 0.66385981\n",
      "Iteration 76, loss = 0.66350944\n",
      "Iteration 77, loss = 0.66315913\n",
      "Iteration 78, loss = 0.66280890\n",
      "Iteration 79, loss = 0.66245874\n",
      "Iteration 80, loss = 0.66210864\n",
      "Iteration 81, loss = 0.66175861\n",
      "Iteration 82, loss = 0.66140864\n",
      "Iteration 83, loss = 0.66105873\n",
      "Iteration 84, loss = 0.66070888\n",
      "Iteration 85, loss = 0.66035909\n",
      "Iteration 86, loss = 0.66000934\n",
      "Iteration 87, loss = 0.65965965\n",
      "Iteration 88, loss = 0.65931000\n",
      "Iteration 89, loss = 0.65896040\n",
      "Iteration 90, loss = 0.65861084\n",
      "Iteration 91, loss = 0.65826132\n",
      "Iteration 92, loss = 0.65791183\n",
      "Iteration 93, loss = 0.65756238\n",
      "Iteration 94, loss = 0.65721296\n",
      "Iteration 95, loss = 0.65686357\n",
      "Iteration 96, loss = 0.65651420\n",
      "Iteration 97, loss = 0.65616486\n",
      "Iteration 98, loss = 0.65581553\n",
      "Iteration 99, loss = 0.65546623\n",
      "Iteration 100, loss = 0.65511694\n",
      "Iteration 101, loss = 0.65476766\n",
      "Iteration 102, loss = 0.65441839\n",
      "Iteration 103, loss = 0.65406912\n",
      "Iteration 104, loss = 0.65371986\n",
      "Iteration 105, loss = 0.65337060\n",
      "Iteration 106, loss = 0.65302134\n",
      "Iteration 107, loss = 0.65267207\n",
      "Iteration 108, loss = 0.65232280\n",
      "Iteration 109, loss = 0.65197352\n",
      "Iteration 110, loss = 0.65162422\n",
      "Iteration 111, loss = 0.65127491\n",
      "Iteration 112, loss = 0.65092558\n",
      "Iteration 113, loss = 0.65057623\n",
      "Iteration 114, loss = 0.65022686\n",
      "Iteration 115, loss = 0.64987746\n",
      "Iteration 116, loss = 0.64952803\n",
      "Iteration 117, loss = 0.64917858\n",
      "Iteration 118, loss = 0.64882909\n",
      "Iteration 119, loss = 0.64847956\n",
      "Iteration 120, loss = 0.64813000\n",
      "Iteration 121, loss = 0.64778039\n",
      "Iteration 122, loss = 0.64743074\n",
      "Iteration 123, loss = 0.64708105\n",
      "Iteration 124, loss = 0.64673130\n",
      "Iteration 125, loss = 0.64638151\n",
      "Iteration 126, loss = 0.64603166\n",
      "Iteration 127, loss = 0.64568176\n",
      "Iteration 128, loss = 0.64533179\n",
      "Iteration 129, loss = 0.64498177\n",
      "Iteration 130, loss = 0.64463168\n",
      "Iteration 131, loss = 0.64428153\n",
      "Iteration 132, loss = 0.64393131\n",
      "Iteration 133, loss = 0.64358102\n",
      "Iteration 134, loss = 0.64323066\n",
      "Iteration 135, loss = 0.64288022\n",
      "Iteration 136, loss = 0.64252971\n",
      "Iteration 137, loss = 0.64217911\n",
      "Iteration 138, loss = 0.64182844\n",
      "Iteration 139, loss = 0.64147768\n",
      "Iteration 140, loss = 0.64112683\n",
      "Iteration 141, loss = 0.64077589\n",
      "Iteration 142, loss = 0.64042487\n",
      "Iteration 143, loss = 0.64007375\n",
      "Iteration 144, loss = 0.63972253\n",
      "Iteration 145, loss = 0.63937122\n",
      "Iteration 146, loss = 0.63901981\n",
      "Iteration 147, loss = 0.63866830\n",
      "Iteration 148, loss = 0.63831668\n",
      "Iteration 149, loss = 0.63796496\n",
      "Iteration 150, loss = 0.63761312\n",
      "Iteration 151, loss = 0.63726118\n",
      "Iteration 152, loss = 0.63690913\n",
      "Iteration 153, loss = 0.63655696\n",
      "Iteration 154, loss = 0.63620467\n",
      "Iteration 155, loss = 0.63585227\n",
      "Iteration 156, loss = 0.63549974\n",
      "Iteration 157, loss = 0.63514709\n",
      "Iteration 158, loss = 0.63479432\n",
      "Iteration 159, loss = 0.63444142\n",
      "Iteration 160, loss = 0.63408839\n",
      "Iteration 161, loss = 0.63373523\n",
      "Iteration 162, loss = 0.63338194\n",
      "Iteration 163, loss = 0.63302852\n",
      "Iteration 164, loss = 0.63267495\n",
      "Iteration 165, loss = 0.63232125\n",
      "Iteration 166, loss = 0.63196741\n",
      "Iteration 167, loss = 0.63161343\n",
      "Iteration 168, loss = 0.63125930\n",
      "Iteration 169, loss = 0.63090503\n",
      "Iteration 170, loss = 0.63055061\n",
      "Iteration 171, loss = 0.63019603\n",
      "Iteration 172, loss = 0.62984131\n",
      "Iteration 173, loss = 0.62948644\n",
      "Iteration 174, loss = 0.62913140\n",
      "Iteration 175, loss = 0.62877622\n",
      "Iteration 176, loss = 0.62842087\n",
      "Iteration 177, loss = 0.62806536\n",
      "Iteration 178, loss = 0.62770969\n",
      "Iteration 179, loss = 0.62735386\n",
      "Iteration 180, loss = 0.62699786\n",
      "Iteration 181, loss = 0.62664170\n",
      "Iteration 182, loss = 0.62628536\n",
      "Iteration 183, loss = 0.62592885\n",
      "Iteration 184, loss = 0.62557218\n",
      "Iteration 185, loss = 0.62521532\n",
      "Iteration 186, loss = 0.62485830\n",
      "Iteration 187, loss = 0.62450109\n",
      "Iteration 188, loss = 0.62414371\n",
      "Iteration 189, loss = 0.62378614\n",
      "Iteration 190, loss = 0.62342840\n",
      "Iteration 191, loss = 0.62307047\n",
      "Iteration 192, loss = 0.62271235\n",
      "Iteration 193, loss = 0.62235405\n",
      "Iteration 194, loss = 0.62199556\n",
      "Iteration 195, loss = 0.62163688\n",
      "Iteration 196, loss = 0.62127801\n",
      "Iteration 197, loss = 0.62091894\n",
      "Iteration 198, loss = 0.62055968\n",
      "Iteration 199, loss = 0.62020023\n",
      "Iteration 200, loss = 0.61984057\n",
      "Iteration 201, loss = 0.61948072\n",
      "Iteration 202, loss = 0.61912067\n",
      "Iteration 203, loss = 0.61876042\n",
      "Iteration 204, loss = 0.61839997\n",
      "Iteration 205, loss = 0.61803931\n",
      "Iteration 206, loss = 0.61767844\n",
      "Iteration 207, loss = 0.61731737\n",
      "Iteration 208, loss = 0.61695609\n",
      "Iteration 209, loss = 0.61659459\n",
      "Iteration 210, loss = 0.61623289\n",
      "Iteration 211, loss = 0.61587098\n",
      "Iteration 212, loss = 0.61550885\n",
      "Iteration 213, loss = 0.61514651\n",
      "Iteration 214, loss = 0.61478395\n",
      "Iteration 215, loss = 0.61442117\n",
      "Iteration 216, loss = 0.61405817\n",
      "Iteration 217, loss = 0.61369495\n",
      "Iteration 218, loss = 0.61333152\n",
      "Iteration 219, loss = 0.61296786\n",
      "Iteration 220, loss = 0.61260397\n",
      "Iteration 221, loss = 0.61223986\n",
      "Iteration 222, loss = 0.61187553\n",
      "Iteration 223, loss = 0.61151096\n",
      "Iteration 224, loss = 0.61114617\n",
      "Iteration 225, loss = 0.61078115\n",
      "Iteration 226, loss = 0.61041590\n",
      "Iteration 227, loss = 0.61005042\n",
      "Iteration 228, loss = 0.60968470\n",
      "Iteration 229, loss = 0.60931875\n",
      "Iteration 230, loss = 0.60895256\n",
      "Iteration 231, loss = 0.60858614\n",
      "Iteration 232, loss = 0.60821948\n",
      "Iteration 233, loss = 0.60785258\n",
      "Iteration 234, loss = 0.60748544\n",
      "Iteration 235, loss = 0.60711807\n",
      "Iteration 236, loss = 0.60675045\n",
      "Iteration 237, loss = 0.60638259\n",
      "Iteration 238, loss = 0.60601448\n",
      "Iteration 239, loss = 0.60564613\n",
      "Iteration 240, loss = 0.60527754\n",
      "Iteration 241, loss = 0.60490869\n",
      "Iteration 242, loss = 0.60453961\n",
      "Iteration 243, loss = 0.60417027\n",
      "Iteration 244, loss = 0.60380068\n",
      "Iteration 245, loss = 0.60343085\n",
      "Iteration 246, loss = 0.60306076\n",
      "Iteration 247, loss = 0.60269042\n",
      "Iteration 248, loss = 0.60231983\n",
      "Iteration 249, loss = 0.60194899\n",
      "Iteration 250, loss = 0.60157789\n",
      "Iteration 251, loss = 0.60120654\n",
      "Iteration 252, loss = 0.60083493\n",
      "Iteration 253, loss = 0.60046306\n",
      "Iteration 254, loss = 0.60009094\n",
      "Iteration 255, loss = 0.59971856\n",
      "Iteration 256, loss = 0.59934592\n",
      "Iteration 257, loss = 0.59897302\n",
      "Iteration 258, loss = 0.59859985\n",
      "Iteration 259, loss = 0.59822643\n",
      "Iteration 260, loss = 0.59785275\n",
      "Iteration 261, loss = 0.59747880\n",
      "Iteration 262, loss = 0.59710459\n",
      "Iteration 263, loss = 0.59673011\n",
      "Iteration 264, loss = 0.59635538\n",
      "Iteration 265, loss = 0.59598037\n",
      "Iteration 266, loss = 0.59560510\n",
      "Iteration 267, loss = 0.59522956\n",
      "Iteration 268, loss = 0.59485376\n",
      "Iteration 269, loss = 0.59447769\n",
      "Iteration 270, loss = 0.59410134\n",
      "Iteration 271, loss = 0.59372473\n",
      "Iteration 272, loss = 0.59334785\n",
      "Iteration 273, loss = 0.59297070\n",
      "Iteration 274, loss = 0.59259328\n",
      "Iteration 275, loss = 0.59221559\n",
      "Iteration 276, loss = 0.59183763\n",
      "Iteration 277, loss = 0.59145939\n",
      "Iteration 278, loss = 0.59108088\n",
      "Iteration 279, loss = 0.59070210\n",
      "Iteration 280, loss = 0.59032304\n",
      "Iteration 281, loss = 0.58994371\n",
      "Iteration 282, loss = 0.58956411\n",
      "Iteration 283, loss = 0.58918423\n",
      "Iteration 284, loss = 0.58880407\n",
      "Iteration 285, loss = 0.58842364\n",
      "Iteration 286, loss = 0.58804293\n",
      "Iteration 287, loss = 0.58766195\n",
      "Iteration 288, loss = 0.58728068\n",
      "Iteration 289, loss = 0.58689914\n",
      "Iteration 290, loss = 0.58651733\n",
      "Iteration 291, loss = 0.58613523\n",
      "Iteration 292, loss = 0.58575285\n",
      "Iteration 293, loss = 0.58537020\n",
      "Iteration 294, loss = 0.58498727\n",
      "Iteration 295, loss = 0.58460405\n",
      "Iteration 296, loss = 0.58422056\n",
      "Iteration 297, loss = 0.58383679\n",
      "Iteration 298, loss = 0.58345274\n",
      "Iteration 299, loss = 0.58306840\n",
      "Iteration 300, loss = 0.58268379\n",
      "Iteration 1, loss = 0.71967970\n",
      "Iteration 2, loss = 3.08046245\n",
      "Iteration 3, loss = 1.56812678\n",
      "Iteration 4, loss = 0.71809153\n",
      "Iteration 5, loss = 0.71814824\n",
      "Iteration 6, loss = 0.71818606\n",
      "Iteration 7, loss = 0.71820922\n",
      "Iteration 8, loss = 0.71822053\n",
      "Iteration 9, loss = 0.71822197\n",
      "Iteration 10, loss = 0.71821504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.71820089\n",
      "Iteration 12, loss = 0.71818045\n",
      "Iteration 13, loss = 0.71815446\n",
      "Iteration 14, loss = 0.71812356\n",
      "Iteration 15, loss = 0.71808829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71598122\n",
      "Iteration 2, loss = 3.12114329\n",
      "Iteration 3, loss = 1.55381616\n",
      "Iteration 4, loss = 0.71524796\n",
      "Iteration 5, loss = 0.71530273\n",
      "Iteration 6, loss = 0.71534015\n",
      "Iteration 7, loss = 0.71536409\n",
      "Iteration 8, loss = 0.71537711\n",
      "Iteration 9, loss = 0.71538103\n",
      "Iteration 10, loss = 0.71537720\n",
      "Iteration 11, loss = 0.71536669\n",
      "Iteration 12, loss = 0.71535034\n",
      "Iteration 13, loss = 0.71532883\n",
      "Iteration 14, loss = 0.71530275\n",
      "Iteration 15, loss = 0.71527258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448156\n",
      "Iteration 2, loss = 3.09886872\n",
      "Iteration 3, loss = 1.56176307\n",
      "Iteration 4, loss = 0.71666962\n",
      "Iteration 5, loss = 0.71672536\n",
      "Iteration 6, loss = 0.71676299\n",
      "Iteration 7, loss = 0.71678657\n",
      "Iteration 8, loss = 0.71679876\n",
      "Iteration 9, loss = 0.71680148\n",
      "Iteration 10, loss = 0.71679615\n",
      "Iteration 11, loss = 0.71678386\n",
      "Iteration 12, loss = 0.71676551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72546705\n",
      "Iteration 2, loss = 0.69329974\n",
      "Iteration 3, loss = 0.69811988\n",
      "Iteration 4, loss = 0.69712530\n",
      "Iteration 5, loss = 0.69301332\n",
      "Iteration 6, loss = 0.69138203\n",
      "Iteration 7, loss = 0.69142679\n",
      "Iteration 8, loss = 0.69135482\n",
      "Iteration 9, loss = 0.69100863\n",
      "Iteration 10, loss = 0.69074992\n",
      "Iteration 11, loss = 0.69061622\n",
      "Iteration 12, loss = 0.69048723\n",
      "Iteration 13, loss = 0.69033022\n",
      "Iteration 14, loss = 0.69016741\n",
      "Iteration 15, loss = 0.69000835\n",
      "Iteration 16, loss = 0.68984781\n",
      "Iteration 17, loss = 0.68968239\n",
      "Iteration 18, loss = 0.68951308\n",
      "Iteration 19, loss = 0.68934112\n",
      "Iteration 20, loss = 0.68916692\n",
      "Iteration 21, loss = 0.68899102\n",
      "Iteration 22, loss = 0.68881287\n",
      "Iteration 23, loss = 0.68863276\n",
      "Iteration 24, loss = 0.68845091\n",
      "Iteration 25, loss = 0.68826748\n",
      "Iteration 26, loss = 0.68808263\n",
      "Iteration 27, loss = 0.68789650\n",
      "Iteration 28, loss = 0.68770923\n",
      "Iteration 29, loss = 0.68752094\n",
      "Iteration 30, loss = 0.68733173\n",
      "Iteration 31, loss = 0.68714169\n",
      "Iteration 32, loss = 0.68695092\n",
      "Iteration 33, loss = 0.68675949\n",
      "Iteration 34, loss = 0.68656747\n",
      "Iteration 35, loss = 0.68637492\n",
      "Iteration 36, loss = 0.68618190\n",
      "Iteration 37, loss = 0.68598845\n",
      "Iteration 38, loss = 0.68579462\n",
      "Iteration 39, loss = 0.68560045\n",
      "Iteration 40, loss = 0.68540597\n",
      "Iteration 41, loss = 0.68521122\n",
      "Iteration 42, loss = 0.68501623\n",
      "Iteration 43, loss = 0.68482101\n",
      "Iteration 44, loss = 0.68462560\n",
      "Iteration 45, loss = 0.68443001\n",
      "Iteration 46, loss = 0.68423425\n",
      "Iteration 47, loss = 0.68403836\n",
      "Iteration 48, loss = 0.68384233\n",
      "Iteration 49, loss = 0.68364618\n",
      "Iteration 50, loss = 0.68344993\n",
      "Iteration 51, loss = 0.68325358\n",
      "Iteration 52, loss = 0.68305713\n",
      "Iteration 53, loss = 0.68286061\n",
      "Iteration 54, loss = 0.68266401\n",
      "Iteration 55, loss = 0.68246734\n",
      "Iteration 56, loss = 0.68227060\n",
      "Iteration 57, loss = 0.68207379\n",
      "Iteration 58, loss = 0.68187693\n",
      "Iteration 59, loss = 0.68168002\n",
      "Iteration 60, loss = 0.68148305\n",
      "Iteration 61, loss = 0.68128602\n",
      "Iteration 62, loss = 0.68108895\n",
      "Iteration 63, loss = 0.68089182\n",
      "Iteration 64, loss = 0.68069465\n",
      "Iteration 65, loss = 0.68049743\n",
      "Iteration 66, loss = 0.68030016\n",
      "Iteration 67, loss = 0.68010284\n",
      "Iteration 68, loss = 0.67990547\n",
      "Iteration 69, loss = 0.67970805\n",
      "Iteration 70, loss = 0.67951058\n",
      "Iteration 71, loss = 0.67931306\n",
      "Iteration 72, loss = 0.67911549\n",
      "Iteration 73, loss = 0.67891786\n",
      "Iteration 74, loss = 0.67872018\n",
      "Iteration 75, loss = 0.67852244\n",
      "Iteration 76, loss = 0.67832465\n",
      "Iteration 77, loss = 0.67812680\n",
      "Iteration 78, loss = 0.67792889\n",
      "Iteration 79, loss = 0.67773091\n",
      "Iteration 80, loss = 0.67753287\n",
      "Iteration 81, loss = 0.67733477\n",
      "Iteration 82, loss = 0.67713660\n",
      "Iteration 83, loss = 0.67693836\n",
      "Iteration 84, loss = 0.67674005\n",
      "Iteration 85, loss = 0.67654167\n",
      "Iteration 86, loss = 0.67634321\n",
      "Iteration 87, loss = 0.67614468\n",
      "Iteration 88, loss = 0.67594606\n",
      "Iteration 89, loss = 0.67574737\n",
      "Iteration 90, loss = 0.67554860\n",
      "Iteration 91, loss = 0.67534974\n",
      "Iteration 92, loss = 0.67515079\n",
      "Iteration 93, loss = 0.67495176\n",
      "Iteration 94, loss = 0.67475263\n",
      "Iteration 95, loss = 0.67455342\n",
      "Iteration 96, loss = 0.67435410\n",
      "Iteration 97, loss = 0.67415469\n",
      "Iteration 98, loss = 0.67395519\n",
      "Iteration 99, loss = 0.67375558\n",
      "Iteration 100, loss = 0.67355587\n",
      "Iteration 101, loss = 0.67335605\n",
      "Iteration 102, loss = 0.67315613\n",
      "Iteration 103, loss = 0.67295610\n",
      "Iteration 104, loss = 0.67275596\n",
      "Iteration 105, loss = 0.67255570\n",
      "Iteration 106, loss = 0.67235533\n",
      "Iteration 107, loss = 0.67215484\n",
      "Iteration 108, loss = 0.67195423\n",
      "Iteration 109, loss = 0.67175350\n",
      "Iteration 110, loss = 0.67155265\n",
      "Iteration 111, loss = 0.67135167\n",
      "Iteration 112, loss = 0.67115057\n",
      "Iteration 113, loss = 0.67094933\n",
      "Iteration 114, loss = 0.67074796\n",
      "Iteration 115, loss = 0.67054646\n",
      "Iteration 116, loss = 0.67034482\n",
      "Iteration 117, loss = 0.67014305\n",
      "Iteration 118, loss = 0.66994114\n",
      "Iteration 119, loss = 0.66973908\n",
      "Iteration 120, loss = 0.66953688\n",
      "Iteration 121, loss = 0.66933453\n",
      "Iteration 122, loss = 0.66913204\n",
      "Iteration 123, loss = 0.66892940\n",
      "Iteration 124, loss = 0.66872660\n",
      "Iteration 125, loss = 0.66852365\n",
      "Iteration 126, loss = 0.66832054\n",
      "Iteration 127, loss = 0.66811728\n",
      "Iteration 128, loss = 0.66791386\n",
      "Iteration 129, loss = 0.66771027\n",
      "Iteration 130, loss = 0.66750652\n",
      "Iteration 131, loss = 0.66730261\n",
      "Iteration 132, loss = 0.66709852\n",
      "Iteration 133, loss = 0.66689427\n",
      "Iteration 134, loss = 0.66668984\n",
      "Iteration 135, loss = 0.66648524\n",
      "Iteration 136, loss = 0.66628047\n",
      "Iteration 137, loss = 0.66607552\n",
      "Iteration 138, loss = 0.66587039\n",
      "Iteration 139, loss = 0.66566507\n",
      "Iteration 140, loss = 0.66545958\n",
      "Iteration 141, loss = 0.66525389\n",
      "Iteration 142, loss = 0.66504802\n",
      "Iteration 143, loss = 0.66484196\n",
      "Iteration 144, loss = 0.66463571\n",
      "Iteration 145, loss = 0.66442927\n",
      "Iteration 146, loss = 0.66422263\n",
      "Iteration 147, loss = 0.66401580\n",
      "Iteration 148, loss = 0.66380876\n",
      "Iteration 149, loss = 0.66360153\n",
      "Iteration 150, loss = 0.66339409\n",
      "Iteration 151, loss = 0.66318645\n",
      "Iteration 152, loss = 0.66297860\n",
      "Iteration 153, loss = 0.66277054\n",
      "Iteration 154, loss = 0.66256227\n",
      "Iteration 155, loss = 0.66235379\n",
      "Iteration 156, loss = 0.66214510\n",
      "Iteration 157, loss = 0.66193619\n",
      "Iteration 158, loss = 0.66172706\n",
      "Iteration 159, loss = 0.66151771\n",
      "Iteration 160, loss = 0.66130815\n",
      "Iteration 161, loss = 0.66109836\n",
      "Iteration 162, loss = 0.66088834\n",
      "Iteration 163, loss = 0.66067810\n",
      "Iteration 164, loss = 0.66046762\n",
      "Iteration 165, loss = 0.66025692\n",
      "Iteration 166, loss = 0.66004599\n",
      "Iteration 167, loss = 0.65983482\n",
      "Iteration 168, loss = 0.65962341\n",
      "Iteration 169, loss = 0.65941177\n",
      "Iteration 170, loss = 0.65919989\n",
      "Iteration 171, loss = 0.65898776\n",
      "Iteration 172, loss = 0.65877540\n",
      "Iteration 173, loss = 0.65856278\n",
      "Iteration 174, loss = 0.65834992\n",
      "Iteration 175, loss = 0.65813682\n",
      "Iteration 176, loss = 0.65792346\n",
      "Iteration 177, loss = 0.65770985\n",
      "Iteration 178, loss = 0.65749598\n",
      "Iteration 179, loss = 0.65728186\n",
      "Iteration 180, loss = 0.65706748\n",
      "Iteration 181, loss = 0.65685285\n",
      "Iteration 182, loss = 0.65663795\n",
      "Iteration 183, loss = 0.65642279\n",
      "Iteration 184, loss = 0.65620736\n",
      "Iteration 185, loss = 0.65599167\n",
      "Iteration 186, loss = 0.65577571\n",
      "Iteration 187, loss = 0.65555949\n",
      "Iteration 188, loss = 0.65534298\n",
      "Iteration 189, loss = 0.65512621\n",
      "Iteration 190, loss = 0.65490916\n",
      "Iteration 191, loss = 0.65469184\n",
      "Iteration 192, loss = 0.65447423\n",
      "Iteration 193, loss = 0.65425635\n",
      "Iteration 194, loss = 0.65403819\n",
      "Iteration 195, loss = 0.65381974\n",
      "Iteration 196, loss = 0.65360101\n",
      "Iteration 197, loss = 0.65338198\n",
      "Iteration 198, loss = 0.65316268\n",
      "Iteration 199, loss = 0.65294308\n",
      "Iteration 200, loss = 0.65272319\n",
      "Iteration 201, loss = 0.65250300\n",
      "Iteration 202, loss = 0.65228252\n",
      "Iteration 203, loss = 0.65206175\n",
      "Iteration 204, loss = 0.65184067\n",
      "Iteration 205, loss = 0.65161930\n",
      "Iteration 206, loss = 0.65139762\n",
      "Iteration 207, loss = 0.65117564\n",
      "Iteration 208, loss = 0.65095336\n",
      "Iteration 209, loss = 0.65073077\n",
      "Iteration 210, loss = 0.65050787\n",
      "Iteration 211, loss = 0.65028466\n",
      "Iteration 212, loss = 0.65006114\n",
      "Iteration 213, loss = 0.64983730\n",
      "Iteration 214, loss = 0.64961315\n",
      "Iteration 215, loss = 0.64938869\n",
      "Iteration 216, loss = 0.64916391\n",
      "Iteration 217, loss = 0.64893880\n",
      "Iteration 218, loss = 0.64871338\n",
      "Iteration 219, loss = 0.64848763\n",
      "Iteration 220, loss = 0.64826156\n",
      "Iteration 221, loss = 0.64803516\n",
      "Iteration 222, loss = 0.64780844\n",
      "Iteration 223, loss = 0.64758139\n",
      "Iteration 224, loss = 0.64735400\n",
      "Iteration 225, loss = 0.64712629\n",
      "Iteration 226, loss = 0.64689824\n",
      "Iteration 227, loss = 0.64666985\n",
      "Iteration 228, loss = 0.64644113\n",
      "Iteration 229, loss = 0.64621207\n",
      "Iteration 230, loss = 0.64598267\n",
      "Iteration 231, loss = 0.64575293\n",
      "Iteration 232, loss = 0.64552284\n",
      "Iteration 233, loss = 0.64529241\n",
      "Iteration 234, loss = 0.64506164\n",
      "Iteration 235, loss = 0.64483051\n",
      "Iteration 236, loss = 0.64459904\n",
      "Iteration 237, loss = 0.64436722\n",
      "Iteration 238, loss = 0.64413504\n",
      "Iteration 239, loss = 0.64390252\n",
      "Iteration 240, loss = 0.64366963\n",
      "Iteration 241, loss = 0.64343639\n",
      "Iteration 242, loss = 0.64320280\n",
      "Iteration 243, loss = 0.64296884\n",
      "Iteration 244, loss = 0.64273452\n",
      "Iteration 245, loss = 0.64249984\n",
      "Iteration 246, loss = 0.64226480\n",
      "Iteration 247, loss = 0.64202939\n",
      "Iteration 248, loss = 0.64179361\n",
      "Iteration 249, loss = 0.64155747\n",
      "Iteration 250, loss = 0.64132096\n",
      "Iteration 251, loss = 0.64108407\n",
      "Iteration 252, loss = 0.64084681\n",
      "Iteration 253, loss = 0.64060918\n",
      "Iteration 254, loss = 0.64037118\n",
      "Iteration 255, loss = 0.64013279\n",
      "Iteration 256, loss = 0.63989403\n",
      "Iteration 257, loss = 0.63965489\n",
      "Iteration 258, loss = 0.63941537\n",
      "Iteration 259, loss = 0.63917547\n",
      "Iteration 260, loss = 0.63893518\n",
      "Iteration 261, loss = 0.63869450\n",
      "Iteration 262, loss = 0.63845345\n",
      "Iteration 263, loss = 0.63821200\n",
      "Iteration 264, loss = 0.63797016\n",
      "Iteration 265, loss = 0.63772793\n",
      "Iteration 266, loss = 0.63748532\n",
      "Iteration 267, loss = 0.63724230\n",
      "Iteration 268, loss = 0.63699890\n",
      "Iteration 269, loss = 0.63675509\n",
      "Iteration 270, loss = 0.63651089\n",
      "Iteration 271, loss = 0.63626629\n",
      "Iteration 272, loss = 0.63602130\n",
      "Iteration 273, loss = 0.63577590\n",
      "Iteration 274, loss = 0.63553009\n",
      "Iteration 275, loss = 0.63528389\n",
      "Iteration 276, loss = 0.63503728\n",
      "Iteration 277, loss = 0.63479026\n",
      "Iteration 278, loss = 0.63454283\n",
      "Iteration 279, loss = 0.63429500\n",
      "Iteration 280, loss = 0.63404675\n",
      "Iteration 281, loss = 0.63379809\n",
      "Iteration 282, loss = 0.63354902\n",
      "Iteration 283, loss = 0.63329954\n",
      "Iteration 284, loss = 0.63304964\n",
      "Iteration 285, loss = 0.63279932\n",
      "Iteration 286, loss = 0.63254859\n",
      "Iteration 287, loss = 0.63229744\n",
      "Iteration 288, loss = 0.63204586\n",
      "Iteration 289, loss = 0.63179387\n",
      "Iteration 290, loss = 0.63154145\n",
      "Iteration 291, loss = 0.63128861\n",
      "Iteration 292, loss = 0.63103534\n",
      "Iteration 293, loss = 0.63078164\n",
      "Iteration 294, loss = 0.63052752\n",
      "Iteration 295, loss = 0.63027297\n",
      "Iteration 296, loss = 0.63001799\n",
      "Iteration 297, loss = 0.62976258\n",
      "Iteration 298, loss = 0.62950673\n",
      "Iteration 299, loss = 0.62925046\n",
      "Iteration 300, loss = 0.62899374\n",
      "Iteration 1, loss = 0.72123287\n",
      "Iteration 2, loss = 0.69217677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.69668089\n",
      "Iteration 4, loss = 0.69489339\n",
      "Iteration 5, loss = 0.69161403\n",
      "Iteration 6, loss = 0.69080884\n",
      "Iteration 7, loss = 0.69088688\n",
      "Iteration 8, loss = 0.69081240\n",
      "Iteration 9, loss = 0.69058845\n",
      "Iteration 10, loss = 0.69041511\n",
      "Iteration 11, loss = 0.69030077\n",
      "Iteration 12, loss = 0.69018541\n",
      "Iteration 13, loss = 0.69005417\n",
      "Iteration 14, loss = 0.68991858\n",
      "Iteration 15, loss = 0.68978331\n",
      "Iteration 16, loss = 0.68964580\n",
      "Iteration 17, loss = 0.68950456\n",
      "Iteration 18, loss = 0.68936025\n",
      "Iteration 19, loss = 0.68921361\n",
      "Iteration 20, loss = 0.68906481\n",
      "Iteration 21, loss = 0.68891394\n",
      "Iteration 22, loss = 0.68876120\n",
      "Iteration 23, loss = 0.68860681\n",
      "Iteration 24, loss = 0.68845095\n",
      "Iteration 25, loss = 0.68829375\n",
      "Iteration 26, loss = 0.68813535\n",
      "Iteration 27, loss = 0.68797587\n",
      "Iteration 28, loss = 0.68781545\n",
      "Iteration 29, loss = 0.68765414\n",
      "Iteration 30, loss = 0.68749206\n",
      "Iteration 31, loss = 0.68732929\n",
      "Iteration 32, loss = 0.68716589\n",
      "Iteration 33, loss = 0.68700194\n",
      "Iteration 34, loss = 0.68683749\n",
      "Iteration 35, loss = 0.68667259\n",
      "Iteration 36, loss = 0.68650728\n",
      "Iteration 37, loss = 0.68634162\n",
      "Iteration 38, loss = 0.68617564\n",
      "Iteration 39, loss = 0.68600936\n",
      "Iteration 40, loss = 0.68584283\n",
      "Iteration 41, loss = 0.68567606\n",
      "Iteration 42, loss = 0.68550909\n",
      "Iteration 43, loss = 0.68534192\n",
      "Iteration 44, loss = 0.68517460\n",
      "Iteration 45, loss = 0.68500711\n",
      "Iteration 46, loss = 0.68483950\n",
      "Iteration 47, loss = 0.68467176\n",
      "Iteration 48, loss = 0.68450391\n",
      "Iteration 49, loss = 0.68433596\n",
      "Iteration 50, loss = 0.68416792\n",
      "Iteration 51, loss = 0.68399980\n",
      "Iteration 52, loss = 0.68383161\n",
      "Iteration 53, loss = 0.68366334\n",
      "Iteration 54, loss = 0.68349501\n",
      "Iteration 55, loss = 0.68332662\n",
      "Iteration 56, loss = 0.68315818\n",
      "Iteration 57, loss = 0.68298969\n",
      "Iteration 58, loss = 0.68282115\n",
      "Iteration 59, loss = 0.68265257\n",
      "Iteration 60, loss = 0.68248394\n",
      "Iteration 61, loss = 0.68231527\n",
      "Iteration 62, loss = 0.68214656\n",
      "Iteration 63, loss = 0.68197781\n",
      "Iteration 64, loss = 0.68180902\n",
      "Iteration 65, loss = 0.68164020\n",
      "Iteration 66, loss = 0.68147134\n",
      "Iteration 67, loss = 0.68130244\n",
      "Iteration 68, loss = 0.68113351\n",
      "Iteration 69, loss = 0.68096454\n",
      "Iteration 70, loss = 0.68079553\n",
      "Iteration 71, loss = 0.68062648\n",
      "Iteration 72, loss = 0.68045740\n",
      "Iteration 73, loss = 0.68028828\n",
      "Iteration 74, loss = 0.68011911\n",
      "Iteration 75, loss = 0.67994991\n",
      "Iteration 76, loss = 0.67978066\n",
      "Iteration 77, loss = 0.67961137\n",
      "Iteration 78, loss = 0.67944204\n",
      "Iteration 79, loss = 0.67927266\n",
      "Iteration 80, loss = 0.67910324\n",
      "Iteration 81, loss = 0.67893377\n",
      "Iteration 82, loss = 0.67876425\n",
      "Iteration 83, loss = 0.67859468\n",
      "Iteration 84, loss = 0.67842506\n",
      "Iteration 85, loss = 0.67825539\n",
      "Iteration 86, loss = 0.67808566\n",
      "Iteration 87, loss = 0.67791588\n",
      "Iteration 88, loss = 0.67774604\n",
      "Iteration 89, loss = 0.67757615\n",
      "Iteration 90, loss = 0.67740619\n",
      "Iteration 91, loss = 0.67723617\n",
      "Iteration 92, loss = 0.67706609\n",
      "Iteration 93, loss = 0.67689595\n",
      "Iteration 94, loss = 0.67672574\n",
      "Iteration 95, loss = 0.67655547\n",
      "Iteration 96, loss = 0.67638512\n",
      "Iteration 97, loss = 0.67621471\n",
      "Iteration 98, loss = 0.67604422\n",
      "Iteration 99, loss = 0.67587366\n",
      "Iteration 100, loss = 0.67570303\n",
      "Iteration 101, loss = 0.67553232\n",
      "Iteration 102, loss = 0.67536154\n",
      "Iteration 103, loss = 0.67519067\n",
      "Iteration 104, loss = 0.67501973\n",
      "Iteration 105, loss = 0.67484870\n",
      "Iteration 106, loss = 0.67467759\n",
      "Iteration 107, loss = 0.67450639\n",
      "Iteration 108, loss = 0.67433511\n",
      "Iteration 109, loss = 0.67416374\n",
      "Iteration 110, loss = 0.67399228\n",
      "Iteration 111, loss = 0.67382073\n",
      "Iteration 112, loss = 0.67364908\n",
      "Iteration 113, loss = 0.67347735\n",
      "Iteration 114, loss = 0.67330551\n",
      "Iteration 115, loss = 0.67313358\n",
      "Iteration 116, loss = 0.67296155\n",
      "Iteration 117, loss = 0.67278942\n",
      "Iteration 118, loss = 0.67261719\n",
      "Iteration 119, loss = 0.67244486\n",
      "Iteration 120, loss = 0.67227242\n",
      "Iteration 121, loss = 0.67209987\n",
      "Iteration 122, loss = 0.67192722\n",
      "Iteration 123, loss = 0.67175446\n",
      "Iteration 124, loss = 0.67158159\n",
      "Iteration 125, loss = 0.67140860\n",
      "Iteration 126, loss = 0.67123550\n",
      "Iteration 127, loss = 0.67106229\n",
      "Iteration 128, loss = 0.67088896\n",
      "Iteration 129, loss = 0.67071552\n",
      "Iteration 130, loss = 0.67054195\n",
      "Iteration 131, loss = 0.67036826\n",
      "Iteration 132, loss = 0.67019445\n",
      "Iteration 133, loss = 0.67002052\n",
      "Iteration 134, loss = 0.66984646\n",
      "Iteration 135, loss = 0.66967227\n",
      "Iteration 136, loss = 0.66949796\n",
      "Iteration 137, loss = 0.66932351\n",
      "Iteration 138, loss = 0.66914894\n",
      "Iteration 139, loss = 0.66897423\n",
      "Iteration 140, loss = 0.66879939\n",
      "Iteration 141, loss = 0.66862441\n",
      "Iteration 142, loss = 0.66844930\n",
      "Iteration 143, loss = 0.66827405\n",
      "Iteration 144, loss = 0.66809866\n",
      "Iteration 145, loss = 0.66792313\n",
      "Iteration 146, loss = 0.66774745\n",
      "Iteration 147, loss = 0.66757163\n",
      "Iteration 148, loss = 0.66739567\n",
      "Iteration 149, loss = 0.66721956\n",
      "Iteration 150, loss = 0.66704330\n",
      "Iteration 151, loss = 0.66686689\n",
      "Iteration 152, loss = 0.66669033\n",
      "Iteration 153, loss = 0.66651362\n",
      "Iteration 154, loss = 0.66633675\n",
      "Iteration 155, loss = 0.66615973\n",
      "Iteration 156, loss = 0.66598256\n",
      "Iteration 157, loss = 0.66580522\n",
      "Iteration 158, loss = 0.66562773\n",
      "Iteration 159, loss = 0.66545007\n",
      "Iteration 160, loss = 0.66527226\n",
      "Iteration 161, loss = 0.66509427\n",
      "Iteration 162, loss = 0.66491613\n",
      "Iteration 163, loss = 0.66473782\n",
      "Iteration 164, loss = 0.66455934\n",
      "Iteration 165, loss = 0.66438069\n",
      "Iteration 166, loss = 0.66420188\n",
      "Iteration 167, loss = 0.66402289\n",
      "Iteration 168, loss = 0.66384372\n",
      "Iteration 169, loss = 0.66366439\n",
      "Iteration 170, loss = 0.66348488\n",
      "Iteration 171, loss = 0.66330519\n",
      "Iteration 172, loss = 0.66312532\n",
      "Iteration 173, loss = 0.66294527\n",
      "Iteration 174, loss = 0.66276504\n",
      "Iteration 175, loss = 0.66258463\n",
      "Iteration 176, loss = 0.66240404\n",
      "Iteration 177, loss = 0.66222326\n",
      "Iteration 178, loss = 0.66204230\n",
      "Iteration 179, loss = 0.66186114\n",
      "Iteration 180, loss = 0.66167980\n",
      "Iteration 181, loss = 0.66149827\n",
      "Iteration 182, loss = 0.66131655\n",
      "Iteration 183, loss = 0.66113463\n",
      "Iteration 184, loss = 0.66095252\n",
      "Iteration 185, loss = 0.66077021\n",
      "Iteration 186, loss = 0.66058771\n",
      "Iteration 187, loss = 0.66040501\n",
      "Iteration 188, loss = 0.66022211\n",
      "Iteration 189, loss = 0.66003901\n",
      "Iteration 190, loss = 0.65985571\n",
      "Iteration 191, loss = 0.65967220\n",
      "Iteration 192, loss = 0.65948849\n",
      "Iteration 193, loss = 0.65930458\n",
      "Iteration 194, loss = 0.65912045\n",
      "Iteration 195, loss = 0.65893612\n",
      "Iteration 196, loss = 0.65875158\n",
      "Iteration 197, loss = 0.65856683\n",
      "Iteration 198, loss = 0.65838187\n",
      "Iteration 199, loss = 0.65819669\n",
      "Iteration 200, loss = 0.65801130\n",
      "Iteration 201, loss = 0.65782569\n",
      "Iteration 202, loss = 0.65763987\n",
      "Iteration 203, loss = 0.65745382\n",
      "Iteration 204, loss = 0.65726756\n",
      "Iteration 205, loss = 0.65708108\n",
      "Iteration 206, loss = 0.65689437\n",
      "Iteration 207, loss = 0.65670744\n",
      "Iteration 208, loss = 0.65652029\n",
      "Iteration 209, loss = 0.65633291\n",
      "Iteration 210, loss = 0.65614530\n",
      "Iteration 211, loss = 0.65595747\n",
      "Iteration 212, loss = 0.65576940\n",
      "Iteration 213, loss = 0.65558111\n",
      "Iteration 214, loss = 0.65539258\n",
      "Iteration 215, loss = 0.65520382\n",
      "Iteration 216, loss = 0.65501483\n",
      "Iteration 217, loss = 0.65482560\n",
      "Iteration 218, loss = 0.65463613\n",
      "Iteration 219, loss = 0.65444643\n",
      "Iteration 220, loss = 0.65425648\n",
      "Iteration 221, loss = 0.65406630\n",
      "Iteration 222, loss = 0.65387587\n",
      "Iteration 223, loss = 0.65368521\n",
      "Iteration 224, loss = 0.65349429\n",
      "Iteration 225, loss = 0.65330314\n",
      "Iteration 226, loss = 0.65311173\n",
      "Iteration 227, loss = 0.65292008\n",
      "Iteration 228, loss = 0.65272818\n",
      "Iteration 229, loss = 0.65253604\n",
      "Iteration 230, loss = 0.65234364\n",
      "Iteration 231, loss = 0.65215098\n",
      "Iteration 232, loss = 0.65195808\n",
      "Iteration 233, loss = 0.65176492\n",
      "Iteration 234, loss = 0.65157150\n",
      "Iteration 235, loss = 0.65137783\n",
      "Iteration 236, loss = 0.65118390\n",
      "Iteration 237, loss = 0.65098971\n",
      "Iteration 238, loss = 0.65079526\n",
      "Iteration 239, loss = 0.65060055\n",
      "Iteration 240, loss = 0.65040558\n",
      "Iteration 241, loss = 0.65021034\n",
      "Iteration 242, loss = 0.65001484\n",
      "Iteration 243, loss = 0.64981908\n",
      "Iteration 244, loss = 0.64962304\n",
      "Iteration 245, loss = 0.64942674\n",
      "Iteration 246, loss = 0.64923017\n",
      "Iteration 247, loss = 0.64903333\n",
      "Iteration 248, loss = 0.64883622\n",
      "Iteration 249, loss = 0.64863883\n",
      "Iteration 250, loss = 0.64844117\n",
      "Iteration 251, loss = 0.64824324\n",
      "Iteration 252, loss = 0.64804503\n",
      "Iteration 253, loss = 0.64784654\n",
      "Iteration 254, loss = 0.64764778\n",
      "Iteration 255, loss = 0.64744873\n",
      "Iteration 256, loss = 0.64724941\n",
      "Iteration 257, loss = 0.64704981\n",
      "Iteration 258, loss = 0.64684992\n",
      "Iteration 259, loss = 0.64664975\n",
      "Iteration 260, loss = 0.64644929\n",
      "Iteration 261, loss = 0.64624855\n",
      "Iteration 262, loss = 0.64604752\n",
      "Iteration 263, loss = 0.64584621\n",
      "Iteration 264, loss = 0.64564461\n",
      "Iteration 265, loss = 0.64544271\n",
      "Iteration 266, loss = 0.64524053\n",
      "Iteration 267, loss = 0.64503805\n",
      "Iteration 268, loss = 0.64483528\n",
      "Iteration 269, loss = 0.64463222\n",
      "Iteration 270, loss = 0.64442887\n",
      "Iteration 271, loss = 0.64422521\n",
      "Iteration 272, loss = 0.64402126\n",
      "Iteration 273, loss = 0.64381701\n",
      "Iteration 274, loss = 0.64361246\n",
      "Iteration 275, loss = 0.64340761\n",
      "Iteration 276, loss = 0.64320246\n",
      "Iteration 277, loss = 0.64299701\n",
      "Iteration 278, loss = 0.64279126\n",
      "Iteration 279, loss = 0.64258520\n",
      "Iteration 280, loss = 0.64237883\n",
      "Iteration 281, loss = 0.64217216\n",
      "Iteration 282, loss = 0.64196518\n",
      "Iteration 283, loss = 0.64175790\n",
      "Iteration 284, loss = 0.64155030\n",
      "Iteration 285, loss = 0.64134240\n",
      "Iteration 286, loss = 0.64113418\n",
      "Iteration 287, loss = 0.64092565\n",
      "Iteration 288, loss = 0.64071681\n",
      "Iteration 289, loss = 0.64050765\n",
      "Iteration 290, loss = 0.64029818\n",
      "Iteration 291, loss = 0.64008839\n",
      "Iteration 292, loss = 0.63987829\n",
      "Iteration 293, loss = 0.63966787\n",
      "Iteration 294, loss = 0.63945713\n",
      "Iteration 295, loss = 0.63924607\n",
      "Iteration 296, loss = 0.63903468\n",
      "Iteration 297, loss = 0.63882298\n",
      "Iteration 298, loss = 0.63861095\n",
      "Iteration 299, loss = 0.63839860\n",
      "Iteration 300, loss = 0.63818593\n",
      "Iteration 1, loss = 0.72354826\n",
      "Iteration 2, loss = 0.69294892\n",
      "Iteration 3, loss = 0.69750807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.69530316\n",
      "Iteration 5, loss = 0.69295849\n",
      "Iteration 6, loss = 0.69139791\n",
      "Iteration 7, loss = 0.69139571\n",
      "Iteration 8, loss = 0.69136008\n",
      "Iteration 9, loss = 0.69106927\n",
      "Iteration 10, loss = 0.69084933\n",
      "Iteration 11, loss = 0.69072977\n",
      "Iteration 12, loss = 0.69061575\n",
      "Iteration 13, loss = 0.69047900\n",
      "Iteration 14, loss = 0.69033666\n",
      "Iteration 15, loss = 0.69019687\n",
      "Iteration 16, loss = 0.69005572\n",
      "Iteration 17, loss = 0.68991042\n",
      "Iteration 18, loss = 0.68976172\n",
      "Iteration 19, loss = 0.68961067\n",
      "Iteration 20, loss = 0.68945762\n",
      "Iteration 21, loss = 0.68930322\n",
      "Iteration 22, loss = 0.68914686\n",
      "Iteration 23, loss = 0.68898878\n",
      "Iteration 24, loss = 0.68882917\n",
      "Iteration 25, loss = 0.68866817\n",
      "Iteration 26, loss = 0.68850593\n",
      "Iteration 27, loss = 0.68834257\n",
      "Iteration 28, loss = 0.68817820\n",
      "Iteration 29, loss = 0.68801294\n",
      "Iteration 30, loss = 0.68784688\n",
      "Iteration 31, loss = 0.68768009\n",
      "Iteration 32, loss = 0.68751265\n",
      "Iteration 33, loss = 0.68734463\n",
      "Iteration 34, loss = 0.68717610\n",
      "Iteration 35, loss = 0.68700709\n",
      "Iteration 36, loss = 0.68683767\n",
      "Iteration 37, loss = 0.68666788\n",
      "Iteration 38, loss = 0.68649774\n",
      "Iteration 39, loss = 0.68632731\n",
      "Iteration 40, loss = 0.68615661\n",
      "Iteration 41, loss = 0.68598567\n",
      "Iteration 42, loss = 0.68581451\n",
      "Iteration 43, loss = 0.68564315\n",
      "Iteration 44, loss = 0.68547162\n",
      "Iteration 45, loss = 0.68529993\n",
      "Iteration 46, loss = 0.68512810\n",
      "Iteration 47, loss = 0.68495615\n",
      "Iteration 48, loss = 0.68478407\n",
      "Iteration 49, loss = 0.68461190\n",
      "Iteration 50, loss = 0.68443963\n",
      "Iteration 51, loss = 0.68426727\n",
      "Iteration 52, loss = 0.68409483\n",
      "Iteration 53, loss = 0.68392232\n",
      "Iteration 54, loss = 0.68374975\n",
      "Iteration 55, loss = 0.68357711\n",
      "Iteration 56, loss = 0.68340441\n",
      "Iteration 57, loss = 0.68323167\n",
      "Iteration 58, loss = 0.68305887\n",
      "Iteration 59, loss = 0.68288602\n",
      "Iteration 60, loss = 0.68271313\n",
      "Iteration 61, loss = 0.68254019\n",
      "Iteration 62, loss = 0.68236721\n",
      "Iteration 63, loss = 0.68219419\n",
      "Iteration 64, loss = 0.68202113\n",
      "Iteration 65, loss = 0.68184803\n",
      "Iteration 66, loss = 0.68167489\n",
      "Iteration 67, loss = 0.68150171\n",
      "Iteration 68, loss = 0.68132849\n",
      "Iteration 69, loss = 0.68115523\n",
      "Iteration 70, loss = 0.68098193\n",
      "Iteration 71, loss = 0.68080859\n",
      "Iteration 72, loss = 0.68063521\n",
      "Iteration 73, loss = 0.68046178\n",
      "Iteration 74, loss = 0.68028832\n",
      "Iteration 75, loss = 0.68011480\n",
      "Iteration 76, loss = 0.67994125\n",
      "Iteration 77, loss = 0.67976765\n",
      "Iteration 78, loss = 0.67959400\n",
      "Iteration 79, loss = 0.67942030\n",
      "Iteration 80, loss = 0.67924655\n",
      "Iteration 81, loss = 0.67907275\n",
      "Iteration 82, loss = 0.67889890\n",
      "Iteration 83, loss = 0.67872500\n",
      "Iteration 84, loss = 0.67855104\n",
      "Iteration 85, loss = 0.67837702\n",
      "Iteration 86, loss = 0.67820295\n",
      "Iteration 87, loss = 0.67802882\n",
      "Iteration 88, loss = 0.67785462\n",
      "Iteration 89, loss = 0.67768036\n",
      "Iteration 90, loss = 0.67750604\n",
      "Iteration 91, loss = 0.67733166\n",
      "Iteration 92, loss = 0.67715720\n",
      "Iteration 93, loss = 0.67698268\n",
      "Iteration 94, loss = 0.67680809\n",
      "Iteration 95, loss = 0.67663342\n",
      "Iteration 96, loss = 0.67645868\n",
      "Iteration 97, loss = 0.67628387\n",
      "Iteration 98, loss = 0.67610898\n",
      "Iteration 99, loss = 0.67593401\n",
      "Iteration 100, loss = 0.67575896\n",
      "Iteration 101, loss = 0.67558383\n",
      "Iteration 102, loss = 0.67540862\n",
      "Iteration 103, loss = 0.67523332\n",
      "Iteration 104, loss = 0.67505794\n",
      "Iteration 105, loss = 0.67488247\n",
      "Iteration 106, loss = 0.67470690\n",
      "Iteration 107, loss = 0.67453125\n",
      "Iteration 108, loss = 0.67435551\n",
      "Iteration 109, loss = 0.67417966\n",
      "Iteration 110, loss = 0.67400373\n",
      "Iteration 111, loss = 0.67382769\n",
      "Iteration 112, loss = 0.67365156\n",
      "Iteration 113, loss = 0.67347533\n",
      "Iteration 114, loss = 0.67329899\n",
      "Iteration 115, loss = 0.67312255\n",
      "Iteration 116, loss = 0.67294600\n",
      "Iteration 117, loss = 0.67276934\n",
      "Iteration 118, loss = 0.67259258\n",
      "Iteration 119, loss = 0.67241571\n",
      "Iteration 120, loss = 0.67223872\n",
      "Iteration 121, loss = 0.67206162\n",
      "Iteration 122, loss = 0.67188441\n",
      "Iteration 123, loss = 0.67170707\n",
      "Iteration 124, loss = 0.67152962\n",
      "Iteration 125, loss = 0.67135205\n",
      "Iteration 126, loss = 0.67117436\n",
      "Iteration 127, loss = 0.67099654\n",
      "Iteration 128, loss = 0.67081860\n",
      "Iteration 129, loss = 0.67064053\n",
      "Iteration 130, loss = 0.67046234\n",
      "Iteration 131, loss = 0.67028401\n",
      "Iteration 132, loss = 0.67010556\n",
      "Iteration 133, loss = 0.66992697\n",
      "Iteration 134, loss = 0.66974825\n",
      "Iteration 135, loss = 0.66956939\n",
      "Iteration 136, loss = 0.66939039\n",
      "Iteration 137, loss = 0.66921126\n",
      "Iteration 138, loss = 0.66903198\n",
      "Iteration 139, loss = 0.66885257\n",
      "Iteration 140, loss = 0.66867301\n",
      "Iteration 141, loss = 0.66849330\n",
      "Iteration 142, loss = 0.66831345\n",
      "Iteration 143, loss = 0.66813345\n",
      "Iteration 144, loss = 0.66795330\n",
      "Iteration 145, loss = 0.66777300\n",
      "Iteration 146, loss = 0.66759254\n",
      "Iteration 147, loss = 0.66741194\n",
      "Iteration 148, loss = 0.66723117\n",
      "Iteration 149, loss = 0.66705025\n",
      "Iteration 150, loss = 0.66686917\n",
      "Iteration 151, loss = 0.66668793\n",
      "Iteration 152, loss = 0.66650653\n",
      "Iteration 153, loss = 0.66632497\n",
      "Iteration 154, loss = 0.66614324\n",
      "Iteration 155, loss = 0.66596134\n",
      "Iteration 156, loss = 0.66577928\n",
      "Iteration 157, loss = 0.66559704\n",
      "Iteration 158, loss = 0.66541464\n",
      "Iteration 159, loss = 0.66523207\n",
      "Iteration 160, loss = 0.66504932\n",
      "Iteration 161, loss = 0.66486639\n",
      "Iteration 162, loss = 0.66468329\n",
      "Iteration 163, loss = 0.66450001\n",
      "Iteration 164, loss = 0.66431656\n",
      "Iteration 165, loss = 0.66413292\n",
      "Iteration 166, loss = 0.66394910\n",
      "Iteration 167, loss = 0.66376509\n",
      "Iteration 168, loss = 0.66358090\n",
      "Iteration 169, loss = 0.66339652\n",
      "Iteration 170, loss = 0.66321196\n",
      "Iteration 171, loss = 0.66302720\n",
      "Iteration 172, loss = 0.66284225\n",
      "Iteration 173, loss = 0.66265712\n",
      "Iteration 174, loss = 0.66247178\n",
      "Iteration 175, loss = 0.66228625\n",
      "Iteration 176, loss = 0.66210053\n",
      "Iteration 177, loss = 0.66191461\n",
      "Iteration 178, loss = 0.66172848\n",
      "Iteration 179, loss = 0.66154216\n",
      "Iteration 180, loss = 0.66135563\n",
      "Iteration 181, loss = 0.66116890\n",
      "Iteration 182, loss = 0.66098196\n",
      "Iteration 183, loss = 0.66079482\n",
      "Iteration 184, loss = 0.66060747\n",
      "Iteration 185, loss = 0.66041991\n",
      "Iteration 186, loss = 0.66023214\n",
      "Iteration 187, loss = 0.66004415\n",
      "Iteration 188, loss = 0.65985595\n",
      "Iteration 189, loss = 0.65966754\n",
      "Iteration 190, loss = 0.65947891\n",
      "Iteration 191, loss = 0.65929006\n",
      "Iteration 192, loss = 0.65910100\n",
      "Iteration 193, loss = 0.65891171\n",
      "Iteration 194, loss = 0.65872220\n",
      "Iteration 195, loss = 0.65853247\n",
      "Iteration 196, loss = 0.65834251\n",
      "Iteration 197, loss = 0.65815232\n",
      "Iteration 198, loss = 0.65796191\n",
      "Iteration 199, loss = 0.65777127\n",
      "Iteration 200, loss = 0.65758040\n",
      "Iteration 201, loss = 0.65738930\n",
      "Iteration 202, loss = 0.65719796\n",
      "Iteration 203, loss = 0.65700639\n",
      "Iteration 204, loss = 0.65681459\n",
      "Iteration 205, loss = 0.65662254\n",
      "Iteration 206, loss = 0.65643026\n",
      "Iteration 207, loss = 0.65623774\n",
      "Iteration 208, loss = 0.65604498\n",
      "Iteration 209, loss = 0.65585197\n",
      "Iteration 210, loss = 0.65565873\n",
      "Iteration 211, loss = 0.65546523\n",
      "Iteration 212, loss = 0.65527149\n",
      "Iteration 213, loss = 0.65507751\n",
      "Iteration 214, loss = 0.65488327\n",
      "Iteration 215, loss = 0.65468879\n",
      "Iteration 216, loss = 0.65449405\n",
      "Iteration 217, loss = 0.65429906\n",
      "Iteration 218, loss = 0.65410381\n",
      "Iteration 219, loss = 0.65390831\n",
      "Iteration 220, loss = 0.65371255\n",
      "Iteration 221, loss = 0.65351654\n",
      "Iteration 222, loss = 0.65332026\n",
      "Iteration 223, loss = 0.65312373\n",
      "Iteration 224, loss = 0.65292693\n",
      "Iteration 225, loss = 0.65272987\n",
      "Iteration 226, loss = 0.65253255\n",
      "Iteration 227, loss = 0.65233496\n",
      "Iteration 228, loss = 0.65213710\n",
      "Iteration 229, loss = 0.65193897\n",
      "Iteration 230, loss = 0.65174057\n",
      "Iteration 231, loss = 0.65154191\n",
      "Iteration 232, loss = 0.65134297\n",
      "Iteration 233, loss = 0.65114376\n",
      "Iteration 234, loss = 0.65094427\n",
      "Iteration 235, loss = 0.65074450\n",
      "Iteration 236, loss = 0.65054446\n",
      "Iteration 237, loss = 0.65034415\n",
      "Iteration 238, loss = 0.65014355\n",
      "Iteration 239, loss = 0.64994267\n",
      "Iteration 240, loss = 0.64974151\n",
      "Iteration 241, loss = 0.64954006\n",
      "Iteration 242, loss = 0.64933833\n",
      "Iteration 243, loss = 0.64913632\n",
      "Iteration 244, loss = 0.64893401\n",
      "Iteration 245, loss = 0.64873142\n",
      "Iteration 246, loss = 0.64852854\n",
      "Iteration 247, loss = 0.64832537\n",
      "Iteration 248, loss = 0.64812191\n",
      "Iteration 249, loss = 0.64791815\n",
      "Iteration 250, loss = 0.64771410\n",
      "Iteration 251, loss = 0.64750976\n",
      "Iteration 252, loss = 0.64730512\n",
      "Iteration 253, loss = 0.64710018\n",
      "Iteration 254, loss = 0.64689494\n",
      "Iteration 255, loss = 0.64668940\n",
      "Iteration 256, loss = 0.64648355\n",
      "Iteration 257, loss = 0.64627741\n",
      "Iteration 258, loss = 0.64607096\n",
      "Iteration 259, loss = 0.64586421\n",
      "Iteration 260, loss = 0.64565715\n",
      "Iteration 261, loss = 0.64544978\n",
      "Iteration 262, loss = 0.64524211\n",
      "Iteration 263, loss = 0.64503413\n",
      "Iteration 264, loss = 0.64482583\n",
      "Iteration 265, loss = 0.64461722\n",
      "Iteration 266, loss = 0.64440830\n",
      "Iteration 267, loss = 0.64419907\n",
      "Iteration 268, loss = 0.64398952\n",
      "Iteration 269, loss = 0.64377965\n",
      "Iteration 270, loss = 0.64356947\n",
      "Iteration 271, loss = 0.64335896\n",
      "Iteration 272, loss = 0.64314814\n",
      "Iteration 273, loss = 0.64293699\n",
      "Iteration 274, loss = 0.64272553\n",
      "Iteration 275, loss = 0.64251374\n",
      "Iteration 276, loss = 0.64230162\n",
      "Iteration 277, loss = 0.64208918\n",
      "Iteration 278, loss = 0.64187641\n",
      "Iteration 279, loss = 0.64166332\n",
      "Iteration 280, loss = 0.64144990\n",
      "Iteration 281, loss = 0.64123614\n",
      "Iteration 282, loss = 0.64102206\n",
      "Iteration 283, loss = 0.64080764\n",
      "Iteration 284, loss = 0.64059289\n",
      "Iteration 285, loss = 0.64037780\n",
      "Iteration 286, loss = 0.64016238\n",
      "Iteration 287, loss = 0.63994663\n",
      "Iteration 288, loss = 0.63973053\n",
      "Iteration 289, loss = 0.63951410\n",
      "Iteration 290, loss = 0.63929733\n",
      "Iteration 291, loss = 0.63908021\n",
      "Iteration 292, loss = 0.63886276\n",
      "Iteration 293, loss = 0.63864496\n",
      "Iteration 294, loss = 0.63842682\n",
      "Iteration 295, loss = 0.63820833\n",
      "Iteration 296, loss = 0.63798950\n",
      "Iteration 297, loss = 0.63777032\n",
      "Iteration 298, loss = 0.63755079\n",
      "Iteration 299, loss = 0.63733091\n",
      "Iteration 300, loss = 0.63711068\n",
      "Iteration 1, loss = 0.72546705\n",
      "Iteration 2, loss = 1.50668005\n",
      "Iteration 3, loss = 0.99146668\n",
      "Iteration 4, loss = 0.69305781\n",
      "Iteration 5, loss = 0.69410888\n",
      "Iteration 6, loss = 0.69409722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.69408680\n",
      "Iteration 8, loss = 0.69407734\n",
      "Iteration 9, loss = 0.69406863\n",
      "Iteration 10, loss = 0.69406055\n",
      "Iteration 11, loss = 0.69405297\n",
      "Iteration 12, loss = 0.69404583\n",
      "Iteration 13, loss = 0.69403906\n",
      "Iteration 14, loss = 0.69403260\n",
      "Iteration 15, loss = 0.69402640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72123287\n",
      "Iteration 2, loss = 1.52599024\n",
      "Iteration 3, loss = 0.99127470\n",
      "Iteration 4, loss = 0.69469597\n",
      "Iteration 5, loss = 0.69477109\n",
      "Iteration 6, loss = 0.69475553\n",
      "Iteration 7, loss = 0.69474147\n",
      "Iteration 8, loss = 0.69472855\n",
      "Iteration 9, loss = 0.69471654\n",
      "Iteration 10, loss = 0.69470526\n",
      "Iteration 11, loss = 0.69469459\n",
      "Iteration 12, loss = 0.69468442\n",
      "Iteration 13, loss = 0.69467467\n",
      "Iteration 14, loss = 0.69466528\n",
      "Iteration 15, loss = 0.69465619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72354826\n",
      "Iteration 2, loss = 1.51583276\n",
      "Iteration 3, loss = 0.99074834\n",
      "Iteration 4, loss = 0.69416778\n",
      "Iteration 5, loss = 0.69443997\n",
      "Iteration 6, loss = 0.69442639\n",
      "Iteration 7, loss = 0.69441418\n",
      "Iteration 8, loss = 0.69440302\n",
      "Iteration 9, loss = 0.69439270\n",
      "Iteration 10, loss = 0.69438306\n",
      "Iteration 11, loss = 0.69437398\n",
      "Iteration 12, loss = 0.69436538\n",
      "Iteration 13, loss = 0.69435716\n",
      "Iteration 14, loss = 0.69434929\n",
      "Iteration 15, loss = 0.69434170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72388585\n",
      "Iteration 2, loss = 1.09912934\n",
      "Iteration 3, loss = 0.69733952\n",
      "Iteration 4, loss = 0.69723675\n",
      "Iteration 5, loss = 0.69763508\n",
      "Iteration 6, loss = 0.69696077\n",
      "Iteration 7, loss = 0.69681366\n",
      "Iteration 8, loss = 0.69666603\n",
      "Iteration 9, loss = 0.69651880\n",
      "Iteration 10, loss = 0.69637275\n",
      "Iteration 11, loss = 0.69622855\n",
      "Iteration 12, loss = 0.69608679\n",
      "Iteration 13, loss = 0.69594794\n",
      "Iteration 14, loss = 0.69581241\n",
      "Iteration 15, loss = 0.69568052\n",
      "Iteration 16, loss = 0.69555255\n",
      "Iteration 17, loss = 0.69542868\n",
      "Iteration 18, loss = 0.69530908\n",
      "Iteration 19, loss = 0.69519385\n",
      "Iteration 20, loss = 0.69508304\n",
      "Iteration 21, loss = 0.69497669\n",
      "Iteration 22, loss = 0.69487479\n",
      "Iteration 23, loss = 0.69477731\n",
      "Iteration 24, loss = 0.69468420\n",
      "Iteration 25, loss = 0.69459539\n",
      "Iteration 26, loss = 0.69451079\n",
      "Iteration 27, loss = 0.69443030\n",
      "Iteration 28, loss = 0.69435381\n",
      "Iteration 29, loss = 0.69428120\n",
      "Iteration 30, loss = 0.69421233\n",
      "Iteration 31, loss = 0.69414709\n",
      "Iteration 32, loss = 0.69408534\n",
      "Iteration 33, loss = 0.69402694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72598529\n",
      "Iteration 2, loss = 1.15245787\n",
      "Iteration 3, loss = 0.69626452\n",
      "Iteration 4, loss = 0.69614887\n",
      "Iteration 5, loss = 0.69603215\n",
      "Iteration 6, loss = 0.69591516\n",
      "Iteration 7, loss = 0.69579858\n",
      "Iteration 8, loss = 0.69568304\n",
      "Iteration 9, loss = 0.69556905\n",
      "Iteration 10, loss = 0.69545705\n",
      "Iteration 11, loss = 0.69534742\n",
      "Iteration 12, loss = 0.69524047\n",
      "Iteration 13, loss = 0.69513644\n",
      "Iteration 14, loss = 0.69503554\n",
      "Iteration 15, loss = 0.69493792\n",
      "Iteration 16, loss = 0.69484369\n",
      "Iteration 17, loss = 0.69475293\n",
      "Iteration 18, loss = 0.69466568\n",
      "Iteration 19, loss = 0.69458196\n",
      "Iteration 20, loss = 0.69450177\n",
      "Iteration 21, loss = 0.69442507\n",
      "Iteration 22, loss = 0.69435183\n",
      "Iteration 23, loss = 0.69428198\n",
      "Iteration 24, loss = 0.69421545\n",
      "Iteration 25, loss = 0.69415216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72621091\n",
      "Iteration 2, loss = 1.12233239\n",
      "Iteration 3, loss = 0.69682695\n",
      "Iteration 4, loss = 0.69670041\n",
      "Iteration 5, loss = 0.69657132\n",
      "Iteration 6, loss = 0.69644074\n",
      "Iteration 7, loss = 0.69630961\n",
      "Iteration 8, loss = 0.69617874\n",
      "Iteration 9, loss = 0.69604883\n",
      "Iteration 10, loss = 0.69592050\n",
      "Iteration 11, loss = 0.69579428\n",
      "Iteration 12, loss = 0.69567059\n",
      "Iteration 13, loss = 0.69554982\n",
      "Iteration 14, loss = 0.69543225\n",
      "Iteration 15, loss = 0.69531812\n",
      "Iteration 16, loss = 0.69520763\n",
      "Iteration 17, loss = 0.69510091\n",
      "Iteration 18, loss = 0.69499806\n",
      "Iteration 19, loss = 0.69489914\n",
      "Iteration 20, loss = 0.69480417\n",
      "Iteration 21, loss = 0.69471316\n",
      "Iteration 22, loss = 0.69462608\n",
      "Iteration 23, loss = 0.69454288\n",
      "Iteration 24, loss = 0.69446352\n",
      "Iteration 25, loss = 0.69438790\n",
      "Iteration 26, loss = 0.69431594\n",
      "Iteration 27, loss = 0.69424755\n",
      "Iteration 28, loss = 0.69418261\n",
      "Iteration 29, loss = 0.69412102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72388585\n",
      "Iteration 2, loss = 17.91045610\n",
      "Iteration 3, loss = 7.48248489\n",
      "Iteration 4, loss = 0.69741745\n",
      "Iteration 5, loss = 0.69712062\n",
      "Iteration 6, loss = 0.69686459\n",
      "Iteration 7, loss = 0.69663861\n",
      "Iteration 8, loss = 0.69643592\n",
      "Iteration 9, loss = 0.69625193\n",
      "Iteration 10, loss = 0.69608340\n",
      "Iteration 11, loss = 0.69592789\n",
      "Iteration 12, loss = 0.69578355\n",
      "Iteration 13, loss = 0.69564893\n",
      "Iteration 14, loss = 0.69552287\n",
      "Iteration 15, loss = 0.69540441\n",
      "Iteration 16, loss = 0.69529278\n",
      "Iteration 17, loss = 0.69518732\n",
      "Iteration 18, loss = 0.69508747\n",
      "Iteration 19, loss = 0.69499277\n",
      "Iteration 20, loss = 0.69490281\n",
      "Iteration 21, loss = 0.69481724\n",
      "Iteration 22, loss = 0.69473575\n",
      "Iteration 23, loss = 0.69465807\n",
      "Iteration 24, loss = 0.69458081\n",
      "Iteration 25, loss = 5.96301636\n",
      "Iteration 26, loss = 0.69447487\n",
      "Iteration 27, loss = 2.22903167\n",
      "Iteration 28, loss = 5.76268303\n",
      "Iteration 29, loss = 2.57523490\n",
      "Iteration 30, loss = 1.65025673\n",
      "Iteration 31, loss = 1.70306970\n",
      "Iteration 32, loss = 1.02351237\n",
      "Iteration 33, loss = 0.69474608\n",
      "Iteration 34, loss = 0.69482759\n",
      "Iteration 35, loss = 0.69490028\n",
      "Iteration 36, loss = 0.69496427\n",
      "Iteration 37, loss = 0.69501982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72598529\n",
      "Iteration 2, loss = 17.68393741\n",
      "Iteration 3, loss = 7.74299964\n",
      "Iteration 4, loss = 0.69630060\n",
      "Iteration 5, loss = 0.69605045\n",
      "Iteration 6, loss = 0.69583725\n",
      "Iteration 7, loss = 0.69565123\n",
      "Iteration 8, loss = 0.69548623\n",
      "Iteration 9, loss = 0.69533809\n",
      "Iteration 10, loss = 0.69520382\n",
      "Iteration 11, loss = 0.69508120\n",
      "Iteration 12, loss = 0.69496854\n",
      "Iteration 13, loss = 0.69486449\n",
      "Iteration 14, loss = 0.69476800\n",
      "Iteration 15, loss = 0.69467817\n",
      "Iteration 16, loss = 0.69459429\n",
      "Iteration 17, loss = 0.69451575\n",
      "Iteration 18, loss = 0.69444204\n",
      "Iteration 19, loss = 0.69437272\n",
      "Iteration 20, loss = 0.69430741\n",
      "Iteration 21, loss = 0.69424579\n",
      "Iteration 22, loss = 0.69418755\n",
      "Iteration 23, loss = 0.69413247\n",
      "Iteration 24, loss = 0.69407463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72621091\n",
      "Iteration 2, loss = 17.78778542\n",
      "Iteration 3, loss = 7.60195741\n",
      "Iteration 4, loss = 0.69685967\n",
      "Iteration 5, loss = 0.69658642\n",
      "Iteration 6, loss = 0.69635206\n",
      "Iteration 7, loss = 0.69614633\n",
      "Iteration 8, loss = 0.69596277\n",
      "Iteration 9, loss = 0.69579701\n",
      "Iteration 10, loss = 0.69564591\n",
      "Iteration 11, loss = 0.69550717\n",
      "Iteration 12, loss = 0.69537901\n",
      "Iteration 13, loss = 0.69526002\n",
      "Iteration 14, loss = 0.69514909\n",
      "Iteration 15, loss = 0.69504531\n",
      "Iteration 16, loss = 0.69494793\n",
      "Iteration 17, loss = 0.69485630\n",
      "Iteration 18, loss = 0.69476990\n",
      "Iteration 19, loss = 0.69468827\n",
      "Iteration 20, loss = 0.69461103\n",
      "Iteration 21, loss = 0.69453782\n",
      "Iteration 22, loss = 0.69446835\n",
      "Iteration 23, loss = 0.69440236\n",
      "Iteration 24, loss = 0.69433962\n",
      "Iteration 25, loss = 0.69413459\n",
      "Iteration 26, loss = 6.01313573\n",
      "Iteration 27, loss = 0.69990566\n",
      "Iteration 28, loss = 5.79165115\n",
      "Iteration 29, loss = 0.72810670\n",
      "Iteration 30, loss = 0.69420484\n",
      "Iteration 31, loss = 0.69416096\n",
      "Iteration 32, loss = 0.69411971\n",
      "Iteration 33, loss = 0.69408083\n",
      "Iteration 34, loss = 0.69404407\n",
      "Iteration 35, loss = 0.69400925\n",
      "Iteration 36, loss = 0.69397618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71967970\n",
      "Iteration 2, loss = 2.63501966\n",
      "Iteration 3, loss = 0.73426796\n",
      "Iteration 4, loss = 0.73338412\n",
      "Iteration 5, loss = 0.72966980\n",
      "Iteration 6, loss = 0.72498057\n",
      "Iteration 7, loss = 0.72055292\n",
      "Iteration 8, loss = 0.71764824\n",
      "Iteration 9, loss = 0.71715367\n",
      "Iteration 10, loss = 0.71659753\n",
      "Iteration 11, loss = 0.71599014\n",
      "Iteration 12, loss = 0.71534097\n",
      "Iteration 13, loss = 0.71465863\n",
      "Iteration 14, loss = 0.71395095\n",
      "Iteration 15, loss = 0.71322497\n",
      "Iteration 16, loss = 0.71248698\n",
      "Iteration 17, loss = 0.71174256\n",
      "Iteration 18, loss = 0.71099665\n",
      "Iteration 19, loss = 0.71025357\n",
      "Iteration 20, loss = 0.70951708\n",
      "Iteration 21, loss = 0.70879040\n",
      "Iteration 22, loss = 0.70807629\n",
      "Iteration 23, loss = 0.70737708\n",
      "Iteration 24, loss = 0.70669469\n",
      "Iteration 25, loss = 0.70603071\n",
      "Iteration 26, loss = 0.70538638\n",
      "Iteration 27, loss = 0.70476268\n",
      "Iteration 28, loss = 0.70416034\n",
      "Iteration 29, loss = 0.70357984\n",
      "Iteration 30, loss = 0.70302150\n",
      "Iteration 31, loss = 0.70248544\n",
      "Iteration 32, loss = 0.70197163\n",
      "Iteration 33, loss = 0.70147994\n",
      "Iteration 34, loss = 0.70101010\n",
      "Iteration 35, loss = 0.70056176\n",
      "Iteration 36, loss = 0.70013448\n",
      "Iteration 37, loss = 0.69972779\n",
      "Iteration 38, loss = 0.69934112\n",
      "Iteration 39, loss = 0.69897388\n",
      "Iteration 40, loss = 0.69862547\n",
      "Iteration 41, loss = 0.69829522\n",
      "Iteration 42, loss = 0.69798247\n",
      "Iteration 43, loss = 0.69768656\n",
      "Iteration 44, loss = 0.69740681\n",
      "Iteration 45, loss = 0.69714254\n",
      "Iteration 46, loss = 0.69689307\n",
      "Iteration 47, loss = 0.69665775\n",
      "Iteration 48, loss = 0.69643591\n",
      "Iteration 49, loss = 0.69622693\n",
      "Iteration 50, loss = 0.69603016\n",
      "Iteration 51, loss = 0.69584501\n",
      "Iteration 52, loss = 0.69567089\n",
      "Iteration 53, loss = 0.69550722\n",
      "Iteration 54, loss = 0.69535346\n",
      "Iteration 55, loss = 0.69520907\n",
      "Iteration 56, loss = 0.69507354\n",
      "Iteration 57, loss = 0.69494639\n",
      "Iteration 58, loss = 0.69482715\n",
      "Iteration 59, loss = 0.69471538\n",
      "Iteration 60, loss = 0.69461063\n",
      "Iteration 61, loss = 0.69451252\n",
      "Iteration 62, loss = 0.69442064\n",
      "Iteration 63, loss = 0.69433464\n",
      "Iteration 64, loss = 0.69425417\n",
      "Iteration 65, loss = 0.69417889\n",
      "Iteration 66, loss = 0.69410848\n",
      "Iteration 67, loss = 0.69404266\n",
      "Iteration 68, loss = 0.69398114\n",
      "Iteration 69, loss = 0.69392365\n",
      "Iteration 70, loss = 0.69386995\n",
      "Iteration 71, loss = 0.69381979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71598122\n",
      "Iteration 2, loss = 2.52789649\n",
      "Iteration 3, loss = 0.73284740\n",
      "Iteration 4, loss = 0.73259281\n",
      "Iteration 5, loss = 0.72916208\n",
      "Iteration 6, loss = 0.72454286\n",
      "Iteration 7, loss = 0.72017166\n",
      "Iteration 8, loss = 0.71669832\n",
      "Iteration 9, loss = 0.71449450\n",
      "Iteration 10, loss = 0.71398242\n",
      "Iteration 11, loss = 0.71343786\n",
      "Iteration 12, loss = 0.71285691\n",
      "Iteration 13, loss = 0.71224719\n",
      "Iteration 14, loss = 0.71161557\n",
      "Iteration 15, loss = 0.71096825\n",
      "Iteration 16, loss = 0.71031075\n",
      "Iteration 17, loss = 0.70964799\n",
      "Iteration 18, loss = 0.70898430\n",
      "Iteration 19, loss = 0.70832347\n",
      "Iteration 20, loss = 0.70766878\n",
      "Iteration 21, loss = 0.70702307\n",
      "Iteration 22, loss = 0.70638875\n",
      "Iteration 23, loss = 0.70576785\n",
      "Iteration 24, loss = 0.70516204\n",
      "Iteration 25, loss = 0.70457272\n",
      "Iteration 26, loss = 0.70400095\n",
      "Iteration 27, loss = 0.70344760\n",
      "Iteration 28, loss = 0.70291327\n",
      "Iteration 29, loss = 0.70239839\n",
      "Iteration 30, loss = 0.70190322\n",
      "Iteration 31, loss = 0.70142786\n",
      "Iteration 32, loss = 0.70097228\n",
      "Iteration 33, loss = 0.70053633\n",
      "Iteration 34, loss = 0.70011979\n",
      "Iteration 35, loss = 0.69972232\n",
      "Iteration 36, loss = 0.69934355\n",
      "Iteration 37, loss = 0.69898302\n",
      "Iteration 38, loss = 0.69864026\n",
      "Iteration 39, loss = 0.69831473\n",
      "Iteration 40, loss = 0.69800587\n",
      "Iteration 41, loss = 0.69771312\n",
      "Iteration 42, loss = 0.69743589\n",
      "Iteration 43, loss = 0.69717357\n",
      "Iteration 44, loss = 0.69692557\n",
      "Iteration 45, loss = 0.69669128\n",
      "Iteration 46, loss = 0.69647011\n",
      "Iteration 47, loss = 0.69626148\n",
      "Iteration 48, loss = 0.69606479\n",
      "Iteration 49, loss = 0.69587948\n",
      "Iteration 50, loss = 0.69570501\n",
      "Iteration 51, loss = 0.69554082\n",
      "Iteration 52, loss = 0.69538640\n",
      "Iteration 53, loss = 0.69524125\n",
      "Iteration 54, loss = 0.69510486\n",
      "Iteration 55, loss = 0.69497679\n",
      "Iteration 56, loss = 0.69485657\n",
      "Iteration 57, loss = 0.69474377\n",
      "Iteration 58, loss = 0.69463798\n",
      "Iteration 59, loss = 0.69453880\n",
      "Iteration 60, loss = 0.69444586\n",
      "Iteration 61, loss = 0.69435879\n",
      "Iteration 62, loss = 0.69427725\n",
      "Iteration 63, loss = 0.69420092\n",
      "Iteration 64, loss = 0.69412949\n",
      "Iteration 65, loss = 0.69406266\n",
      "Iteration 66, loss = 0.69400016\n",
      "Iteration 67, loss = 0.69394172\n",
      "Iteration 68, loss = 0.69388709\n",
      "Iteration 69, loss = 0.69383604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448156\n",
      "Iteration 2, loss = 2.59167640\n",
      "Iteration 3, loss = 0.73372819\n",
      "Iteration 4, loss = 0.73317547\n",
      "Iteration 5, loss = 0.72959942\n",
      "Iteration 6, loss = 0.72494543\n",
      "Iteration 7, loss = 0.72056794\n",
      "Iteration 8, loss = 0.71704660\n",
      "Iteration 9, loss = 0.71581344\n",
      "Iteration 10, loss = 0.71528529\n",
      "Iteration 11, loss = 0.71470930\n",
      "Iteration 12, loss = 0.71409437\n",
      "Iteration 13, loss = 0.71344859\n",
      "Iteration 14, loss = 0.71277930\n",
      "Iteration 15, loss = 0.71209309\n",
      "Iteration 16, loss = 0.71139587\n",
      "Iteration 17, loss = 0.71069287\n",
      "Iteration 18, loss = 0.70998871\n",
      "Iteration 19, loss = 0.70928743\n",
      "Iteration 20, loss = 0.70859256\n",
      "Iteration 21, loss = 0.70790710\n",
      "Iteration 22, loss = 0.70723365\n",
      "Iteration 23, loss = 0.70657436\n",
      "Iteration 24, loss = 0.70593104\n",
      "Iteration 25, loss = 0.70530515\n",
      "Iteration 26, loss = 0.70469787\n",
      "Iteration 27, loss = 0.70411011\n",
      "Iteration 28, loss = 0.70354252\n",
      "Iteration 29, loss = 0.70299557\n",
      "Iteration 30, loss = 0.70246953\n",
      "Iteration 31, loss = 0.70196451\n",
      "Iteration 32, loss = 0.70148049\n",
      "Iteration 33, loss = 0.70101733\n",
      "Iteration 34, loss = 0.70057477\n",
      "Iteration 35, loss = 0.70015248\n",
      "Iteration 36, loss = 0.69975004\n",
      "Iteration 37, loss = 0.69936699\n",
      "Iteration 38, loss = 0.69900281\n",
      "Iteration 39, loss = 0.69865695\n",
      "Iteration 40, loss = 0.69832880\n",
      "Iteration 41, loss = 0.69801777\n",
      "Iteration 42, loss = 0.69772323\n",
      "Iteration 43, loss = 0.69744454\n",
      "Iteration 44, loss = 0.69718107\n",
      "Iteration 45, loss = 0.69693217\n",
      "Iteration 46, loss = 0.69669722\n",
      "Iteration 47, loss = 0.69647558\n",
      "Iteration 48, loss = 0.69626665\n",
      "Iteration 49, loss = 0.69606981\n",
      "Iteration 50, loss = 0.69588448\n",
      "Iteration 51, loss = 0.69571009\n",
      "Iteration 52, loss = 0.69554608\n",
      "Iteration 53, loss = 0.69539191\n",
      "Iteration 54, loss = 0.69524706\n",
      "Iteration 55, loss = 0.69511105\n",
      "Iteration 56, loss = 0.69498338\n",
      "Iteration 57, loss = 0.69486360\n",
      "Iteration 58, loss = 0.69475126\n",
      "Iteration 59, loss = 0.69464595\n",
      "Iteration 60, loss = 0.69454727\n",
      "Iteration 61, loss = 0.69445482\n",
      "Iteration 62, loss = 0.69436826\n",
      "Iteration 63, loss = 0.69428722\n",
      "Iteration 64, loss = 0.69421139\n",
      "Iteration 65, loss = 0.69414045\n",
      "Iteration 66, loss = 0.69407411\n",
      "Iteration 67, loss = 0.69401208\n",
      "Iteration 68, loss = 0.69395410\n",
      "Iteration 69, loss = 0.69389992\n",
      "Iteration 70, loss = 0.69384930\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71967970\n",
      "Iteration 2, loss = 17.90404646\n",
      "Iteration 3, loss = 12.22573571\n",
      "Iteration 4, loss = 0.71842063\n",
      "Iteration 5, loss = 0.71900949\n",
      "Iteration 6, loss = 0.71941181\n",
      "Iteration 7, loss = 0.71966616\n",
      "Iteration 8, loss = 0.71979899\n",
      "Iteration 9, loss = 0.71982980\n",
      "Iteration 10, loss = 0.71977378\n",
      "Iteration 11, loss = 0.71964314\n",
      "Iteration 12, loss = 0.71944801\n",
      "Iteration 13, loss = 0.71919696\n",
      "Iteration 14, loss = 0.71889735\n",
      "Iteration 15, loss = 0.71855556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71598122\n",
      "Iteration 2, loss = 18.13962589\n",
      "Iteration 3, loss = 12.00599192\n",
      "Iteration 4, loss = 0.71557899\n",
      "Iteration 5, loss = 0.71614827\n",
      "Iteration 6, loss = 0.71654646\n",
      "Iteration 7, loss = 0.71680853\n",
      "Iteration 8, loss = 0.71695843\n",
      "Iteration 9, loss = 0.71701386\n",
      "Iteration 10, loss = 0.71698856\n",
      "Iteration 11, loss = 0.71689364\n",
      "Iteration 12, loss = 0.71673833\n",
      "Iteration 13, loss = 0.71653043\n",
      "Iteration 14, loss = 0.71627668\n",
      "Iteration 15, loss = 0.71598295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448156\n",
      "Iteration 2, loss = 18.02183611\n",
      "Iteration 3, loss = 12.13806186\n",
      "Iteration 4, loss = 0.71699891\n",
      "Iteration 5, loss = 0.71757785\n",
      "Iteration 6, loss = 0.71797811\n",
      "Iteration 7, loss = 0.71823642\n",
      "Iteration 8, loss = 0.71837795\n",
      "Iteration 9, loss = 0.71842130\n",
      "Iteration 10, loss = 0.71838089\n",
      "Iteration 11, loss = 0.71826840\n",
      "Iteration 12, loss = 0.71809348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72546705\n",
      "Iteration 2, loss = 0.92687444\n",
      "Iteration 3, loss = 0.69407572\n",
      "Iteration 4, loss = 0.69403373\n",
      "Iteration 5, loss = 0.69399246\n",
      "Iteration 6, loss = 0.69395203\n",
      "Iteration 7, loss = 0.69391257\n",
      "Iteration 8, loss = 0.69387417\n",
      "Iteration 9, loss = 0.69383690\n",
      "Iteration 10, loss = 0.69380084\n",
      "Iteration 11, loss = 0.69376601\n",
      "Iteration 12, loss = 0.69373246\n",
      "Iteration 13, loss = 0.69370020\n",
      "Iteration 14, loss = 0.69366923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72123287\n",
      "Iteration 2, loss = 0.92260389\n",
      "Iteration 3, loss = 0.69472437\n",
      "Iteration 4, loss = 0.69466507\n",
      "Iteration 5, loss = 0.69460528\n",
      "Iteration 6, loss = 0.69454540\n",
      "Iteration 7, loss = 0.69448579\n",
      "Iteration 8, loss = 0.69442675\n",
      "Iteration 9, loss = 0.69436854\n",
      "Iteration 10, loss = 0.69431138\n",
      "Iteration 11, loss = 0.69425547\n",
      "Iteration 12, loss = 0.69420095\n",
      "Iteration 13, loss = 0.69414795\n",
      "Iteration 14, loss = 0.69409656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72354826\n",
      "Iteration 2, loss = 0.92394418\n",
      "Iteration 3, loss = 0.69440015\n",
      "Iteration 4, loss = 0.69434977\n",
      "Iteration 5, loss = 0.69429957\n",
      "Iteration 6, loss = 0.69424982\n",
      "Iteration 7, loss = 0.69420074\n",
      "Iteration 8, loss = 0.69415253\n",
      "Iteration 9, loss = 0.69410533\n",
      "Iteration 10, loss = 0.69405929\n",
      "Iteration 11, loss = 0.69401452\n",
      "Iteration 12, loss = 0.69397110\n",
      "Iteration 13, loss = 0.69392909\n",
      "Iteration 14, loss = 0.69388855\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72546705\n",
      "Iteration 2, loss = 12.62171994\n",
      "Iteration 3, loss = 5.48554404\n",
      "Iteration 4, loss = 0.69406729\n",
      "Iteration 5, loss = 0.69393939\n",
      "Iteration 6, loss = 0.69383706\n",
      "Iteration 7, loss = 0.69375316\n",
      "Iteration 8, loss = 0.69368319\n",
      "Iteration 9, loss = 0.69362409\n",
      "Iteration 10, loss = 0.69357371\n",
      "Iteration 11, loss = 0.69353041\n",
      "Iteration 12, loss = 0.69349298\n",
      "Iteration 13, loss = 0.69346046\n",
      "Iteration 14, loss = 0.69343208\n",
      "Iteration 15, loss = 0.69340721\n",
      "Iteration 16, loss = 0.69338537\n",
      "Iteration 17, loss = 0.69336612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72123287\n",
      "Iteration 2, loss = 12.78366184\n",
      "Iteration 3, loss = 5.41469974\n",
      "Iteration 4, loss = 0.69471030\n",
      "Iteration 5, loss = 0.69453845\n",
      "Iteration 6, loss = 0.69439714\n",
      "Iteration 7, loss = 0.69427805\n",
      "Iteration 8, loss = 0.69417596\n",
      "Iteration 9, loss = 0.69408730\n",
      "Iteration 10, loss = 0.69400953\n",
      "Iteration 11, loss = 0.69394077\n",
      "Iteration 12, loss = 0.69387956\n",
      "Iteration 13, loss = 0.69382478\n",
      "Iteration 14, loss = 0.69377551\n",
      "Iteration 15, loss = 0.69373102\n",
      "Iteration 16, loss = 0.69369069\n",
      "Iteration 17, loss = 0.69365402\n",
      "Iteration 18, loss = 0.69362059\n",
      "Iteration 19, loss = 0.69359002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72354826\n",
      "Iteration 2, loss = 12.69929473\n",
      "Iteration 3, loss = 5.44472730\n",
      "Iteration 4, loss = 0.69438879\n",
      "Iteration 5, loss = 0.69423901\n",
      "Iteration 6, loss = 0.69411734\n",
      "Iteration 7, loss = 0.69401605\n",
      "Iteration 8, loss = 0.69393025\n",
      "Iteration 9, loss = 0.69385665\n",
      "Iteration 10, loss = 0.69379288\n",
      "Iteration 11, loss = 0.69373719\n",
      "Iteration 12, loss = 0.69368823\n",
      "Iteration 13, loss = 0.69364495\n",
      "Iteration 14, loss = 0.69360652\n",
      "Iteration 15, loss = 0.69357225\n",
      "Iteration 16, loss = 0.69354159\n",
      "Iteration 17, loss = 0.69351406\n",
      "Iteration 18, loss = 0.69348928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72388585\n",
      "Iteration 2, loss = 8.41177908\n",
      "Iteration 3, loss = 0.69510539\n",
      "Iteration 4, loss = 0.69410013\n",
      "Iteration 5, loss = 0.69348613\n",
      "Iteration 6, loss = 0.69320096\n",
      "Iteration 7, loss = 0.69316945\n",
      "Iteration 8, loss = 0.69331328\n",
      "Iteration 9, loss = 0.69355861\n",
      "Iteration 10, loss = 0.69384169\n",
      "Iteration 11, loss = 0.69411213\n",
      "Iteration 12, loss = 0.69433428\n",
      "Iteration 13, loss = 0.69448688\n",
      "Iteration 14, loss = 0.69456150\n",
      "Iteration 15, loss = 0.69456017\n",
      "Iteration 16, loss = 0.69449265\n",
      "Iteration 17, loss = 0.69437357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72598529\n",
      "Iteration 2, loss = 9.19558494\n",
      "Iteration 3, loss = 0.69444652\n",
      "Iteration 4, loss = 0.69369560\n",
      "Iteration 5, loss = 0.69328789\n",
      "Iteration 6, loss = 0.69315617\n",
      "Iteration 7, loss = 0.69322614\n",
      "Iteration 8, loss = 0.69342463\n",
      "Iteration 9, loss = 0.69368585\n",
      "Iteration 10, loss = 0.69395555\n",
      "Iteration 11, loss = 0.69419311\n",
      "Iteration 12, loss = 0.69437200\n",
      "Iteration 13, loss = 0.69447875\n",
      "Iteration 14, loss = 0.69451100\n",
      "Iteration 15, loss = 0.69447499\n",
      "Iteration 16, loss = 0.69438290\n",
      "Iteration 17, loss = 0.69425017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72621091\n",
      "Iteration 2, loss = 8.77352941\n",
      "Iteration 3, loss = 0.69478345\n",
      "Iteration 4, loss = 0.69390770\n",
      "Iteration 5, loss = 0.69339881\n",
      "Iteration 6, loss = 0.69319197\n",
      "Iteration 7, loss = 0.69321246\n",
      "Iteration 8, loss = 0.69338458\n",
      "Iteration 9, loss = 0.69363856\n",
      "Iteration 10, loss = 0.69391544\n",
      "Iteration 11, loss = 0.69416979\n",
      "Iteration 12, loss = 0.69437055\n",
      "Iteration 13, loss = 0.69450038\n",
      "Iteration 14, loss = 0.69455393\n",
      "Iteration 15, loss = 0.69453537\n",
      "Iteration 16, loss = 0.69445566\n",
      "Iteration 17, loss = 0.69432987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72388585\n",
      "Iteration 2, loss = 18.13975467\n",
      "Iteration 3, loss = 18.13985027\n",
      "Iteration 4, loss = 0.69654540\n",
      "Iteration 5, loss = 0.69474459\n",
      "Iteration 6, loss = 17.90487487\n",
      "Iteration 7, loss = 18.14071484\n",
      "Iteration 8, loss = 18.14098132\n",
      "Iteration 9, loss = 14.37949073\n",
      "Iteration 10, loss = 16.25282341\n",
      "Iteration 11, loss = 2.69857866\n",
      "Iteration 12, loss = 6.75080562\n",
      "Iteration 13, loss = 5.11575664\n",
      "Iteration 14, loss = 0.96778256\n",
      "Iteration 15, loss = 4.27110007\n",
      "Iteration 16, loss = 6.39257223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72598529\n",
      "Iteration 2, loss = 17.90417526\n",
      "Iteration 3, loss = 17.90427167\n",
      "Iteration 4, loss = 0.69567775\n",
      "Iteration 5, loss = 0.69428440\n",
      "Iteration 6, loss = 18.14044874\n",
      "Iteration 7, loss = 17.90512709\n",
      "Iteration 8, loss = 17.90539104\n",
      "Iteration 9, loss = 13.25523911\n",
      "Iteration 10, loss = 14.43085889\n",
      "Iteration 11, loss = 0.68524123\n",
      "Iteration 12, loss = 5.27360285\n",
      "Iteration 13, loss = 2.89537356\n",
      "Iteration 14, loss = 0.78209875\n",
      "Iteration 15, loss = 0.79496608\n",
      "Iteration 16, loss = 0.70093799\n",
      "Iteration 17, loss = 0.70204345\n",
      "Iteration 18, loss = 0.70303642\n",
      "Iteration 19, loss = 0.70389118\n",
      "Iteration 20, loss = 0.70459187\n",
      "Iteration 21, loss = 0.70513106\n",
      "Iteration 22, loss = 0.70550838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72621091\n",
      "Iteration 2, loss = 18.02196409\n",
      "Iteration 3, loss = 18.02205945\n",
      "Iteration 4, loss = 0.69611532\n",
      "Iteration 5, loss = 0.69452041\n",
      "Iteration 6, loss = 0.69428417\n",
      "Iteration 7, loss = 18.02291813\n",
      "Iteration 8, loss = 1.89680899\n",
      "Iteration 9, loss = 0.69805371\n",
      "Iteration 10, loss = 0.70138635\n",
      "Iteration 11, loss = 0.70503389\n",
      "Iteration 12, loss = 0.70865641\n",
      "Iteration 13, loss = 0.71200307\n",
      "Iteration 14, loss = 0.71489877\n",
      "Iteration 15, loss = 0.71723255\n",
      "Iteration 16, loss = 0.71894702\n",
      "Iteration 17, loss = 0.72002852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71967970\n",
      "Iteration 2, loss = 17.90404546\n",
      "Iteration 3, loss = 0.72530202\n",
      "Iteration 4, loss = 0.72627730\n",
      "Iteration 5, loss = 0.72554320\n",
      "Iteration 6, loss = 0.72337186\n",
      "Iteration 7, loss = 0.72012286\n",
      "Iteration 8, loss = 0.71619074\n",
      "Iteration 9, loss = 0.71196123\n",
      "Iteration 10, loss = 0.70777741\n",
      "Iteration 11, loss = 0.70391642\n",
      "Iteration 12, loss = 0.70057667\n",
      "Iteration 13, loss = 0.69787499\n",
      "Iteration 14, loss = 0.69585198\n",
      "Iteration 15, loss = 0.69448380\n",
      "Iteration 16, loss = 0.69369773\n",
      "Iteration 17, loss = 0.69338919\n",
      "Iteration 18, loss = 0.69343819\n",
      "Iteration 19, loss = 0.69372361\n",
      "Iteration 20, loss = 0.69413435\n",
      "Iteration 21, loss = 0.69457694\n",
      "Iteration 22, loss = 0.69497982\n",
      "Iteration 23, loss = 0.69529445\n",
      "Iteration 24, loss = 0.69549425\n",
      "Iteration 25, loss = 0.69557157\n",
      "Iteration 26, loss = 0.69553382\n",
      "Iteration 27, loss = 0.69539905\n",
      "Iteration 28, loss = 0.69519165\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71598122\n",
      "Iteration 2, loss = 18.13962478\n",
      "Iteration 3, loss = 0.72244479\n",
      "Iteration 4, loss = 0.72363859\n",
      "Iteration 5, loss = 0.72322804\n",
      "Iteration 6, loss = 0.72144354\n",
      "Iteration 7, loss = 0.71860505\n",
      "Iteration 8, loss = 0.71507259\n",
      "Iteration 9, loss = 0.71120458\n",
      "Iteration 10, loss = 0.70732512\n",
      "Iteration 11, loss = 0.70370090\n",
      "Iteration 12, loss = 0.70052789\n",
      "Iteration 13, loss = 0.69792706\n",
      "Iteration 14, loss = 0.69594818\n",
      "Iteration 15, loss = 0.69457952\n",
      "Iteration 16, loss = 0.69376166\n",
      "Iteration 17, loss = 0.69340290\n",
      "Iteration 18, loss = 0.69339461\n",
      "Iteration 19, loss = 0.69362471\n",
      "Iteration 20, loss = 0.69398854\n",
      "Iteration 21, loss = 0.69439656\n",
      "Iteration 22, loss = 0.69477880\n",
      "Iteration 23, loss = 0.69508653\n",
      "Iteration 24, loss = 0.69529155\n",
      "Iteration 25, loss = 0.69538378\n",
      "Iteration 26, loss = 0.69536776\n",
      "Iteration 27, loss = 0.69525862\n",
      "Iteration 28, loss = 0.69507807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448156\n",
      "Iteration 2, loss = 18.02183510\n",
      "Iteration 3, loss = 0.72386263\n",
      "Iteration 4, loss = 0.72494631\n",
      "Iteration 5, loss = 0.72437451\n",
      "Iteration 6, loss = 0.72239836\n",
      "Iteration 7, loss = 0.71935737\n",
      "Iteration 8, loss = 0.71562850\n",
      "Iteration 9, loss = 0.71158352\n",
      "Iteration 10, loss = 0.70755571\n",
      "Iteration 11, loss = 0.70381674\n",
      "Iteration 12, loss = 0.70056363\n",
      "Iteration 13, loss = 0.69791516\n",
      "Iteration 14, loss = 0.69591645\n",
      "Iteration 15, loss = 0.69454973\n",
      "Iteration 16, loss = 0.69374896\n",
      "Iteration 17, loss = 0.69341606\n",
      "Iteration 18, loss = 0.69343682\n",
      "Iteration 19, loss = 0.69369470\n",
      "Iteration 20, loss = 0.69408193\n",
      "Iteration 21, loss = 0.69450707\n",
      "Iteration 22, loss = 0.69489941\n",
      "Iteration 23, loss = 0.69521038\n",
      "Iteration 24, loss = 0.69541262\n",
      "Iteration 25, loss = 0.69549730\n",
      "Iteration 26, loss = 0.69547039\n",
      "Iteration 27, loss = 0.69534850\n",
      "Iteration 28, loss = 0.69515465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71967970\n",
      "Iteration 2, loss = 17.90425551\n",
      "Iteration 3, loss = 17.90449449\n",
      "Iteration 4, loss = 0.72249607\n",
      "Iteration 5, loss = 0.72928577\n",
      "Iteration 6, loss = 6.70873470\n",
      "Iteration 7, loss = 0.74006273\n",
      "Iteration 8, loss = 0.74838503\n",
      "Iteration 9, loss = 0.75264708\n",
      "Iteration 10, loss = 0.75506673\n",
      "Iteration 11, loss = 0.75581002\n",
      "Iteration 12, loss = 0.75508786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71598122\n",
      "Iteration 2, loss = 18.13983494\n",
      "Iteration 3, loss = 18.14007343\n",
      "Iteration 4, loss = 0.71969064\n",
      "Iteration 5, loss = 0.72633533\n",
      "Iteration 6, loss = 6.77312660\n",
      "Iteration 7, loss = 0.73735915\n",
      "Iteration 8, loss = 0.74540484\n",
      "Iteration 9, loss = 0.74981497\n",
      "Iteration 10, loss = 0.75246422\n",
      "Iteration 11, loss = 0.75349230\n",
      "Iteration 12, loss = 0.75308751\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71448156\n",
      "Iteration 2, loss = 18.02204381\n",
      "Iteration 3, loss = 18.02228102\n",
      "Iteration 4, loss = 0.72107930\n",
      "Iteration 5, loss = 0.72779124\n",
      "Iteration 6, loss = 6.72180372\n",
      "Iteration 7, loss = 0.73894925\n",
      "Iteration 8, loss = 0.74685923\n",
      "Iteration 9, loss = 0.75118914\n",
      "Iteration 10, loss = 0.75371891\n",
      "Iteration 11, loss = 0.75460127\n",
      "Iteration 12, loss = 0.75403566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72546705\n",
      "Iteration 2, loss = 5.09950322\n",
      "Iteration 3, loss = 0.69324162\n",
      "Iteration 4, loss = 0.69313916\n",
      "Iteration 5, loss = 0.69326557\n",
      "Iteration 6, loss = 0.69353164\n",
      "Iteration 7, loss = 0.69385874\n",
      "Iteration 8, loss = 0.69418337\n",
      "Iteration 9, loss = 0.69445924\n",
      "Iteration 10, loss = 0.69465739\n",
      "Iteration 11, loss = 0.69476454\n",
      "Iteration 12, loss = 0.69478055\n",
      "Iteration 13, loss = 0.69471515\n",
      "Iteration 14, loss = 0.69458463\n",
      "Iteration 15, loss = 0.69440866\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72123287\n",
      "Iteration 2, loss = 4.90525488\n",
      "Iteration 3, loss = 0.69347773\n",
      "Iteration 4, loss = 0.69317764\n",
      "Iteration 5, loss = 0.69316148\n",
      "Iteration 6, loss = 0.69333902\n",
      "Iteration 7, loss = 0.69362599\n",
      "Iteration 8, loss = 0.69395021\n",
      "Iteration 9, loss = 0.69425522\n",
      "Iteration 10, loss = 0.69450156\n",
      "Iteration 11, loss = 0.69466628\n",
      "Iteration 12, loss = 0.69474098\n",
      "Iteration 13, loss = 0.69472903\n",
      "Iteration 14, loss = 0.69464239\n",
      "Iteration 15, loss = 0.69449831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72354826\n",
      "Iteration 2, loss = 4.99217475\n",
      "Iteration 3, loss = 0.69336320\n",
      "Iteration 4, loss = 0.69316556\n",
      "Iteration 5, loss = 0.69322457\n",
      "Iteration 6, loss = 0.69345010\n",
      "Iteration 7, loss = 0.69376041\n",
      "Iteration 8, loss = 0.69408748\n",
      "Iteration 9, loss = 0.69437986\n",
      "Iteration 10, loss = 0.69460333\n",
      "Iteration 11, loss = 0.69473983\n",
      "Iteration 12, loss = 0.69478521\n",
      "Iteration 13, loss = 0.69474614\n",
      "Iteration 14, loss = 0.69463688\n",
      "Iteration 15, loss = 0.69447603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72546705\n",
      "Iteration 2, loss = 17.90425480\n",
      "Iteration 3, loss = 17.90449298\n",
      "Iteration 4, loss = 0.69432433\n",
      "Iteration 5, loss = 0.69427966\n",
      "Iteration 6, loss = 17.52747682\n",
      "Iteration 7, loss = 0.80013586\n",
      "Iteration 8, loss = 0.70020303\n",
      "Iteration 9, loss = 0.70387335\n",
      "Iteration 10, loss = 0.70761058\n",
      "Iteration 11, loss = 0.71110808\n",
      "Iteration 12, loss = 0.71415464\n",
      "Iteration 13, loss = 0.71661752\n",
      "Iteration 14, loss = 0.71842818\n",
      "Iteration 15, loss = 0.71956958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72123287\n",
      "Iteration 2, loss = 18.13983423\n",
      "Iteration 3, loss = 18.14007208\n",
      "Iteration 4, loss = 0.69475063\n",
      "Iteration 5, loss = 0.69430584\n",
      "Iteration 6, loss = 1.17388059\n",
      "Iteration 7, loss = 17.90599615\n",
      "Iteration 8, loss = 17.90637229\n",
      "Iteration 9, loss = 9.73077257\n",
      "Iteration 10, loss = 0.69733416\n",
      "Iteration 11, loss = 0.69720626\n",
      "Iteration 12, loss = 0.69721678\n",
      "Iteration 13, loss = 0.69733732\n",
      "Iteration 14, loss = 0.69754288\n",
      "Iteration 15, loss = 0.69781126\n",
      "Iteration 16, loss = 0.69812287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72354826\n",
      "Iteration 2, loss = 18.02204311\n",
      "Iteration 3, loss = 18.02227956\n",
      "Iteration 4, loss = 0.69453389\n",
      "Iteration 5, loss = 0.69428581\n",
      "Iteration 6, loss = 1.15709870\n",
      "Iteration 7, loss = 18.02377133\n",
      "Iteration 8, loss = 18.02414402\n",
      "Iteration 9, loss = 9.85211665\n",
      "Iteration 10, loss = 0.69752537\n",
      "Iteration 11, loss = 0.69733274\n",
      "Iteration 12, loss = 0.69729129\n",
      "Iteration 13, loss = 0.69737134\n",
      "Iteration 14, loss = 0.69754656\n",
      "Iteration 15, loss = 0.69779343\n",
      "Iteration 16, loss = 0.69809105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70032061\n",
      "Iteration 2, loss = 0.69719106\n",
      "Iteration 3, loss = 0.69524045\n",
      "Iteration 4, loss = 0.69534667\n",
      "Iteration 5, loss = 0.69546963\n",
      "Iteration 6, loss = 0.69525176\n",
      "Iteration 7, loss = 0.69474207\n",
      "Iteration 8, loss = 0.69442548\n",
      "Iteration 9, loss = 0.69421472\n",
      "Iteration 10, loss = 0.69412989\n",
      "Iteration 11, loss = 0.69409330\n",
      "Iteration 12, loss = 0.69403195\n",
      "Iteration 13, loss = 0.69393789\n",
      "Iteration 14, loss = 0.69381926\n",
      "Iteration 15, loss = 0.69370245\n",
      "Iteration 16, loss = 0.69357818\n",
      "Iteration 17, loss = 0.69345168\n",
      "Iteration 18, loss = 0.69334745\n",
      "Iteration 19, loss = 0.69324914\n",
      "Iteration 20, loss = 0.69314510\n",
      "Iteration 21, loss = 0.69303985\n",
      "Iteration 22, loss = 0.69292869\n",
      "Iteration 23, loss = 0.69282028\n",
      "Iteration 24, loss = 0.69271524\n",
      "Iteration 25, loss = 0.69260964\n",
      "Iteration 26, loss = 0.69249963\n",
      "Iteration 27, loss = 0.69239050\n",
      "Iteration 28, loss = 0.69228148\n",
      "Iteration 29, loss = 0.69217239\n",
      "Iteration 30, loss = 0.69205893\n",
      "Iteration 31, loss = 0.69194743\n",
      "Iteration 32, loss = 0.69183510\n",
      "Iteration 33, loss = 0.69172225\n",
      "Iteration 34, loss = 0.69160929\n",
      "Iteration 35, loss = 0.69149595\n",
      "Iteration 36, loss = 0.69138140\n",
      "Iteration 37, loss = 0.69126724\n",
      "Iteration 38, loss = 0.69115469\n",
      "Iteration 39, loss = 0.69103725\n",
      "Iteration 40, loss = 0.69092137\n",
      "Iteration 41, loss = 0.69080531\n",
      "Iteration 42, loss = 0.69068896\n",
      "Iteration 43, loss = 0.69057234\n",
      "Iteration 44, loss = 0.69045560\n",
      "Iteration 45, loss = 0.69033858\n",
      "Iteration 46, loss = 0.69022153\n",
      "Iteration 47, loss = 0.69010423\n",
      "Iteration 48, loss = 0.68998683\n",
      "Iteration 49, loss = 0.68986963\n",
      "Iteration 50, loss = 0.68975215\n",
      "Iteration 51, loss = 0.68963435\n",
      "Iteration 52, loss = 0.68951650\n",
      "Iteration 53, loss = 0.68939908\n",
      "Iteration 54, loss = 0.68928102\n",
      "Iteration 55, loss = 0.68916285\n",
      "Iteration 56, loss = 0.68904479\n",
      "Iteration 57, loss = 0.68892665\n",
      "Iteration 58, loss = 0.68880863\n",
      "Iteration 59, loss = 0.68869097\n",
      "Iteration 60, loss = 0.68857248\n",
      "Iteration 61, loss = 0.68845626\n",
      "Iteration 62, loss = 0.68834115\n",
      "Iteration 63, loss = 0.68821841\n",
      "Iteration 64, loss = 0.68810190\n",
      "Iteration 65, loss = 0.68798055\n",
      "Iteration 66, loss = 0.68786146\n",
      "Iteration 67, loss = 0.68774161\n",
      "Iteration 68, loss = 0.68762401\n",
      "Iteration 69, loss = 0.68750291\n",
      "Iteration 70, loss = 0.68738389\n",
      "Iteration 71, loss = 0.68726476\n",
      "Iteration 72, loss = 0.68715019\n",
      "Iteration 73, loss = 0.68702416\n",
      "Iteration 74, loss = 0.68690455\n",
      "Iteration 75, loss = 0.68678571\n",
      "Iteration 76, loss = 0.68666642\n",
      "Iteration 77, loss = 0.68654562\n",
      "Iteration 78, loss = 0.68642503\n",
      "Iteration 79, loss = 0.68631105\n",
      "Iteration 80, loss = 0.68618642\n",
      "Iteration 81, loss = 0.68606323\n",
      "Iteration 82, loss = 0.68594093\n",
      "Iteration 83, loss = 0.68581831\n",
      "Iteration 84, loss = 0.68569643\n",
      "Iteration 85, loss = 0.68557552\n",
      "Iteration 86, loss = 0.68545403\n",
      "Iteration 87, loss = 0.68533035\n",
      "Iteration 88, loss = 0.68520722\n",
      "Iteration 89, loss = 0.68508450\n",
      "Iteration 90, loss = 0.68496344\n",
      "Iteration 91, loss = 0.68483956\n",
      "Iteration 92, loss = 0.68471526\n",
      "Iteration 93, loss = 0.68459267\n",
      "Iteration 94, loss = 0.68447068\n",
      "Iteration 95, loss = 0.68434672\n",
      "Iteration 96, loss = 0.68422235\n",
      "Iteration 97, loss = 0.68409704\n",
      "Iteration 98, loss = 0.68396635\n",
      "Iteration 99, loss = 0.68381709\n",
      "Iteration 100, loss = 0.68368427\n",
      "Iteration 101, loss = 0.68355180\n",
      "Iteration 102, loss = 0.68342388\n",
      "Iteration 103, loss = 0.68329636\n",
      "Iteration 104, loss = 0.68316653\n",
      "Iteration 105, loss = 0.68303739\n",
      "Iteration 106, loss = 0.68290912\n",
      "Iteration 107, loss = 0.68278130\n",
      "Iteration 108, loss = 0.68264951\n",
      "Iteration 109, loss = 0.68251820\n",
      "Iteration 110, loss = 0.68238802\n",
      "Iteration 111, loss = 0.68225727\n",
      "Iteration 112, loss = 0.68212639\n",
      "Iteration 113, loss = 0.68199534\n",
      "Iteration 114, loss = 0.68186544\n",
      "Iteration 115, loss = 0.68173219\n",
      "Iteration 116, loss = 0.68160036\n",
      "Iteration 117, loss = 0.68146921\n",
      "Iteration 118, loss = 0.68133798\n",
      "Iteration 119, loss = 0.68120523\n",
      "Iteration 120, loss = 0.68107268\n",
      "Iteration 121, loss = 0.68093991\n",
      "Iteration 122, loss = 0.68080776\n",
      "Iteration 123, loss = 0.68067844\n",
      "Iteration 124, loss = 0.68054019\n",
      "Iteration 125, loss = 0.68040658\n",
      "Iteration 126, loss = 0.68027471\n",
      "Iteration 127, loss = 0.68013703\n",
      "Iteration 128, loss = 0.68000325\n",
      "Iteration 129, loss = 0.67987080\n",
      "Iteration 130, loss = 0.67973302\n",
      "Iteration 131, loss = 0.67959784\n",
      "Iteration 132, loss = 0.67946407\n",
      "Iteration 133, loss = 0.67932515\n",
      "Iteration 134, loss = 0.67918915\n",
      "Iteration 135, loss = 0.67905313\n",
      "Iteration 136, loss = 0.67891679\n",
      "Iteration 137, loss = 0.67877952\n",
      "Iteration 138, loss = 0.67864329\n",
      "Iteration 139, loss = 0.67850811\n",
      "Iteration 140, loss = 0.67836794\n",
      "Iteration 141, loss = 0.67823014\n",
      "Iteration 142, loss = 0.67809313\n",
      "Iteration 143, loss = 0.67795427\n",
      "Iteration 144, loss = 0.67781594\n",
      "Iteration 145, loss = 0.67767718\n",
      "Iteration 146, loss = 0.67753857\n",
      "Iteration 147, loss = 0.67739870\n",
      "Iteration 148, loss = 0.67725912\n",
      "Iteration 149, loss = 0.67711843\n",
      "Iteration 150, loss = 0.67697936\n",
      "Iteration 151, loss = 0.67683617\n",
      "Iteration 152, loss = 0.67669024\n",
      "Iteration 153, loss = 0.67654758\n",
      "Iteration 154, loss = 0.67640610\n",
      "Iteration 155, loss = 0.67626268\n",
      "Iteration 156, loss = 0.67611900\n",
      "Iteration 157, loss = 0.67597524\n",
      "Iteration 158, loss = 0.67583168\n",
      "Iteration 159, loss = 0.67568782\n",
      "Iteration 160, loss = 0.67554324\n",
      "Iteration 161, loss = 0.67539863\n",
      "Iteration 162, loss = 0.67525339\n",
      "Iteration 163, loss = 0.67510804\n",
      "Iteration 164, loss = 0.67496218\n",
      "Iteration 165, loss = 0.67481617\n",
      "Iteration 166, loss = 0.67466953\n",
      "Iteration 167, loss = 0.67452298\n",
      "Iteration 168, loss = 0.67437553\n",
      "Iteration 169, loss = 0.67422846\n",
      "Iteration 170, loss = 0.67408033\n",
      "Iteration 171, loss = 0.67393267\n",
      "Iteration 172, loss = 0.67378444\n",
      "Iteration 173, loss = 0.67363558\n",
      "Iteration 174, loss = 0.67348703\n",
      "Iteration 175, loss = 0.67333750\n",
      "Iteration 176, loss = 0.67318373\n",
      "Iteration 177, loss = 0.67302953\n",
      "Iteration 178, loss = 0.67287376\n",
      "Iteration 179, loss = 0.67271424\n",
      "Iteration 180, loss = 0.67255318\n",
      "Iteration 181, loss = 0.67239377\n",
      "Iteration 182, loss = 0.67223334\n",
      "Iteration 183, loss = 0.67207234\n",
      "Iteration 184, loss = 0.67190061\n",
      "Iteration 185, loss = 0.67171775\n",
      "Iteration 186, loss = 0.67147323\n",
      "Iteration 187, loss = 0.67103439\n",
      "Iteration 188, loss = 0.66956939\n",
      "Iteration 189, loss = 0.66903106\n",
      "Iteration 190, loss = 0.66841222\n",
      "Iteration 191, loss = 0.66792494\n",
      "Iteration 192, loss = 0.66753957\n",
      "Iteration 193, loss = 0.66735793\n",
      "Iteration 194, loss = 0.66712256\n",
      "Iteration 195, loss = 0.66686729\n",
      "Iteration 196, loss = 0.66680767\n",
      "Iteration 197, loss = 0.66653103\n",
      "Iteration 198, loss = 0.66628431\n",
      "Iteration 199, loss = 0.66605440\n",
      "Iteration 200, loss = 0.66584052\n",
      "Iteration 201, loss = 0.66563604\n",
      "Iteration 202, loss = 0.66542817\n",
      "Iteration 203, loss = 0.66521433\n",
      "Iteration 204, loss = 0.66500299\n",
      "Iteration 205, loss = 0.66478739\n",
      "Iteration 206, loss = 0.66457148\n",
      "Iteration 207, loss = 0.66435464\n",
      "Iteration 208, loss = 0.66413553\n",
      "Iteration 209, loss = 0.66391991\n",
      "Iteration 210, loss = 0.66369593\n",
      "Iteration 211, loss = 0.66347213\n",
      "Iteration 212, loss = 0.66325354\n",
      "Iteration 213, loss = 0.66302441\n",
      "Iteration 214, loss = 0.66279964\n",
      "Iteration 215, loss = 0.66257383\n",
      "Iteration 216, loss = 0.66234936\n",
      "Iteration 217, loss = 0.66212919\n",
      "Iteration 218, loss = 0.66190062\n",
      "Iteration 219, loss = 0.66167328\n",
      "Iteration 220, loss = 0.66145067\n",
      "Iteration 221, loss = 0.66122603\n",
      "Iteration 222, loss = 0.66099287\n",
      "Iteration 223, loss = 0.66076235\n",
      "Iteration 224, loss = 0.66053314\n",
      "Iteration 225, loss = 0.66030158\n",
      "Iteration 226, loss = 0.66007301\n",
      "Iteration 227, loss = 0.65984071\n",
      "Iteration 228, loss = 0.65960955\n",
      "Iteration 229, loss = 0.65937691\n",
      "Iteration 230, loss = 0.65914344\n",
      "Iteration 231, loss = 0.65891082\n",
      "Iteration 232, loss = 0.65867593\n",
      "Iteration 233, loss = 0.65844105\n",
      "Iteration 234, loss = 0.65820551\n",
      "Iteration 235, loss = 0.65797148\n",
      "Iteration 236, loss = 0.65773484\n",
      "Iteration 237, loss = 0.65749668\n",
      "Iteration 238, loss = 0.65725855\n",
      "Iteration 239, loss = 0.65701982\n",
      "Iteration 240, loss = 0.65678164\n",
      "Iteration 241, loss = 0.65654266\n",
      "Iteration 242, loss = 0.65630184\n",
      "Iteration 243, loss = 0.65606070\n",
      "Iteration 244, loss = 0.65581903\n",
      "Iteration 245, loss = 0.65557791\n",
      "Iteration 246, loss = 0.65533571\n",
      "Iteration 247, loss = 0.65509175\n",
      "Iteration 248, loss = 0.65484741\n",
      "Iteration 249, loss = 0.65460257\n",
      "Iteration 250, loss = 0.65435985\n",
      "Iteration 251, loss = 0.65411627\n",
      "Iteration 252, loss = 0.65386946\n",
      "Iteration 253, loss = 0.65362104\n",
      "Iteration 254, loss = 0.65337210\n",
      "Iteration 255, loss = 0.65312429\n",
      "Iteration 256, loss = 0.65287514\n",
      "Iteration 257, loss = 0.65262713\n",
      "Iteration 258, loss = 0.65237554\n",
      "Iteration 259, loss = 0.65212333\n",
      "Iteration 260, loss = 0.65187304\n",
      "Iteration 261, loss = 0.65161962\n",
      "Iteration 262, loss = 0.65136568\n",
      "Iteration 263, loss = 0.65111122\n",
      "Iteration 264, loss = 0.65085634\n",
      "Iteration 265, loss = 0.65060149\n",
      "Iteration 266, loss = 0.65034710\n",
      "Iteration 267, loss = 0.65008819\n",
      "Iteration 268, loss = 0.64983085\n",
      "Iteration 269, loss = 0.64957225\n",
      "Iteration 270, loss = 0.64931557\n",
      "Iteration 271, loss = 0.64905385\n",
      "Iteration 272, loss = 0.64879212\n",
      "Iteration 273, loss = 0.64853321\n",
      "Iteration 274, loss = 0.64826948\n",
      "Iteration 275, loss = 0.64800805\n",
      "Iteration 276, loss = 0.64774386\n",
      "Iteration 277, loss = 0.64748102\n",
      "Iteration 278, loss = 0.64721426\n",
      "Iteration 279, loss = 0.64694886\n",
      "Iteration 280, loss = 0.64668650\n",
      "Iteration 281, loss = 0.64641651\n",
      "Iteration 282, loss = 0.64614793\n",
      "Iteration 283, loss = 0.64587930\n",
      "Iteration 284, loss = 0.64561052\n",
      "Iteration 285, loss = 0.64533959\n",
      "Iteration 286, loss = 0.64506798\n",
      "Iteration 287, loss = 0.64479759\n",
      "Iteration 288, loss = 0.64452431\n",
      "Iteration 289, loss = 0.64425227\n",
      "Iteration 290, loss = 0.64397641\n",
      "Iteration 291, loss = 0.64370148\n",
      "Iteration 292, loss = 0.64342125\n",
      "Iteration 293, loss = 0.64313857\n",
      "Iteration 294, loss = 0.64285378\n",
      "Iteration 295, loss = 0.64256655\n",
      "Iteration 296, loss = 0.64227551\n",
      "Iteration 297, loss = 0.64198507\n",
      "Iteration 298, loss = 0.64169028\n",
      "Iteration 299, loss = 0.64139764\n",
      "Iteration 300, loss = 0.64110281\n",
      "Iteration 1, loss = 0.69963946\n",
      "Iteration 2, loss = 0.69718998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.69591775\n",
      "Iteration 4, loss = 0.69609347\n",
      "Iteration 5, loss = 0.69604188\n",
      "Iteration 6, loss = 0.69585192\n",
      "Iteration 7, loss = 0.69548455\n",
      "Iteration 8, loss = 0.69517996\n",
      "Iteration 9, loss = 0.69503749\n",
      "Iteration 10, loss = 0.69500441\n",
      "Iteration 11, loss = 0.69497308\n",
      "Iteration 12, loss = 0.69491466\n",
      "Iteration 13, loss = 0.69483145\n",
      "Iteration 14, loss = 0.69472899\n",
      "Iteration 15, loss = 0.69464818\n",
      "Iteration 16, loss = 0.69457397\n",
      "Iteration 17, loss = 0.69450127\n",
      "Iteration 18, loss = 0.69443205\n",
      "Iteration 19, loss = 0.69435263\n",
      "Iteration 20, loss = 0.69427534\n",
      "Iteration 21, loss = 0.69419732\n",
      "Iteration 22, loss = 0.69412049\n",
      "Iteration 23, loss = 0.69404085\n",
      "Iteration 24, loss = 0.69396183\n",
      "Iteration 25, loss = 0.69388197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69950619\n",
      "Iteration 2, loss = 0.69639005\n",
      "Iteration 3, loss = 0.69493924\n",
      "Iteration 4, loss = 0.69480745\n",
      "Iteration 5, loss = 0.69501503\n",
      "Iteration 6, loss = 0.69489205\n",
      "Iteration 7, loss = 0.69449179\n",
      "Iteration 8, loss = 0.69420408\n",
      "Iteration 9, loss = 0.69402629\n",
      "Iteration 10, loss = 0.69392449\n",
      "Iteration 11, loss = 0.69388121\n",
      "Iteration 12, loss = 0.69386191\n",
      "Iteration 13, loss = 0.69378935\n",
      "Iteration 14, loss = 0.69368568\n",
      "Iteration 15, loss = 0.69357902\n",
      "Iteration 16, loss = 0.69348423\n",
      "Iteration 17, loss = 0.69340014\n",
      "Iteration 18, loss = 0.69331849\n",
      "Iteration 19, loss = 0.69323911\n",
      "Iteration 20, loss = 0.69315443\n",
      "Iteration 21, loss = 0.69306783\n",
      "Iteration 22, loss = 0.69297817\n",
      "Iteration 23, loss = 0.69289035\n",
      "Iteration 24, loss = 0.69280011\n",
      "Iteration 25, loss = 0.69271250\n",
      "Iteration 26, loss = 0.69262142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70032061\n",
      "Iteration 2, loss = 2.32543710\n",
      "Iteration 3, loss = 0.91323989\n",
      "Iteration 4, loss = 0.72105603\n",
      "Iteration 5, loss = 0.72113499\n",
      "Iteration 6, loss = 0.72115775\n",
      "Iteration 7, loss = 0.72113962\n",
      "Iteration 8, loss = 0.72108989\n",
      "Iteration 9, loss = 0.72101480\n",
      "Iteration 10, loss = 0.72091890\n",
      "Iteration 11, loss = 0.72080561\n",
      "Iteration 12, loss = 0.72067764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963946\n",
      "Iteration 2, loss = 2.35929337\n",
      "Iteration 3, loss = 0.91305006\n",
      "Iteration 4, loss = 0.71804258\n",
      "Iteration 5, loss = 0.71811980\n",
      "Iteration 6, loss = 0.71814498\n",
      "Iteration 7, loss = 0.71813228\n",
      "Iteration 8, loss = 0.71809023\n",
      "Iteration 9, loss = 0.71802461\n",
      "Iteration 10, loss = 0.71793958\n",
      "Iteration 11, loss = 0.71783832\n",
      "Iteration 12, loss = 0.71772330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69950619\n",
      "Iteration 2, loss = 2.34141617\n",
      "Iteration 3, loss = 0.91941208\n",
      "Iteration 4, loss = 0.71954938\n",
      "Iteration 5, loss = 0.71962685\n",
      "Iteration 6, loss = 0.71965064\n",
      "Iteration 7, loss = 0.71963526\n",
      "Iteration 8, loss = 0.71958954\n",
      "Iteration 9, loss = 0.71951943\n",
      "Iteration 10, loss = 0.71942927\n",
      "Iteration 11, loss = 0.71932233\n",
      "Iteration 12, loss = 0.71920120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85294991\n",
      "Iteration 2, loss = 0.70161905\n",
      "Iteration 3, loss = 0.70021834\n",
      "Iteration 4, loss = 0.69889164\n",
      "Iteration 5, loss = 0.69768414\n",
      "Iteration 6, loss = 0.69662449\n",
      "Iteration 7, loss = 0.69607538\n",
      "Iteration 8, loss = 0.69443102\n",
      "Iteration 9, loss = 0.69408322\n",
      "Iteration 10, loss = 0.69406176\n",
      "Iteration 11, loss = 0.69404114\n",
      "Iteration 12, loss = 0.69402130\n",
      "Iteration 13, loss = 0.69400219\n",
      "Iteration 14, loss = 0.69398375\n",
      "Iteration 15, loss = 0.69396593\n",
      "Iteration 16, loss = 0.69394871\n",
      "Iteration 17, loss = 0.69393202\n",
      "Iteration 18, loss = 0.69391586\n",
      "Iteration 19, loss = 0.69390017\n",
      "Iteration 20, loss = 0.69388493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85956724\n",
      "Iteration 2, loss = 0.70291957\n",
      "Iteration 3, loss = 0.70130625\n",
      "Iteration 4, loss = 0.69976068\n",
      "Iteration 5, loss = 0.69833932\n",
      "Iteration 6, loss = 0.69707640\n",
      "Iteration 7, loss = 0.69580699\n",
      "Iteration 8, loss = 0.69472885\n",
      "Iteration 9, loss = 0.69471963\n",
      "Iteration 10, loss = 0.69468809\n",
      "Iteration 11, loss = 0.69465744\n",
      "Iteration 12, loss = 0.69462763\n",
      "Iteration 13, loss = 0.69459861\n",
      "Iteration 14, loss = 0.69457033\n",
      "Iteration 15, loss = 0.69454278\n",
      "Iteration 16, loss = 0.69451590\n",
      "Iteration 17, loss = 0.69448968\n",
      "Iteration 18, loss = 0.69446407\n",
      "Iteration 19, loss = 0.69443906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86058326\n",
      "Iteration 2, loss = 0.70214976\n",
      "Iteration 3, loss = 0.70064101\n",
      "Iteration 4, loss = 0.69920352\n",
      "Iteration 5, loss = 0.69788810\n",
      "Iteration 6, loss = 0.69672760\n",
      "Iteration 7, loss = 0.69570186\n",
      "Iteration 8, loss = 0.69429627\n",
      "Iteration 9, loss = 0.69440171\n",
      "Iteration 10, loss = 0.69437541\n",
      "Iteration 11, loss = 0.69435000\n",
      "Iteration 12, loss = 0.69432540\n",
      "Iteration 13, loss = 0.69430157\n",
      "Iteration 14, loss = 0.69427846\n",
      "Iteration 15, loss = 0.69425603\n",
      "Iteration 16, loss = 0.69423424\n",
      "Iteration 17, loss = 0.69421305\n",
      "Iteration 18, loss = 0.69419243\n",
      "Iteration 19, loss = 0.69417236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85294991\n",
      "Iteration 2, loss = 0.69419465\n",
      "Iteration 3, loss = 0.69411337\n",
      "Iteration 4, loss = 0.69404458\n",
      "Iteration 5, loss = 0.69398373\n",
      "Iteration 6, loss = 0.69392858\n",
      "Iteration 7, loss = 0.69387787\n",
      "Iteration 8, loss = 0.69383078\n",
      "Iteration 9, loss = 0.69378675\n",
      "Iteration 10, loss = 0.69374540\n",
      "Iteration 11, loss = 0.69370643\n",
      "Iteration 12, loss = 0.69366963\n",
      "Iteration 13, loss = 0.69363480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85956724\n",
      "Iteration 2, loss = 0.69488232\n",
      "Iteration 3, loss = 0.69477586\n",
      "Iteration 4, loss = 0.69468384\n",
      "Iteration 5, loss = 0.69460088\n",
      "Iteration 6, loss = 0.69452440\n",
      "Iteration 7, loss = 0.69445290\n",
      "Iteration 8, loss = 0.69438548\n",
      "Iteration 9, loss = 0.69432151\n",
      "Iteration 10, loss = 0.69426057\n",
      "Iteration 11, loss = 0.69420235\n",
      "Iteration 12, loss = 0.69414661\n",
      "Iteration 13, loss = 0.69409317\n",
      "Iteration 14, loss = 0.69404189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86058326\n",
      "Iteration 2, loss = 0.69453843\n",
      "Iteration 3, loss = 0.69444467\n",
      "Iteration 4, loss = 0.69436443\n",
      "Iteration 5, loss = 0.69429273\n",
      "Iteration 6, loss = 0.69422716\n",
      "Iteration 7, loss = 0.69416633\n",
      "Iteration 8, loss = 0.69410937\n",
      "Iteration 9, loss = 0.69405569\n",
      "Iteration 10, loss = 0.69400487\n",
      "Iteration 11, loss = 0.69395662\n",
      "Iteration 12, loss = 0.69391070\n",
      "Iteration 13, loss = 0.69386694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94758561\n",
      "Iteration 2, loss = 0.75280711\n",
      "Iteration 3, loss = 0.74253960\n",
      "Iteration 4, loss = 0.73681020\n",
      "Iteration 5, loss = 0.73146024\n",
      "Iteration 6, loss = 0.72218090\n",
      "Iteration 7, loss = 0.71297658\n",
      "Iteration 8, loss = 0.70619073\n",
      "Iteration 9, loss = 0.70316072\n",
      "Iteration 10, loss = 0.69903101\n",
      "Iteration 11, loss = 0.69545885\n",
      "Iteration 12, loss = 0.69311748\n",
      "Iteration 13, loss = 0.69209298\n",
      "Iteration 14, loss = 0.69210589\n",
      "Iteration 15, loss = 0.69275680\n",
      "Iteration 16, loss = 0.69370681\n",
      "Iteration 17, loss = 0.69450433\n",
      "Iteration 18, loss = 0.69510026\n",
      "Iteration 19, loss = 0.69538276\n",
      "Iteration 20, loss = 0.69532520\n",
      "Iteration 21, loss = 0.69493686\n",
      "Iteration 22, loss = 0.69432347\n",
      "Iteration 23, loss = 0.69352837\n",
      "Iteration 24, loss = 0.69276997\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95702996\n",
      "Iteration 2, loss = 0.75544455\n",
      "Iteration 3, loss = 0.74575932\n",
      "Iteration 4, loss = 0.74080175\n",
      "Iteration 5, loss = 0.73434355\n",
      "Iteration 6, loss = 0.72431762\n",
      "Iteration 7, loss = 0.71428763\n",
      "Iteration 8, loss = 0.70796058\n",
      "Iteration 9, loss = 0.70345492\n",
      "Iteration 10, loss = 0.69876813\n",
      "Iteration 11, loss = 0.69498650\n",
      "Iteration 12, loss = 0.69260944\n",
      "Iteration 13, loss = 0.69155982\n",
      "Iteration 14, loss = 0.69203485\n",
      "Iteration 15, loss = 0.69269563\n",
      "Iteration 16, loss = 0.69354037\n",
      "Iteration 17, loss = 0.69440013\n",
      "Iteration 18, loss = 0.69478392\n",
      "Iteration 19, loss = 0.69463199\n",
      "Iteration 20, loss = 0.69392367\n",
      "Iteration 21, loss = 0.69310744\n",
      "Iteration 22, loss = 0.69240078\n",
      "Iteration 23, loss = 0.69192834\n",
      "Iteration 24, loss = 0.69171241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95214260\n",
      "Iteration 2, loss = 0.75396736\n",
      "Iteration 3, loss = 0.74430146\n",
      "Iteration 4, loss = 0.73852254\n",
      "Iteration 5, loss = 0.73098818\n",
      "Iteration 6, loss = 0.72116275\n",
      "Iteration 7, loss = 0.71154930\n",
      "Iteration 8, loss = 0.70896021\n",
      "Iteration 9, loss = 0.70463455\n",
      "Iteration 10, loss = 0.69980942\n",
      "Iteration 11, loss = 0.69575763\n",
      "Iteration 12, loss = 0.69318117\n",
      "Iteration 13, loss = 0.69216397\n",
      "Iteration 14, loss = 0.69231673\n",
      "Iteration 15, loss = 0.69311052\n",
      "Iteration 16, loss = 0.69402182\n",
      "Iteration 17, loss = 0.69462133\n",
      "Iteration 18, loss = 0.69467696\n",
      "Iteration 19, loss = 0.69428631\n",
      "Iteration 20, loss = 0.69363616\n",
      "Iteration 21, loss = 0.69294290\n",
      "Iteration 22, loss = 0.69237626\n",
      "Iteration 23, loss = 0.69202152\n",
      "Iteration 24, loss = 0.69187923\n",
      "Iteration 25, loss = 0.69189091\n",
      "Iteration 26, loss = 0.69197463\n",
      "Iteration 27, loss = 0.69205646\n",
      "Iteration 28, loss = 0.69208962\n",
      "Iteration 29, loss = 0.69205940\n",
      "Iteration 30, loss = 0.69197690\n",
      "Iteration 31, loss = 0.69186705\n",
      "Iteration 32, loss = 0.69175637\n",
      "Iteration 33, loss = 0.69166419\n",
      "Iteration 34, loss = 0.69159890\n",
      "Iteration 35, loss = 0.69155880\n",
      "Iteration 36, loss = 0.69153568\n",
      "Iteration 37, loss = 0.69151932\n",
      "Iteration 38, loss = 0.69150111\n",
      "Iteration 39, loss = 0.69147608\n",
      "Iteration 40, loss = 0.69144318\n",
      "Iteration 41, loss = 0.69140428\n",
      "Iteration 42, loss = 0.69136264\n",
      "Iteration 43, loss = 0.69132144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94758561\n",
      "Iteration 2, loss = 0.92314866\n",
      "Iteration 3, loss = 0.69448461\n",
      "Iteration 4, loss = 0.73916027\n",
      "Iteration 5, loss = 0.73912223\n",
      "Iteration 6, loss = 0.73886304\n",
      "Iteration 7, loss = 0.73856817\n",
      "Iteration 8, loss = 0.73824607\n",
      "Iteration 9, loss = 0.73790269\n",
      "Iteration 10, loss = 0.73754235\n",
      "Iteration 11, loss = 0.73716834\n",
      "Iteration 12, loss = 0.73678324\n",
      "Iteration 13, loss = 0.73638910\n",
      "Iteration 14, loss = 0.73598756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95702996\n",
      "Iteration 2, loss = 0.91358747\n",
      "Iteration 3, loss = 0.69554276\n",
      "Iteration 4, loss = 0.74306209\n",
      "Iteration 5, loss = 0.74318351\n",
      "Iteration 6, loss = 0.74290463\n",
      "Iteration 7, loss = 0.74258838\n",
      "Iteration 8, loss = 0.74224378\n",
      "Iteration 9, loss = 0.74187712\n",
      "Iteration 10, loss = 0.74149296\n",
      "Iteration 11, loss = 0.74109478\n",
      "Iteration 12, loss = 0.74068523\n",
      "Iteration 13, loss = 0.74026646\n",
      "Iteration 14, loss = 0.73984017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95214260\n",
      "Iteration 2, loss = 0.91837158\n",
      "Iteration 3, loss = 0.69463282\n",
      "Iteration 4, loss = 0.74138585\n",
      "Iteration 5, loss = 0.74115903\n",
      "Iteration 6, loss = 0.74089087\n",
      "Iteration 7, loss = 0.74058607\n",
      "Iteration 8, loss = 0.74025340\n",
      "Iteration 9, loss = 0.73989898\n",
      "Iteration 10, loss = 0.73952729\n",
      "Iteration 11, loss = 0.73914169\n",
      "Iteration 12, loss = 0.73874483\n",
      "Iteration 13, loss = 0.73833878\n",
      "Iteration 14, loss = 0.73792526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70032061\n",
      "Iteration 2, loss = 0.74351169\n",
      "Iteration 3, loss = 0.72165186\n",
      "Iteration 4, loss = 0.72133001\n",
      "Iteration 5, loss = 0.72079286\n",
      "Iteration 6, loss = 0.72004381\n",
      "Iteration 7, loss = 0.71911617\n",
      "Iteration 8, loss = 0.71804296\n",
      "Iteration 9, loss = 0.71685524\n",
      "Iteration 10, loss = 0.71558184\n",
      "Iteration 11, loss = 0.71424908\n",
      "Iteration 12, loss = 0.71288067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963946\n",
      "Iteration 2, loss = 0.73866189\n",
      "Iteration 3, loss = 0.71855782\n",
      "Iteration 4, loss = 0.71830412\n",
      "Iteration 5, loss = 0.71782854\n",
      "Iteration 6, loss = 0.71716249\n",
      "Iteration 7, loss = 0.71633676\n",
      "Iteration 8, loss = 0.71538081\n",
      "Iteration 9, loss = 0.71432242\n",
      "Iteration 10, loss = 0.71318734\n",
      "Iteration 11, loss = 0.71199910\n",
      "Iteration 12, loss = 0.71077890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69950619\n",
      "Iteration 2, loss = 0.74699055\n",
      "Iteration 3, loss = 0.72028618\n",
      "Iteration 4, loss = 0.71996361\n",
      "Iteration 5, loss = 0.71943174\n",
      "Iteration 6, loss = 0.71870692\n",
      "Iteration 7, loss = 0.71783513\n",
      "Iteration 8, loss = 0.71682901\n",
      "Iteration 9, loss = 0.71571349\n",
      "Iteration 10, loss = 0.71451492\n",
      "Iteration 11, loss = 0.71325837\n",
      "Iteration 12, loss = 0.71196646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70032061\n",
      "Iteration 2, loss = 17.90404624\n",
      "Iteration 3, loss = 7.33248674\n",
      "Iteration 4, loss = 0.72056501\n",
      "Iteration 5, loss = 0.72152612\n",
      "Iteration 6, loss = 0.72197242\n",
      "Iteration 7, loss = 0.72204350\n",
      "Iteration 8, loss = 0.72182956\n",
      "Iteration 9, loss = 0.72139535\n",
      "Iteration 10, loss = 0.72079009\n",
      "Iteration 11, loss = 0.72005258\n",
      "Iteration 12, loss = 0.71921408\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963946\n",
      "Iteration 2, loss = 18.13962566\n",
      "Iteration 3, loss = 7.33791166\n",
      "Iteration 4, loss = 0.71760446\n",
      "Iteration 5, loss = 0.71853478\n",
      "Iteration 6, loss = 0.71898801\n",
      "Iteration 7, loss = 0.71909269\n",
      "Iteration 8, loss = 0.71893187\n",
      "Iteration 9, loss = 0.71856522\n",
      "Iteration 10, loss = 0.71803821\n",
      "Iteration 11, loss = 0.71738674\n",
      "Iteration 12, loss = 0.71663984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69950619\n",
      "Iteration 2, loss = 18.02183589\n",
      "Iteration 3, loss = 7.48967844\n",
      "Iteration 4, loss = 0.71908272\n",
      "Iteration 5, loss = 0.72001881\n",
      "Iteration 6, loss = 0.72046326\n",
      "Iteration 7, loss = 0.72054812\n",
      "Iteration 8, loss = 0.72035924\n",
      "Iteration 9, loss = 0.71995836\n",
      "Iteration 10, loss = 0.71939254\n",
      "Iteration 11, loss = 0.71869892\n",
      "Iteration 12, loss = 0.71790751\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85294991\n",
      "Iteration 2, loss = 0.69393218\n",
      "Iteration 3, loss = 0.69377566\n",
      "Iteration 4, loss = 0.69364110\n",
      "Iteration 5, loss = 0.69352713\n",
      "Iteration 6, loss = 0.69343212\n",
      "Iteration 7, loss = 0.69335428\n",
      "Iteration 8, loss = 0.69329176\n",
      "Iteration 9, loss = 0.69324269\n",
      "Iteration 10, loss = 0.69320524\n",
      "Iteration 11, loss = 0.69317766\n",
      "Iteration 12, loss = 0.69315831\n",
      "Iteration 13, loss = 0.69314568\n",
      "Iteration 14, loss = 0.69313842\n",
      "Iteration 15, loss = 0.69313532\n",
      "Iteration 16, loss = 0.69313532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85956724\n",
      "Iteration 2, loss = 0.69452905\n",
      "Iteration 3, loss = 0.69430522\n",
      "Iteration 4, loss = 0.69410455\n",
      "Iteration 5, loss = 0.69392707\n",
      "Iteration 6, loss = 0.69377221\n",
      "Iteration 7, loss = 0.69363898\n",
      "Iteration 8, loss = 0.69352603\n",
      "Iteration 9, loss = 0.69343177\n",
      "Iteration 10, loss = 0.69335446\n",
      "Iteration 11, loss = 0.69329229\n",
      "Iteration 12, loss = 0.69324342\n",
      "Iteration 13, loss = 0.69320605\n",
      "Iteration 14, loss = 0.69317846\n",
      "Iteration 15, loss = 0.69315904\n",
      "Iteration 16, loss = 0.69314629\n",
      "Iteration 17, loss = 0.69313889\n",
      "Iteration 18, loss = 0.69313563\n",
      "Iteration 19, loss = 0.69313547\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86058326\n",
      "Iteration 2, loss = 0.69422978\n",
      "Iteration 3, loss = 0.69404014\n",
      "Iteration 4, loss = 0.69387343\n",
      "Iteration 5, loss = 0.69372889\n",
      "Iteration 6, loss = 0.69360536\n",
      "Iteration 7, loss = 0.69350136\n",
      "Iteration 8, loss = 0.69341524\n",
      "Iteration 9, loss = 0.69334522\n",
      "Iteration 10, loss = 0.69328946\n",
      "Iteration 11, loss = 0.69324615\n",
      "Iteration 12, loss = 0.69321352\n",
      "Iteration 13, loss = 0.69318990\n",
      "Iteration 14, loss = 0.69317373\n",
      "Iteration 15, loss = 0.69316360\n",
      "Iteration 16, loss = 0.69315824\n",
      "Iteration 17, loss = 0.69315652\n",
      "Iteration 18, loss = 0.69315747\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85294991\n",
      "Iteration 2, loss = 0.69349705\n",
      "Iteration 3, loss = 0.69318209\n",
      "Iteration 4, loss = 0.69315184\n",
      "Iteration 5, loss = 0.69327291\n",
      "Iteration 6, loss = 0.69343717\n",
      "Iteration 7, loss = 0.69358155\n",
      "Iteration 8, loss = 0.69368345\n",
      "Iteration 9, loss = 0.69373849\n",
      "Iteration 10, loss = 0.69375007\n",
      "Iteration 11, loss = 0.69372538\n",
      "Iteration 12, loss = 0.69367329\n",
      "Iteration 13, loss = 0.69360309\n",
      "Iteration 14, loss = 0.69352358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85956724\n",
      "Iteration 2, loss = 0.69393418\n",
      "Iteration 3, loss = 0.69338424\n",
      "Iteration 4, loss = 0.69316565\n",
      "Iteration 5, loss = 0.69315824\n",
      "Iteration 6, loss = 0.69327429\n",
      "Iteration 7, loss = 0.69343628\n",
      "Iteration 8, loss = 0.69358774\n",
      "Iteration 9, loss = 0.69370015\n",
      "Iteration 10, loss = 0.69376452\n",
      "Iteration 11, loss = 0.69378234\n",
      "Iteration 12, loss = 0.69376037\n",
      "Iteration 13, loss = 0.69370786\n",
      "Iteration 14, loss = 0.69363483\n",
      "Iteration 15, loss = 0.69355095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86058326\n",
      "Iteration 2, loss = 0.69371554\n",
      "Iteration 3, loss = 0.69328408\n",
      "Iteration 4, loss = 0.69316180\n",
      "Iteration 5, loss = 0.69322487\n",
      "Iteration 6, loss = 0.69337558\n",
      "Iteration 7, loss = 0.69353629\n",
      "Iteration 8, loss = 0.69366539\n",
      "Iteration 9, loss = 0.69374841\n",
      "Iteration 10, loss = 0.69378429\n",
      "Iteration 11, loss = 0.69377835\n",
      "Iteration 12, loss = 0.69373894\n",
      "Iteration 13, loss = 0.69367559\n",
      "Iteration 14, loss = 0.69359786\n",
      "Iteration 15, loss = 0.69351449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94758561\n",
      "Iteration 2, loss = 0.72693555\n",
      "Iteration 3, loss = 0.69262828\n",
      "Iteration 4, loss = 0.69392269\n",
      "Iteration 5, loss = 0.69226433\n",
      "Iteration 6, loss = 0.69213242\n",
      "Iteration 7, loss = 0.69201414\n",
      "Iteration 8, loss = 0.69233792\n",
      "Iteration 9, loss = 0.69714039\n",
      "Iteration 10, loss = 0.69235640\n",
      "Iteration 11, loss = 0.69235168\n",
      "Iteration 12, loss = 0.69234681\n",
      "Iteration 13, loss = 0.69230718\n",
      "Iteration 14, loss = 0.69224400\n",
      "Iteration 15, loss = 0.69218728\n",
      "Iteration 16, loss = 0.69214850\n",
      "Iteration 17, loss = 0.69211754\n",
      "Iteration 18, loss = 0.69208362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95702996\n",
      "Iteration 2, loss = 0.73063862\n",
      "Iteration 3, loss = 0.69262689\n",
      "Iteration 4, loss = 0.69262662\n",
      "Iteration 5, loss = 0.69201202\n",
      "Iteration 6, loss = 0.69178523\n",
      "Iteration 7, loss = 0.69181585\n",
      "Iteration 8, loss = 0.69246111\n",
      "Iteration 9, loss = 0.69287773\n",
      "Iteration 10, loss = 0.69300452\n",
      "Iteration 11, loss = 0.69271931\n",
      "Iteration 12, loss = 0.69239247\n",
      "Iteration 13, loss = 0.69228569\n",
      "Iteration 14, loss = 0.69232581\n",
      "Iteration 15, loss = 0.69233369\n",
      "Iteration 16, loss = 0.69226374\n",
      "Iteration 17, loss = 0.69218313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95214260\n",
      "Iteration 2, loss = 0.72773812\n",
      "Iteration 3, loss = 0.69248112\n",
      "Iteration 4, loss = 0.69234054\n",
      "Iteration 5, loss = 0.69225828\n",
      "Iteration 6, loss = 0.69199548\n",
      "Iteration 7, loss = 0.69231343\n",
      "Iteration 8, loss = 0.69465549\n",
      "Iteration 9, loss = 0.69250355\n",
      "Iteration 10, loss = 0.69245255\n",
      "Iteration 11, loss = 0.69238952\n",
      "Iteration 12, loss = 0.69235384\n",
      "Iteration 13, loss = 0.69233897\n",
      "Iteration 14, loss = 0.69231684\n",
      "Iteration 15, loss = 0.69227955\n",
      "Iteration 16, loss = 0.69223979\n",
      "Iteration 17, loss = 0.69220638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94758561\n",
      "Iteration 2, loss = 9.57835654\n",
      "Iteration 3, loss = 2.05684843\n",
      "Iteration 4, loss = 0.72174383\n",
      "Iteration 5, loss = 0.69439659\n",
      "Iteration 6, loss = 0.76701432\n",
      "Iteration 7, loss = 0.74191909\n",
      "Iteration 8, loss = 0.69284518\n",
      "Iteration 9, loss = 0.75199620\n",
      "Iteration 10, loss = 0.72840695\n",
      "Iteration 11, loss = 0.69491585\n",
      "Iteration 12, loss = 0.74315081\n",
      "Iteration 13, loss = 0.72048050\n",
      "Iteration 14, loss = 0.69480559\n",
      "Iteration 15, loss = 0.73235371\n",
      "Iteration 16, loss = 0.71124833\n",
      "Iteration 17, loss = 0.69594359\n",
      "Iteration 18, loss = 0.72377923\n",
      "Iteration 19, loss = 0.70506299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95702996\n",
      "Iteration 2, loss = 9.45645368\n",
      "Iteration 3, loss = 2.04122413\n",
      "Iteration 4, loss = 0.71954443\n",
      "Iteration 5, loss = 0.69486583\n",
      "Iteration 6, loss = 0.76330063\n",
      "Iteration 7, loss = 0.73556824\n",
      "Iteration 8, loss = 0.69369033\n",
      "Iteration 9, loss = 0.75155721\n",
      "Iteration 10, loss = 0.72012410\n",
      "Iteration 11, loss = 0.69876238\n",
      "Iteration 12, loss = 0.74336070\n",
      "Iteration 13, loss = 0.71056794\n",
      "Iteration 14, loss = 0.70051298\n",
      "Iteration 15, loss = 0.73231342\n",
      "Iteration 16, loss = 0.70142737\n",
      "Iteration 17, loss = 0.70344985\n",
      "Iteration 18, loss = 0.72246210\n",
      "Iteration 19, loss = 0.69627785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95214260\n",
      "Iteration 2, loss = 9.51971923\n",
      "Iteration 3, loss = 2.01176177\n",
      "Iteration 4, loss = 0.71897330\n",
      "Iteration 5, loss = 0.69617368\n",
      "Iteration 6, loss = 0.76386671\n",
      "Iteration 7, loss = 0.73315258\n",
      "Iteration 8, loss = 0.69458882\n",
      "Iteration 9, loss = 0.75279315\n",
      "Iteration 10, loss = 0.71654362\n",
      "Iteration 11, loss = 0.70172875\n",
      "Iteration 12, loss = 0.74378044\n",
      "Iteration 13, loss = 0.70657509\n",
      "Iteration 14, loss = 0.70444561\n",
      "Iteration 15, loss = 0.73167756\n",
      "Iteration 16, loss = 0.69783330\n",
      "Iteration 17, loss = 0.70812982\n",
      "Iteration 18, loss = 0.72055980\n",
      "Iteration 19, loss = 0.69391116\n",
      "Iteration 20, loss = 0.70930356\n",
      "Iteration 21, loss = 0.71045363\n",
      "Iteration 22, loss = 0.69291572\n",
      "Iteration 23, loss = 0.70903965\n",
      "Iteration 24, loss = 0.70228710\n",
      "Iteration 25, loss = 0.69422924\n",
      "Iteration 26, loss = 0.70679050\n",
      "Iteration 27, loss = 0.69635310\n",
      "Iteration 28, loss = 0.69664618\n",
      "Iteration 29, loss = 0.70302721\n",
      "Iteration 30, loss = 0.69337857\n",
      "Iteration 31, loss = 0.69851052\n",
      "Iteration 32, loss = 0.69867684\n",
      "Iteration 33, loss = 0.69300325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70032061\n",
      "Iteration 2, loss = 3.83760823\n",
      "Iteration 3, loss = 0.79909936\n",
      "Iteration 4, loss = 0.82282897\n",
      "Iteration 5, loss = 0.82831391\n",
      "Iteration 6, loss = 0.81266798\n",
      "Iteration 7, loss = 0.77869697\n",
      "Iteration 8, loss = 0.73688569\n",
      "Iteration 9, loss = 0.72189973\n",
      "Iteration 10, loss = 0.71187927\n",
      "Iteration 11, loss = 0.70418529\n",
      "Iteration 12, loss = 0.69875377\n",
      "Iteration 13, loss = 0.69534818\n",
      "Iteration 14, loss = 0.69362456\n",
      "Iteration 15, loss = 0.69318903\n",
      "Iteration 16, loss = 0.69364460\n",
      "Iteration 17, loss = 0.69462633\n",
      "Iteration 18, loss = 0.69582456\n",
      "Iteration 19, loss = 0.69699757\n",
      "Iteration 20, loss = 0.69797509\n",
      "Iteration 21, loss = 0.69865490\n",
      "Iteration 22, loss = 0.69899441\n",
      "Iteration 23, loss = 0.69899921\n",
      "Iteration 24, loss = 0.69871034\n",
      "Iteration 25, loss = 0.69819143\n",
      "Iteration 26, loss = 0.69751703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963946\n",
      "Iteration 2, loss = 3.56379402\n",
      "Iteration 3, loss = 0.78924044\n",
      "Iteration 4, loss = 0.81131218\n",
      "Iteration 5, loss = 0.81786055\n",
      "Iteration 6, loss = 0.80660994\n",
      "Iteration 7, loss = 0.77938621\n",
      "Iteration 8, loss = 0.74349348\n",
      "Iteration 9, loss = 0.72157004\n",
      "Iteration 10, loss = 0.71179456\n",
      "Iteration 11, loss = 0.70424095\n",
      "Iteration 12, loss = 0.69886590\n",
      "Iteration 13, loss = 0.69545542\n",
      "Iteration 14, loss = 0.69368727\n",
      "Iteration 15, loss = 0.69318651\n",
      "Iteration 16, loss = 0.69357141\n",
      "Iteration 17, loss = 0.69448814\n",
      "Iteration 18, loss = 0.69563425\n",
      "Iteration 19, loss = 0.69677174\n",
      "Iteration 20, loss = 0.69773133\n",
      "Iteration 21, loss = 0.69840966\n",
      "Iteration 22, loss = 0.69876167\n",
      "Iteration 23, loss = 0.69878966\n",
      "Iteration 24, loss = 0.69853107\n",
      "Iteration 25, loss = 0.69804604\n",
      "Iteration 26, loss = 0.69740599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69950619\n",
      "Iteration 2, loss = 3.93706250\n",
      "Iteration 3, loss = 0.79840272\n",
      "Iteration 4, loss = 0.82261861\n",
      "Iteration 5, loss = 0.82783048\n",
      "Iteration 6, loss = 0.81087747\n",
      "Iteration 7, loss = 0.77500945\n",
      "Iteration 8, loss = 0.73308759\n",
      "Iteration 9, loss = 0.72130170\n",
      "Iteration 10, loss = 0.71156938\n",
      "Iteration 11, loss = 0.70407070\n",
      "Iteration 12, loss = 0.69875316\n",
      "Iteration 13, loss = 0.69539605\n",
      "Iteration 14, loss = 0.69367282\n",
      "Iteration 15, loss = 0.69320637\n",
      "Iteration 16, loss = 0.69361454\n",
      "Iteration 17, loss = 0.69454434\n",
      "Iteration 18, loss = 0.69569498\n",
      "Iteration 19, loss = 0.69683052\n",
      "Iteration 20, loss = 0.69778379\n",
      "Iteration 21, loss = 0.69845337\n",
      "Iteration 22, loss = 0.69879580\n",
      "Iteration 23, loss = 0.69881460\n",
      "Iteration 24, loss = 0.69854805\n",
      "Iteration 25, loss = 0.69805675\n",
      "Iteration 26, loss = 0.69741229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70032061\n",
      "Iteration 2, loss = 17.90420301\n",
      "Iteration 3, loss = 17.90427239\n",
      "Iteration 4, loss = 0.72053226\n",
      "Iteration 5, loss = 0.73375833\n",
      "Iteration 6, loss = 0.75287533\n",
      "Iteration 7, loss = 0.75223451\n",
      "Iteration 8, loss = 0.75642641\n",
      "Iteration 9, loss = 0.75756971\n",
      "Iteration 10, loss = 0.75625750\n",
      "Iteration 11, loss = 0.75307751\n",
      "Iteration 12, loss = 0.74856758\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69963946\n",
      "Iteration 2, loss = 18.13978178\n",
      "Iteration 3, loss = 18.13984916\n",
      "Iteration 4, loss = 0.71783544\n",
      "Iteration 5, loss = 0.73067628\n",
      "Iteration 6, loss = 0.74934384\n",
      "Iteration 7, loss = 0.74925398\n",
      "Iteration 8, loss = 0.75376900\n",
      "Iteration 9, loss = 0.75531964\n",
      "Iteration 10, loss = 0.75444534\n",
      "Iteration 11, loss = 0.75169673\n",
      "Iteration 12, loss = 0.74758716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69950619\n",
      "Iteration 2, loss = 18.02199160\n",
      "Iteration 3, loss = 18.02206016\n",
      "Iteration 4, loss = 0.71916338\n",
      "Iteration 5, loss = 0.73217830\n",
      "Iteration 6, loss = 0.75091189\n",
      "Iteration 7, loss = 0.75072032\n",
      "Iteration 8, loss = 0.75508116\n",
      "Iteration 9, loss = 0.75643653\n",
      "Iteration 10, loss = 0.75535216\n",
      "Iteration 11, loss = 0.75239685\n",
      "Iteration 12, loss = 0.74809588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85294991\n",
      "Iteration 2, loss = 0.69367443\n",
      "Iteration 3, loss = 0.69502165\n",
      "Iteration 4, loss = 0.69633447\n",
      "Iteration 5, loss = 0.69710621\n",
      "Iteration 6, loss = 0.69718087\n",
      "Iteration 7, loss = 0.69666376\n",
      "Iteration 8, loss = 0.69596426\n",
      "Iteration 9, loss = 0.69522453\n",
      "Iteration 10, loss = 0.69448298\n",
      "Iteration 11, loss = 0.69387226\n",
      "Iteration 12, loss = 0.69346143\n",
      "Iteration 13, loss = 0.69325873\n",
      "Iteration 14, loss = 0.69322710\n",
      "Iteration 15, loss = 0.69330539\n",
      "Iteration 16, loss = 0.69342869\n",
      "Iteration 17, loss = 0.69354361\n",
      "Iteration 18, loss = 0.69361653\n",
      "Iteration 19, loss = 0.69363490\n",
      "Iteration 20, loss = 0.69360360\n",
      "Iteration 21, loss = 0.69353833\n",
      "Iteration 22, loss = 0.69345876\n",
      "Iteration 23, loss = 0.69338274\n",
      "Iteration 24, loss = 0.69332275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85956724\n",
      "Iteration 2, loss = 0.69337385\n",
      "Iteration 3, loss = 0.69453174\n",
      "Iteration 4, loss = 0.69587855\n",
      "Iteration 5, loss = 0.69681917\n",
      "Iteration 6, loss = 0.69710529\n",
      "Iteration 7, loss = 0.69677025\n",
      "Iteration 8, loss = 0.69609028\n",
      "Iteration 9, loss = 0.69541757\n",
      "Iteration 10, loss = 0.69468807\n",
      "Iteration 11, loss = 0.69404643\n",
      "Iteration 12, loss = 0.69358007\n",
      "Iteration 13, loss = 0.69331601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86058326\n",
      "Iteration 2, loss = 0.69353853\n",
      "Iteration 3, loss = 0.69481023\n",
      "Iteration 4, loss = 0.69615488\n",
      "Iteration 5, loss = 0.69701878\n",
      "Iteration 6, loss = 0.69719999\n",
      "Iteration 7, loss = 0.69676968\n",
      "Iteration 8, loss = 0.69607982\n",
      "Iteration 9, loss = 0.69536729\n",
      "Iteration 10, loss = 0.69462519\n",
      "Iteration 11, loss = 0.69399333\n",
      "Iteration 12, loss = 0.69355064\n",
      "Iteration 13, loss = 0.69331499\n",
      "Iteration 14, loss = 0.69325672\n",
      "Iteration 15, loss = 0.69331871\n",
      "Iteration 16, loss = 0.69343676\n",
      "Iteration 17, loss = 0.69355556\n",
      "Iteration 18, loss = 0.69363804\n",
      "Iteration 19, loss = 0.69366792\n",
      "Iteration 20, loss = 0.69364690\n",
      "Iteration 21, loss = 0.69358869\n",
      "Iteration 22, loss = 0.69351218\n",
      "Iteration 23, loss = 0.69343560\n",
      "Iteration 24, loss = 0.69337250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85294991\n",
      "Iteration 2, loss = 0.70606650\n",
      "Iteration 3, loss = 0.72065509\n",
      "Iteration 4, loss = 0.72278677\n",
      "Iteration 5, loss = 0.71683023\n",
      "Iteration 6, loss = 0.70883593\n",
      "Iteration 7, loss = 12.92883389\n",
      "Iteration 8, loss = 1.22976721\n",
      "Iteration 9, loss = 0.70550152\n",
      "Iteration 10, loss = 0.70897738\n",
      "Iteration 11, loss = 0.71180370\n",
      "Iteration 12, loss = 0.71550827\n",
      "Iteration 13, loss = 0.71866227\n",
      "Iteration 14, loss = 0.72011609\n",
      "Iteration 15, loss = 0.71948262\n",
      "Iteration 16, loss = 0.71712000\n",
      "Iteration 17, loss = 0.71353458\n",
      "Iteration 18, loss = 0.71184222\n",
      "Iteration 19, loss = 0.71001415\n",
      "Iteration 20, loss = 0.70814022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85956724\n",
      "Iteration 2, loss = 0.70405654\n",
      "Iteration 3, loss = 0.71882788\n",
      "Iteration 4, loss = 0.72176956\n",
      "Iteration 5, loss = 0.71669453\n",
      "Iteration 6, loss = 0.70914888\n",
      "Iteration 7, loss = 12.92176663\n",
      "Iteration 8, loss = 1.22014891\n",
      "Iteration 9, loss = 0.70556472\n",
      "Iteration 10, loss = 0.70898274\n",
      "Iteration 11, loss = 0.71173106\n",
      "Iteration 12, loss = 0.71554342\n",
      "Iteration 13, loss = 0.71868647\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86058326\n",
      "Iteration 2, loss = 0.70506110\n",
      "Iteration 3, loss = 0.71978585\n",
      "Iteration 4, loss = 0.72235953\n",
      "Iteration 5, loss = 0.71685251\n",
      "Iteration 6, loss = 0.70910131\n",
      "Iteration 7, loss = 12.94850744\n",
      "Iteration 8, loss = 1.22653539\n",
      "Iteration 9, loss = 0.70559519\n",
      "Iteration 10, loss = 0.70903452\n",
      "Iteration 11, loss = 0.71180916\n",
      "Iteration 12, loss = 0.71555817\n",
      "Iteration 13, loss = 0.71869604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94758561\n",
      "Iteration 2, loss = 0.73624711\n",
      "Iteration 3, loss = 0.80386083\n",
      "Iteration 4, loss = 8.38257220\n",
      "Iteration 5, loss = 0.72975938\n",
      "Iteration 6, loss = 0.73702511\n",
      "Iteration 7, loss = 0.73783533\n",
      "Iteration 8, loss = 0.73230311\n",
      "Iteration 9, loss = 0.72216763\n",
      "Iteration 10, loss = 0.71039199\n",
      "Iteration 11, loss = 0.70029412\n",
      "Iteration 12, loss = 0.69444073\n",
      "Iteration 13, loss = 0.69327239\n",
      "Iteration 14, loss = 0.69488917\n",
      "Iteration 15, loss = 0.69787669\n",
      "Iteration 16, loss = 0.70048856\n",
      "Iteration 17, loss = 0.70140821\n",
      "Iteration 18, loss = 0.70035681\n",
      "Iteration 19, loss = 0.69802231\n",
      "Iteration 20, loss = 0.69554464\n",
      "Iteration 21, loss = 0.69386916\n",
      "Iteration 22, loss = 0.69332840\n",
      "Iteration 23, loss = 0.69363634\n",
      "Iteration 24, loss = 0.69420963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95702996\n",
      "Iteration 2, loss = 0.73837795\n",
      "Iteration 3, loss = 0.80822559\n",
      "Iteration 4, loss = 8.61680562\n",
      "Iteration 5, loss = 0.73114292\n",
      "Iteration 6, loss = 0.73812575\n",
      "Iteration 7, loss = 0.73866061\n",
      "Iteration 8, loss = 0.73289891\n",
      "Iteration 9, loss = 0.72257683\n",
      "Iteration 10, loss = 0.71064555\n",
      "Iteration 11, loss = 0.70042095\n",
      "Iteration 12, loss = 0.69457044\n",
      "Iteration 13, loss = 0.69326520\n",
      "Iteration 14, loss = 0.69483488\n",
      "Iteration 15, loss = 0.69782138\n",
      "Iteration 16, loss = 0.70039494\n",
      "Iteration 17, loss = 0.70124851\n",
      "Iteration 18, loss = 0.70032475\n",
      "Iteration 19, loss = 0.69815906\n",
      "Iteration 20, loss = 0.69573752\n",
      "Iteration 21, loss = 0.69400237\n",
      "Iteration 22, loss = 0.69334721\n",
      "Iteration 23, loss = 0.69356415\n",
      "Iteration 24, loss = 0.69411552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95214260\n",
      "Iteration 2, loss = 0.73138729\n",
      "Iteration 3, loss = 0.80727478\n",
      "Iteration 4, loss = 7.54330562\n",
      "Iteration 5, loss = 0.72724727\n",
      "Iteration 6, loss = 0.73305408\n",
      "Iteration 7, loss = 0.73344467\n",
      "Iteration 8, loss = 0.72874039\n",
      "Iteration 9, loss = 0.72038007\n",
      "Iteration 10, loss = 0.71056859\n",
      "Iteration 11, loss = 0.70170027\n",
      "Iteration 12, loss = 0.69567183\n",
      "Iteration 13, loss = 0.69348029\n",
      "Iteration 14, loss = 0.69357446\n",
      "Iteration 15, loss = 0.69528803\n",
      "Iteration 16, loss = 0.69753095\n",
      "Iteration 17, loss = 0.69921499\n",
      "Iteration 18, loss = 0.69967084\n",
      "Iteration 19, loss = 0.69883632\n",
      "Iteration 20, loss = 0.69716914\n",
      "Iteration 21, loss = 0.69536199\n",
      "Iteration 22, loss = 0.69401170\n",
      "Iteration 23, loss = 0.69339422\n",
      "Iteration 24, loss = 0.69342732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94758561\n",
      "Iteration 2, loss = 18.13980955\n",
      "Iteration 3, loss = 17.53180376\n",
      "Iteration 4, loss = 0.69765242\n",
      "Iteration 5, loss = 0.70005769\n",
      "Iteration 6, loss = 0.70210545\n",
      "Iteration 7, loss = 0.70343001\n",
      "Iteration 8, loss = 0.70434450\n",
      "Iteration 9, loss = 0.70531009\n",
      "Iteration 10, loss = 0.70690680\n",
      "Iteration 11, loss = 0.70800968\n",
      "Iteration 12, loss = 0.70861905\n",
      "Iteration 13, loss = 0.70876350\n",
      "Iteration 14, loss = 0.70849281\n",
      "Iteration 15, loss = 0.70787235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95702996\n",
      "Iteration 2, loss = 17.90425644\n",
      "Iteration 3, loss = 17.28218154\n",
      "Iteration 4, loss = 0.69887644\n",
      "Iteration 5, loss = 0.70146994\n",
      "Iteration 6, loss = 0.70347939\n",
      "Iteration 7, loss = 0.70487005\n",
      "Iteration 8, loss = 0.70569353\n",
      "Iteration 9, loss = 0.70672759\n",
      "Iteration 10, loss = 0.70826804\n",
      "Iteration 11, loss = 0.70927363\n",
      "Iteration 12, loss = 0.70975785\n",
      "Iteration 13, loss = 0.70976059\n",
      "Iteration 14, loss = 0.70934134\n",
      "Iteration 15, loss = 0.70857374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95214260\n",
      "Iteration 2, loss = 18.02204473\n",
      "Iteration 3, loss = 18.01777915\n",
      "Iteration 4, loss = 0.69832376\n",
      "Iteration 5, loss = 0.70084868\n",
      "Iteration 6, loss = 0.70285465\n",
      "Iteration 7, loss = 0.70428249\n",
      "Iteration 8, loss = 0.70516645\n",
      "Iteration 9, loss = 0.70617133\n",
      "Iteration 10, loss = 0.70774507\n",
      "Iteration 11, loss = 0.70880151\n",
      "Iteration 12, loss = 0.70934813\n",
      "Iteration 13, loss = 0.70941955\n",
      "Iteration 14, loss = 0.70907070\n",
      "Iteration 15, loss = 0.70837133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69446993\n",
      "Iteration 2, loss = 0.69425750\n",
      "Iteration 3, loss = 0.69402129\n",
      "Iteration 4, loss = 0.69384501\n",
      "Iteration 5, loss = 0.69374886\n",
      "Iteration 6, loss = 0.69370282\n",
      "Iteration 7, loss = 0.69363152\n",
      "Iteration 8, loss = 0.69354695\n",
      "Iteration 9, loss = 0.69350308\n",
      "Iteration 10, loss = 0.69343503\n",
      "Iteration 11, loss = 0.69334043\n",
      "Iteration 12, loss = 0.69328813\n",
      "Iteration 13, loss = 0.69323930\n",
      "Iteration 14, loss = 0.69319113\n",
      "Iteration 15, loss = 0.69314096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69468780\n",
      "Iteration 2, loss = 0.69457484\n",
      "Iteration 3, loss = 0.69446964\n",
      "Iteration 4, loss = 0.69439093\n",
      "Iteration 5, loss = 0.69433842\n",
      "Iteration 6, loss = 0.69429598\n",
      "Iteration 7, loss = 0.69426436\n",
      "Iteration 8, loss = 0.69424284\n",
      "Iteration 9, loss = 0.69421965\n",
      "Iteration 10, loss = 0.69418847\n",
      "Iteration 11, loss = 0.69414910\n",
      "Iteration 12, loss = 0.69410635\n",
      "Iteration 13, loss = 0.69406487\n",
      "Iteration 14, loss = 0.69402691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69475916\n",
      "Iteration 2, loss = 0.69459222\n",
      "Iteration 3, loss = 0.69443198\n",
      "Iteration 4, loss = 0.69430927\n",
      "Iteration 5, loss = 0.69423804\n",
      "Iteration 6, loss = 0.69417633\n",
      "Iteration 7, loss = 0.69409372\n",
      "Iteration 8, loss = 0.69394878\n",
      "Iteration 9, loss = 0.69383863\n",
      "Iteration 10, loss = 0.69375153\n",
      "Iteration 11, loss = 0.69370292\n",
      "Iteration 12, loss = 0.69366318\n",
      "Iteration 13, loss = 0.69362210\n",
      "Iteration 14, loss = 0.69357702\n",
      "Iteration 15, loss = 0.69352941\n",
      "Iteration 16, loss = 0.69347988\n",
      "Iteration 17, loss = 0.69342896\n",
      "Iteration 18, loss = 0.69337745\n",
      "Iteration 19, loss = 0.69332615\n",
      "Iteration 20, loss = 0.69327466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69446993\n",
      "Iteration 2, loss = 0.74937275\n",
      "Iteration 3, loss = 1.03644799\n",
      "Iteration 4, loss = 0.72355674\n",
      "Iteration 5, loss = 0.77970808\n",
      "Iteration 6, loss = 0.71560129\n",
      "Iteration 7, loss = 0.72284223\n",
      "Iteration 8, loss = 0.71871137\n",
      "Iteration 9, loss = 0.69475093\n",
      "Iteration 10, loss = 0.72194992\n",
      "Iteration 11, loss = 0.70142423\n",
      "Iteration 12, loss = 0.69795990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69468780\n",
      "Iteration 2, loss = 1.02721392\n",
      "Iteration 3, loss = 0.73549376\n",
      "Iteration 4, loss = 0.70346467\n",
      "Iteration 5, loss = 0.70351432\n",
      "Iteration 6, loss = 0.70352826\n",
      "Iteration 7, loss = 0.70351685\n",
      "Iteration 8, loss = 0.70348621\n",
      "Iteration 9, loss = 0.70344041\n",
      "Iteration 10, loss = 0.70338237\n",
      "Iteration 11, loss = 0.70331428\n",
      "Iteration 12, loss = 0.70323785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69475916\n",
      "Iteration 2, loss = 0.74834325\n",
      "Iteration 3, loss = 1.06779565\n",
      "Iteration 4, loss = 0.73385755\n",
      "Iteration 5, loss = 0.81524149\n",
      "Iteration 6, loss = 0.72821575\n",
      "Iteration 7, loss = 0.72897520\n",
      "Iteration 8, loss = 0.73884949\n",
      "Iteration 9, loss = 0.69335582\n",
      "Iteration 10, loss = 0.72744827\n",
      "Iteration 11, loss = 0.72005622\n",
      "Iteration 12, loss = 0.69278377\n",
      "Iteration 13, loss = 0.71766655\n",
      "Iteration 14, loss = 0.71105663\n",
      "Iteration 15, loss = 0.69198075\n",
      "Iteration 16, loss = 0.70957203\n",
      "Iteration 17, loss = 0.70588151\n",
      "Iteration 18, loss = 0.69103395\n",
      "Iteration 19, loss = 0.70325445\n",
      "Iteration 20, loss = 0.70168971\n",
      "Iteration 21, loss = 0.69012141\n",
      "Iteration 22, loss = 0.69872365\n",
      "Iteration 23, loss = 0.69807100\n",
      "Iteration 24, loss = 0.68915916\n",
      "Iteration 25, loss = 0.69539528\n",
      "Iteration 26, loss = 0.69478029\n",
      "Iteration 27, loss = 0.68812234\n",
      "Iteration 28, loss = 0.69290691\n",
      "Iteration 29, loss = 0.69182172\n",
      "Iteration 30, loss = 0.68706731\n",
      "Iteration 31, loss = 0.69086242\n",
      "Iteration 32, loss = 0.68912857\n",
      "Iteration 33, loss = 0.68606860\n",
      "Iteration 34, loss = 0.68900600\n",
      "Iteration 35, loss = 0.68670599\n",
      "Iteration 36, loss = 0.68515621\n",
      "Iteration 37, loss = 0.68713371\n",
      "Iteration 38, loss = 0.68457239\n",
      "Iteration 39, loss = 0.68429875\n",
      "Iteration 40, loss = 0.68513679\n",
      "Iteration 41, loss = 0.68280417\n",
      "Iteration 42, loss = 0.68335398\n",
      "Iteration 43, loss = 0.68303478\n",
      "Iteration 44, loss = 0.68142401\n",
      "Iteration 45, loss = 0.68214581\n",
      "Iteration 46, loss = 0.68098762\n",
      "Iteration 47, loss = 0.68032766\n",
      "Iteration 48, loss = 0.68056513\n",
      "Iteration 49, loss = 0.67921682\n",
      "Iteration 50, loss = 0.67924472\n",
      "Iteration 51, loss = 0.67871590\n",
      "Iteration 52, loss = 0.67781793\n",
      "Iteration 53, loss = 0.67788007\n",
      "Iteration 54, loss = 0.67690274\n",
      "Iteration 55, loss = 0.67660057\n",
      "Iteration 56, loss = 0.67617997\n",
      "Iteration 57, loss = 0.67537302\n",
      "Iteration 58, loss = 0.67521564\n",
      "Iteration 59, loss = 0.67441440\n",
      "Iteration 60, loss = 0.67402801\n",
      "Iteration 61, loss = 0.67353787\n",
      "Iteration 62, loss = 0.67285783\n",
      "Iteration 63, loss = 0.67254002\n",
      "Iteration 64, loss = 0.67181330\n",
      "Iteration 65, loss = 0.67141218\n",
      "Iteration 66, loss = 0.67082673\n",
      "Iteration 67, loss = 0.67025498\n",
      "Iteration 68, loss = 0.66979938\n",
      "Iteration 69, loss = 0.66913770\n",
      "Iteration 70, loss = 0.66869891\n",
      "Iteration 71, loss = 0.66805727\n",
      "Iteration 72, loss = 0.66755116\n",
      "Iteration 73, loss = 0.66697624\n",
      "Iteration 74, loss = 0.66639231\n",
      "Iteration 75, loss = 0.66586579\n",
      "Iteration 76, loss = 0.66523932\n",
      "Iteration 77, loss = 0.66471937\n",
      "Iteration 78, loss = 0.66408938\n",
      "Iteration 79, loss = 0.66354491\n",
      "Iteration 80, loss = 0.66293164\n",
      "Iteration 81, loss = 0.66235270\n",
      "Iteration 82, loss = 0.66175699\n",
      "Iteration 83, loss = 0.66114893\n",
      "Iteration 84, loss = 0.66056145\n",
      "Iteration 85, loss = 0.65993490\n",
      "Iteration 86, loss = 0.65934497\n",
      "Iteration 87, loss = 0.65870909\n",
      "Iteration 88, loss = 0.65810930\n",
      "Iteration 89, loss = 0.65746924\n",
      "Iteration 90, loss = 0.65685610\n",
      "Iteration 91, loss = 0.65621350\n",
      "Iteration 92, loss = 0.65558650\n",
      "Iteration 93, loss = 0.65494076\n",
      "Iteration 94, loss = 0.65430080\n",
      "Iteration 95, loss = 0.65365048\n",
      "Iteration 96, loss = 0.65299894\n",
      "Iteration 97, loss = 0.65234249\n",
      "Iteration 98, loss = 0.65168052\n",
      "Iteration 99, loss = 0.65101667\n",
      "Iteration 100, loss = 0.65034514\n",
      "Iteration 101, loss = 0.64967301\n",
      "Iteration 102, loss = 0.64899237\n",
      "Iteration 103, loss = 0.64831133\n",
      "Iteration 104, loss = 0.64762183\n",
      "Iteration 105, loss = 0.64693150\n",
      "Iteration 106, loss = 0.64623315\n",
      "Iteration 107, loss = 0.64553325\n",
      "Iteration 108, loss = 0.64482601\n",
      "Iteration 109, loss = 0.64411635\n",
      "Iteration 110, loss = 0.64340005\n",
      "Iteration 111, loss = 0.64268044\n",
      "Iteration 112, loss = 0.64195497\n",
      "Iteration 113, loss = 0.64122527\n",
      "Iteration 114, loss = 0.64049038\n",
      "Iteration 115, loss = 0.63975044\n",
      "Iteration 116, loss = 0.63900594\n",
      "Iteration 117, loss = 0.63825566\n",
      "Iteration 118, loss = 0.63750124\n",
      "Iteration 119, loss = 0.63674052\n",
      "Iteration 120, loss = 0.63597591\n",
      "Iteration 121, loss = 0.63520468\n",
      "Iteration 122, loss = 0.63442951\n",
      "Iteration 123, loss = 0.63364771\n",
      "Iteration 124, loss = 0.63286166\n",
      "Iteration 125, loss = 0.63206919\n",
      "Iteration 126, loss = 0.63127192\n",
      "Iteration 127, loss = 0.63046866\n",
      "Iteration 128, loss = 0.62965992\n",
      "Iteration 129, loss = 0.62884559\n",
      "Iteration 130, loss = 0.62802523\n",
      "Iteration 131, loss = 0.62719950\n",
      "Iteration 132, loss = 0.62636742\n",
      "Iteration 133, loss = 0.62552988\n",
      "Iteration 134, loss = 0.62468598\n",
      "Iteration 135, loss = 0.62383627\n",
      "Iteration 136, loss = 0.62298037\n",
      "Iteration 137, loss = 0.62211821\n",
      "Iteration 138, loss = 0.62125001\n",
      "Iteration 139, loss = 0.62037525\n",
      "Iteration 140, loss = 0.61949433\n",
      "Iteration 141, loss = 0.61860682\n",
      "Iteration 142, loss = 0.61771284\n",
      "Iteration 143, loss = 0.61681233\n",
      "Iteration 144, loss = 0.61590504\n",
      "Iteration 145, loss = 0.61499116\n",
      "Iteration 146, loss = 0.61407037\n",
      "Iteration 147, loss = 0.61314274\n",
      "Iteration 148, loss = 0.61220821\n",
      "Iteration 149, loss = 0.61126656\n",
      "Iteration 150, loss = 0.61031789\n",
      "Iteration 151, loss = 0.60936202\n",
      "Iteration 152, loss = 0.60839887\n",
      "Iteration 153, loss = 0.60742844\n",
      "Iteration 154, loss = 0.60645055\n",
      "Iteration 155, loss = 0.60546519\n",
      "Iteration 156, loss = 0.60447228\n",
      "Iteration 157, loss = 0.60347168\n",
      "Iteration 158, loss = 0.60246338\n",
      "Iteration 159, loss = 0.60144726\n",
      "Iteration 160, loss = 0.60042323\n",
      "Iteration 161, loss = 0.59939124\n",
      "Iteration 162, loss = 0.59835118\n",
      "Iteration 163, loss = 0.59730297\n",
      "Iteration 164, loss = 0.59624654\n",
      "Iteration 165, loss = 0.59518179\n",
      "Iteration 166, loss = 0.59410863\n",
      "Iteration 167, loss = 0.59302699\n",
      "Iteration 168, loss = 0.59193678\n",
      "Iteration 169, loss = 0.59083790\n",
      "Iteration 170, loss = 0.58973027\n",
      "Iteration 171, loss = 0.58861382\n",
      "Iteration 172, loss = 0.58748844\n",
      "Iteration 173, loss = 0.58635404\n",
      "Iteration 174, loss = 0.58521055\n",
      "Iteration 175, loss = 0.58405788\n",
      "Iteration 176, loss = 0.58289592\n",
      "Iteration 177, loss = 0.58172460\n",
      "Iteration 178, loss = 0.58054383\n",
      "Iteration 179, loss = 0.57935345\n",
      "Iteration 180, loss = 0.59925026\n",
      "Iteration 181, loss = 0.77544390\n",
      "Iteration 182, loss = 0.71283830\n",
      "Iteration 183, loss = 0.58823916\n",
      "Iteration 184, loss = 0.74471171\n",
      "Iteration 185, loss = 0.57739725\n",
      "Iteration 186, loss = 0.69062014\n",
      "Iteration 187, loss = 0.59265136\n",
      "Iteration 188, loss = 0.63992958\n",
      "Iteration 189, loss = 0.60357359\n",
      "Iteration 190, loss = 0.60807640\n",
      "Iteration 191, loss = 0.60728885\n",
      "Iteration 192, loss = 0.58994531\n",
      "Iteration 193, loss = 0.60604373\n",
      "Iteration 194, loss = 1.12217977\n",
      "Iteration 195, loss = 1.35005702\n",
      "Iteration 196, loss = 1.19952479\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69941293\n",
      "Iteration 2, loss = 0.69567464\n",
      "Iteration 3, loss = 0.69583813\n",
      "Iteration 4, loss = 0.69483372\n",
      "Iteration 5, loss = 0.69443240\n",
      "Iteration 6, loss = 0.69426419\n",
      "Iteration 7, loss = 0.69400176\n",
      "Iteration 8, loss = 0.69378645\n",
      "Iteration 9, loss = 0.69355803\n",
      "Iteration 10, loss = 0.69336822\n",
      "Iteration 11, loss = 0.69311648\n",
      "Iteration 12, loss = 0.69290876\n",
      "Iteration 13, loss = 0.69266484\n",
      "Iteration 14, loss = 0.69238678\n",
      "Iteration 15, loss = 0.69215496\n",
      "Iteration 16, loss = 0.69187766\n",
      "Iteration 17, loss = 0.69156908\n",
      "Iteration 18, loss = 0.69091115\n",
      "Iteration 19, loss = 0.69067830\n",
      "Iteration 20, loss = 0.69037734\n",
      "Iteration 21, loss = 0.69005201\n",
      "Iteration 22, loss = 0.68977029\n",
      "Iteration 23, loss = 0.68972441\n",
      "Iteration 24, loss = 0.68918093\n",
      "Iteration 25, loss = 0.68887203\n",
      "Iteration 26, loss = 0.68853468\n",
      "Iteration 27, loss = 0.68836893\n",
      "Iteration 28, loss = 0.68806862\n",
      "Iteration 29, loss = 0.68750225\n",
      "Iteration 30, loss = 0.68713803\n",
      "Iteration 31, loss = 0.68661884\n",
      "Iteration 32, loss = 0.68634838\n",
      "Iteration 33, loss = 0.68604127\n",
      "Iteration 34, loss = 0.68568648\n",
      "Iteration 35, loss = 0.68523169\n",
      "Iteration 36, loss = 0.68490913\n",
      "Iteration 37, loss = 0.68454143\n",
      "Iteration 38, loss = 0.68431898\n",
      "Iteration 39, loss = 0.68406696\n",
      "Iteration 40, loss = 0.68346466\n",
      "Iteration 41, loss = 0.68310575\n",
      "Iteration 42, loss = 0.68283188\n",
      "Iteration 43, loss = 0.68251148\n",
      "Iteration 44, loss = 0.68218186\n",
      "Iteration 45, loss = 0.68176459\n",
      "Iteration 46, loss = 0.68144875\n",
      "Iteration 47, loss = 0.68102333\n",
      "Iteration 48, loss = 0.68072076\n",
      "Iteration 49, loss = 0.68028339\n",
      "Iteration 50, loss = 0.67989696\n",
      "Iteration 51, loss = 0.67951068\n",
      "Iteration 52, loss = 0.67925999\n",
      "Iteration 53, loss = 0.67893522\n",
      "Iteration 54, loss = 0.67850739\n",
      "Iteration 55, loss = 0.67812554\n",
      "Iteration 56, loss = 0.67760386\n",
      "Iteration 57, loss = 0.67731100\n",
      "Iteration 58, loss = 0.67687730\n",
      "Iteration 59, loss = 0.67655055\n",
      "Iteration 60, loss = 0.67608924\n",
      "Iteration 61, loss = 0.67596973\n",
      "Iteration 62, loss = 0.67559278\n",
      "Iteration 63, loss = 0.67499790\n",
      "Iteration 64, loss = 0.67455178\n",
      "Iteration 65, loss = 0.67422504\n",
      "Iteration 66, loss = 0.67392203\n",
      "Iteration 67, loss = 0.67348861\n",
      "Iteration 68, loss = 0.67314420\n",
      "Iteration 69, loss = 0.67271539\n",
      "Iteration 70, loss = 0.67224954\n",
      "Iteration 71, loss = 0.67196319\n",
      "Iteration 72, loss = 0.67170431\n",
      "Iteration 73, loss = 0.67114205\n",
      "Iteration 74, loss = 0.67068542\n",
      "Iteration 75, loss = 0.67030737\n",
      "Iteration 76, loss = 0.66989303\n",
      "Iteration 77, loss = 0.66953491\n",
      "Iteration 78, loss = 0.66918601\n",
      "Iteration 79, loss = 0.66870816\n",
      "Iteration 80, loss = 0.66833339\n",
      "Iteration 81, loss = 0.66788018\n",
      "Iteration 82, loss = 0.66750618\n",
      "Iteration 83, loss = 0.66704825\n",
      "Iteration 84, loss = 0.66664942\n",
      "Iteration 85, loss = 0.66624995\n",
      "Iteration 86, loss = 0.66586168\n",
      "Iteration 87, loss = 0.66550471\n",
      "Iteration 88, loss = 0.66510624\n",
      "Iteration 89, loss = 0.66467920\n",
      "Iteration 90, loss = 0.66428047\n",
      "Iteration 91, loss = 0.66387745\n",
      "Iteration 92, loss = 0.66347667\n",
      "Iteration 93, loss = 0.66309451\n",
      "Iteration 94, loss = 0.66265822\n",
      "Iteration 95, loss = 0.66228075\n",
      "Iteration 96, loss = 0.66194720\n",
      "Iteration 97, loss = 0.66148276\n",
      "Iteration 98, loss = 0.66104862\n",
      "Iteration 99, loss = 0.66064709\n",
      "Iteration 100, loss = 0.66024993\n",
      "Iteration 101, loss = 0.65985820\n",
      "Iteration 102, loss = 0.65942809\n",
      "Iteration 103, loss = 0.65901245\n",
      "Iteration 104, loss = 0.65861786\n",
      "Iteration 105, loss = 0.65818857\n",
      "Iteration 106, loss = 0.65777136\n",
      "Iteration 107, loss = 0.65735866\n",
      "Iteration 108, loss = 0.65694372\n",
      "Iteration 109, loss = 0.65652974\n",
      "Iteration 110, loss = 0.65613759\n",
      "Iteration 111, loss = 0.65570410\n",
      "Iteration 112, loss = 0.65528417\n",
      "Iteration 113, loss = 0.65488644\n",
      "Iteration 114, loss = 0.65452281\n",
      "Iteration 115, loss = 0.65405653\n",
      "Iteration 116, loss = 0.65361183\n",
      "Iteration 117, loss = 0.65319159\n",
      "Iteration 118, loss = 0.65278966\n",
      "Iteration 119, loss = 0.65242141\n",
      "Iteration 120, loss = 0.65196125\n",
      "Iteration 121, loss = 0.65151094\n",
      "Iteration 122, loss = 0.65108817\n",
      "Iteration 123, loss = 0.65066606\n",
      "Iteration 124, loss = 0.65025716\n",
      "Iteration 125, loss = 0.64982373\n",
      "Iteration 126, loss = 0.64939416\n",
      "Iteration 127, loss = 0.64897952\n",
      "Iteration 128, loss = 0.64859687\n",
      "Iteration 129, loss = 0.64813896\n",
      "Iteration 130, loss = 0.64769256\n",
      "Iteration 131, loss = 0.64726919\n",
      "Iteration 132, loss = 0.64686547\n",
      "Iteration 133, loss = 0.64641735\n",
      "Iteration 134, loss = 0.64598168\n",
      "Iteration 135, loss = 0.64555137\n",
      "Iteration 136, loss = 0.64512535\n",
      "Iteration 137, loss = 0.64469140\n",
      "Iteration 138, loss = 0.64425932\n",
      "Iteration 139, loss = 0.64383063\n",
      "Iteration 140, loss = 0.64339558\n",
      "Iteration 141, loss = 0.64296015\n",
      "Iteration 142, loss = 0.64252603\n",
      "Iteration 143, loss = 0.64209191\n",
      "Iteration 144, loss = 0.64165594\n",
      "Iteration 145, loss = 0.64121944\n",
      "Iteration 146, loss = 0.64078240\n",
      "Iteration 147, loss = 0.64034582\n",
      "Iteration 148, loss = 0.63990706\n",
      "Iteration 149, loss = 0.63946771\n",
      "Iteration 150, loss = 0.63902761\n",
      "Iteration 151, loss = 0.63858665\n",
      "Iteration 152, loss = 0.63814714\n",
      "Iteration 153, loss = 0.63770909\n",
      "Iteration 154, loss = 0.63726283\n",
      "Iteration 155, loss = 0.63681703\n",
      "Iteration 156, loss = 0.63637202\n",
      "Iteration 157, loss = 0.63592799\n",
      "Iteration 158, loss = 0.63548124\n",
      "Iteration 159, loss = 0.63503378\n",
      "Iteration 160, loss = 0.63458612\n",
      "Iteration 161, loss = 0.63413832\n",
      "Iteration 162, loss = 0.63368853\n",
      "Iteration 163, loss = 0.63323797\n",
      "Iteration 164, loss = 0.63278654\n",
      "Iteration 165, loss = 0.63233413\n",
      "Iteration 166, loss = 0.63188242\n",
      "Iteration 167, loss = 0.63142808\n",
      "Iteration 168, loss = 0.63097287\n",
      "Iteration 169, loss = 0.63051727\n",
      "Iteration 170, loss = 0.63006273\n",
      "Iteration 171, loss = 0.62960636\n",
      "Iteration 172, loss = 0.62914615\n",
      "Iteration 173, loss = 0.62868586\n",
      "Iteration 174, loss = 0.62822904\n",
      "Iteration 175, loss = 0.62776663\n",
      "Iteration 176, loss = 0.62730461\n",
      "Iteration 177, loss = 0.62684490\n",
      "Iteration 178, loss = 0.62638272\n",
      "Iteration 179, loss = 0.62591397\n",
      "Iteration 180, loss = 0.62544802\n",
      "Iteration 181, loss = 0.62498115\n",
      "Iteration 182, loss = 0.62450996\n",
      "Iteration 183, loss = 0.62404138\n",
      "Iteration 184, loss = 0.62357199\n",
      "Iteration 185, loss = 0.62310291\n",
      "Iteration 186, loss = 0.62262754\n",
      "Iteration 187, loss = 0.62215488\n",
      "Iteration 188, loss = 0.62168210\n",
      "Iteration 189, loss = 0.62120782\n",
      "Iteration 190, loss = 0.62072900\n",
      "Iteration 191, loss = 0.62026003\n",
      "Iteration 192, loss = 0.61980649\n",
      "Iteration 193, loss = 0.61932719\n",
      "Iteration 194, loss = 0.61881994\n",
      "Iteration 195, loss = 0.61834380\n",
      "Iteration 196, loss = 0.61787802\n",
      "Iteration 197, loss = 0.61737720\n",
      "Iteration 198, loss = 0.61688010\n",
      "Iteration 199, loss = 0.61639804\n",
      "Iteration 200, loss = 0.61590812\n",
      "Iteration 201, loss = 0.61542948\n",
      "Iteration 202, loss = 0.61495774\n",
      "Iteration 203, loss = 0.61445729\n",
      "Iteration 204, loss = 0.61395643\n",
      "Iteration 205, loss = 0.61345681\n",
      "Iteration 206, loss = 0.61295592\n",
      "Iteration 207, loss = 0.61246050\n",
      "Iteration 208, loss = 0.61196239\n",
      "Iteration 209, loss = 0.61147154\n",
      "Iteration 210, loss = 0.61097086\n",
      "Iteration 211, loss = 0.61047344\n",
      "Iteration 212, loss = 0.60997379\n",
      "Iteration 213, loss = 0.60946626\n",
      "Iteration 214, loss = 0.60896159\n",
      "Iteration 215, loss = 0.60845410\n",
      "Iteration 216, loss = 0.60794592\n",
      "Iteration 217, loss = 0.60743670\n",
      "Iteration 218, loss = 0.60692651\n",
      "Iteration 219, loss = 0.60641506\n",
      "Iteration 220, loss = 0.60590231\n",
      "Iteration 221, loss = 0.60538825\n",
      "Iteration 222, loss = 0.60487287\n",
      "Iteration 223, loss = 0.60435679\n",
      "Iteration 224, loss = 0.60383885\n",
      "Iteration 225, loss = 0.60331930\n",
      "Iteration 226, loss = 0.60279974\n",
      "Iteration 227, loss = 0.60227697\n",
      "Iteration 228, loss = 0.60175316\n",
      "Iteration 229, loss = 0.60122802\n",
      "Iteration 230, loss = 0.60070287\n",
      "Iteration 231, loss = 0.60017499\n",
      "Iteration 232, loss = 0.59964697\n",
      "Iteration 233, loss = 0.59911623\n",
      "Iteration 234, loss = 0.59858413\n",
      "Iteration 235, loss = 0.59805229\n",
      "Iteration 236, loss = 0.59752322\n",
      "Iteration 237, loss = 0.59698543\n",
      "Iteration 238, loss = 0.59644745\n",
      "Iteration 239, loss = 0.59590756\n",
      "Iteration 240, loss = 0.59536972\n",
      "Iteration 241, loss = 0.59483024\n",
      "Iteration 242, loss = 0.59429558\n",
      "Iteration 243, loss = 0.59374632\n",
      "Iteration 244, loss = 0.59319764\n",
      "Iteration 245, loss = 0.59264962\n",
      "Iteration 246, loss = 0.59210116\n",
      "Iteration 247, loss = 0.59155675\n",
      "Iteration 248, loss = 0.59100949\n",
      "Iteration 249, loss = 0.59045286\n",
      "Iteration 250, loss = 0.58989305\n",
      "Iteration 251, loss = 0.58934245\n",
      "Iteration 252, loss = 0.58878936\n",
      "Iteration 253, loss = 0.58822750\n",
      "Iteration 254, loss = 0.58766115\n",
      "Iteration 255, loss = 0.58709517\n",
      "Iteration 256, loss = 0.58652940\n",
      "Iteration 257, loss = 0.58596317\n",
      "Iteration 258, loss = 0.58540045\n",
      "Iteration 259, loss = 0.58483451\n",
      "Iteration 260, loss = 0.58426903\n",
      "Iteration 261, loss = 0.58370036\n",
      "Iteration 262, loss = 0.58313641\n",
      "Iteration 263, loss = 0.58257244\n",
      "Iteration 264, loss = 0.58202009\n",
      "Iteration 265, loss = 0.58150770\n",
      "Iteration 266, loss = 0.58105073\n",
      "Iteration 267, loss = 0.58069500\n",
      "Iteration 268, loss = 0.58026428\n",
      "Iteration 269, loss = 0.57978083\n",
      "Iteration 270, loss = 0.57938850\n",
      "Iteration 271, loss = 0.57873126\n",
      "Iteration 272, loss = 0.57813498\n",
      "Iteration 273, loss = 0.57749184\n",
      "Iteration 274, loss = 0.57693370\n",
      "Iteration 275, loss = 0.57631294\n",
      "Iteration 276, loss = 0.57576977\n",
      "Iteration 277, loss = 0.57517878\n",
      "Iteration 278, loss = 0.57470444\n",
      "Iteration 279, loss = 0.57411927\n",
      "Iteration 280, loss = 0.57362312\n",
      "Iteration 281, loss = 0.57306219\n",
      "Iteration 282, loss = 0.57266090\n",
      "Iteration 283, loss = 0.57207485\n",
      "Iteration 284, loss = 0.57177337\n",
      "Iteration 285, loss = 0.57119169\n",
      "Iteration 286, loss = 0.57059237\n",
      "Iteration 287, loss = 0.57000651\n",
      "Iteration 288, loss = 0.56966021\n",
      "Iteration 289, loss = 0.56917970\n",
      "Iteration 290, loss = 0.56917496\n",
      "Iteration 291, loss = 0.56962508\n",
      "Iteration 292, loss = 0.57086916\n",
      "Iteration 293, loss = 0.57289917\n",
      "Iteration 294, loss = 0.57710806\n",
      "Iteration 295, loss = 0.58272556\n",
      "Iteration 296, loss = 0.60066340\n",
      "Iteration 297, loss = 0.63169401\n",
      "Iteration 298, loss = 0.69307759\n",
      "Iteration 299, loss = 0.76064246\n",
      "Iteration 300, loss = 0.60638595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70286440\n",
      "Iteration 2, loss = 0.69797517\n",
      "Iteration 3, loss = 0.69841660\n",
      "Iteration 4, loss = 0.69708239\n",
      "Iteration 5, loss = 0.69676731\n",
      "Iteration 6, loss = 0.69663384\n",
      "Iteration 7, loss = 0.69641918\n",
      "Iteration 8, loss = 0.69617015\n",
      "Iteration 9, loss = 0.69599852\n",
      "Iteration 10, loss = 0.69583720\n",
      "Iteration 11, loss = 0.69558395\n",
      "Iteration 12, loss = 0.69538113\n",
      "Iteration 13, loss = 0.69517385\n",
      "Iteration 14, loss = 0.69497804\n",
      "Iteration 15, loss = 0.69471570\n",
      "Iteration 16, loss = 0.69447409\n",
      "Iteration 17, loss = 0.69417641\n",
      "Iteration 18, loss = 0.69375360\n",
      "Iteration 19, loss = 0.69379329\n",
      "Iteration 20, loss = 0.69337051\n",
      "Iteration 21, loss = 0.69297831\n",
      "Iteration 22, loss = 0.69276768\n",
      "Iteration 23, loss = 0.69243536\n",
      "Iteration 24, loss = 0.69206219\n",
      "Iteration 25, loss = 0.69140007\n",
      "Iteration 26, loss = 0.69183646\n",
      "Iteration 27, loss = 0.69105654\n",
      "Iteration 28, loss = 0.69064043\n",
      "Iteration 29, loss = 0.69023851\n",
      "Iteration 30, loss = 0.69006941\n",
      "Iteration 31, loss = 0.68995523\n",
      "Iteration 32, loss = 0.68957954\n",
      "Iteration 33, loss = 0.68931472\n",
      "Iteration 34, loss = 0.68895924\n",
      "Iteration 35, loss = 0.68887596\n",
      "Iteration 36, loss = 0.68837377\n",
      "Iteration 37, loss = 0.68816791\n",
      "Iteration 38, loss = 0.68802110\n",
      "Iteration 39, loss = 0.68771019\n",
      "Iteration 40, loss = 0.68748704\n",
      "Iteration 41, loss = 0.68776728\n",
      "Iteration 42, loss = 0.68696085\n",
      "Iteration 43, loss = 0.68675645\n",
      "Iteration 44, loss = 0.68623611\n",
      "Iteration 45, loss = 0.68612298\n",
      "Iteration 46, loss = 0.68645804\n",
      "Iteration 47, loss = 0.68552125\n",
      "Iteration 48, loss = 0.68513131\n",
      "Iteration 49, loss = 0.68504746\n",
      "Iteration 50, loss = 0.68489393\n",
      "Iteration 51, loss = 0.68441282\n",
      "Iteration 52, loss = 0.68414444\n",
      "Iteration 53, loss = 0.68365933\n",
      "Iteration 54, loss = 0.68338124\n",
      "Iteration 55, loss = 0.68323279\n",
      "Iteration 56, loss = 0.68286014\n",
      "Iteration 57, loss = 0.68317218\n",
      "Iteration 58, loss = 0.68253302\n",
      "Iteration 59, loss = 0.68217839\n",
      "Iteration 60, loss = 0.68176472\n",
      "Iteration 61, loss = 0.68130051\n",
      "Iteration 62, loss = 0.68095947\n",
      "Iteration 63, loss = 0.68071299\n",
      "Iteration 64, loss = 0.68037928\n",
      "Iteration 65, loss = 0.68029963\n",
      "Iteration 66, loss = 0.67997100\n",
      "Iteration 67, loss = 0.67966098\n",
      "Iteration 68, loss = 0.67938681\n",
      "Iteration 69, loss = 0.67908233\n",
      "Iteration 70, loss = 0.67882653\n",
      "Iteration 71, loss = 0.67852935\n",
      "Iteration 72, loss = 0.67825027\n",
      "Iteration 73, loss = 0.67802964\n",
      "Iteration 74, loss = 0.67772928\n",
      "Iteration 75, loss = 0.67739433\n",
      "Iteration 76, loss = 0.67704465\n",
      "Iteration 77, loss = 0.67668414\n",
      "Iteration 78, loss = 0.67631803\n",
      "Iteration 79, loss = 0.67589859\n",
      "Iteration 80, loss = 0.67569385\n",
      "Iteration 81, loss = 0.67543396\n",
      "Iteration 82, loss = 0.67510909\n",
      "Iteration 83, loss = 0.67470503\n",
      "Iteration 84, loss = 0.67443593\n",
      "Iteration 85, loss = 0.67429219\n",
      "Iteration 86, loss = 0.67379137\n",
      "Iteration 87, loss = 0.67356279\n",
      "Iteration 88, loss = 0.67316045\n",
      "Iteration 89, loss = 0.67284534\n",
      "Iteration 90, loss = 0.67253313\n",
      "Iteration 91, loss = 0.67228375\n",
      "Iteration 92, loss = 0.67205548\n",
      "Iteration 93, loss = 0.67164143\n",
      "Iteration 94, loss = 0.67135080\n",
      "Iteration 95, loss = 0.67089385\n",
      "Iteration 96, loss = 0.67053345\n",
      "Iteration 97, loss = 0.67019121\n",
      "Iteration 98, loss = 0.66987048\n",
      "Iteration 99, loss = 0.66955609\n",
      "Iteration 100, loss = 0.66931661\n",
      "Iteration 101, loss = 0.66891526\n",
      "Iteration 102, loss = 0.66867894\n",
      "Iteration 103, loss = 0.66834168\n",
      "Iteration 104, loss = 0.66804106\n",
      "Iteration 105, loss = 0.66765322\n",
      "Iteration 106, loss = 0.66723453\n",
      "Iteration 107, loss = 0.66686952\n",
      "Iteration 108, loss = 0.66655693\n",
      "Iteration 109, loss = 0.66629002\n",
      "Iteration 110, loss = 0.66606573\n",
      "Iteration 111, loss = 0.66559515\n",
      "Iteration 112, loss = 0.66534012\n",
      "Iteration 113, loss = 0.66507874\n",
      "Iteration 114, loss = 0.66466091\n",
      "Iteration 115, loss = 0.66431619\n",
      "Iteration 116, loss = 0.66423136\n",
      "Iteration 117, loss = 0.66402450\n",
      "Iteration 118, loss = 0.66365724\n",
      "Iteration 119, loss = 0.66351126\n",
      "Iteration 120, loss = 0.66300915\n",
      "Iteration 121, loss = 0.66258414\n",
      "Iteration 122, loss = 0.66217499\n",
      "Iteration 123, loss = 0.66186289\n",
      "Iteration 124, loss = 0.66168764\n",
      "Iteration 125, loss = 0.66118953\n",
      "Iteration 126, loss = 0.66074207\n",
      "Iteration 127, loss = 0.66038977\n",
      "Iteration 128, loss = 0.66005354\n",
      "Iteration 129, loss = 0.65969769\n",
      "Iteration 130, loss = 0.65934817\n",
      "Iteration 131, loss = 0.65898282\n",
      "Iteration 132, loss = 0.65862388\n",
      "Iteration 133, loss = 0.65827255\n",
      "Iteration 134, loss = 0.65791857\n",
      "Iteration 135, loss = 0.65756606\n",
      "Iteration 136, loss = 0.65721231\n",
      "Iteration 137, loss = 0.65685739\n",
      "Iteration 138, loss = 0.65650280\n",
      "Iteration 139, loss = 0.65614832\n",
      "Iteration 140, loss = 0.65580122\n",
      "Iteration 141, loss = 0.65545837\n",
      "Iteration 142, loss = 0.65516526\n",
      "Iteration 143, loss = 0.65489968\n",
      "Iteration 144, loss = 0.65445467\n",
      "Iteration 145, loss = 0.65401439\n",
      "Iteration 146, loss = 0.65365671\n",
      "Iteration 147, loss = 0.65329352\n",
      "Iteration 148, loss = 0.65294256\n",
      "Iteration 149, loss = 0.65263557\n",
      "Iteration 150, loss = 0.65237106\n",
      "Iteration 151, loss = 0.65192774\n",
      "Iteration 152, loss = 0.65149625\n",
      "Iteration 153, loss = 0.65115583\n",
      "Iteration 154, loss = 0.65085747\n",
      "Iteration 155, loss = 0.65044719\n",
      "Iteration 156, loss = 0.65011199\n",
      "Iteration 157, loss = 0.64970433\n",
      "Iteration 158, loss = 0.64935574\n",
      "Iteration 159, loss = 0.64907371\n",
      "Iteration 160, loss = 0.64883125\n",
      "Iteration 161, loss = 0.64837588\n",
      "Iteration 162, loss = 0.64794989\n",
      "Iteration 163, loss = 0.64750256\n",
      "Iteration 164, loss = 0.64715121\n",
      "Iteration 165, loss = 0.64686371\n",
      "Iteration 166, loss = 0.64655505\n",
      "Iteration 167, loss = 0.64610010\n",
      "Iteration 168, loss = 0.64566276\n",
      "Iteration 169, loss = 0.64529467\n",
      "Iteration 170, loss = 0.64492560\n",
      "Iteration 171, loss = 0.64455956\n",
      "Iteration 172, loss = 0.64419612\n",
      "Iteration 173, loss = 0.64387260\n",
      "Iteration 174, loss = 0.64345907\n",
      "Iteration 175, loss = 0.64308840\n",
      "Iteration 176, loss = 0.64278871\n",
      "Iteration 177, loss = 0.64249996\n",
      "Iteration 178, loss = 0.64204068\n",
      "Iteration 179, loss = 0.64158997\n",
      "Iteration 180, loss = 0.64124792\n",
      "Iteration 181, loss = 0.64096406\n",
      "Iteration 182, loss = 0.64049090\n",
      "Iteration 183, loss = 0.64008312\n",
      "Iteration 184, loss = 0.63973125\n",
      "Iteration 185, loss = 0.63937926\n",
      "Iteration 186, loss = 0.63910535\n",
      "Iteration 187, loss = 0.63864728\n",
      "Iteration 188, loss = 0.63819150\n",
      "Iteration 189, loss = 0.63782045\n",
      "Iteration 190, loss = 0.63743792\n",
      "Iteration 191, loss = 0.63706298\n",
      "Iteration 192, loss = 0.63669969\n",
      "Iteration 193, loss = 0.63630874\n",
      "Iteration 194, loss = 0.63594167\n",
      "Iteration 195, loss = 0.63554320\n",
      "Iteration 196, loss = 0.63516943\n",
      "Iteration 197, loss = 0.63477768\n",
      "Iteration 198, loss = 0.63442115\n",
      "Iteration 199, loss = 0.63398897\n",
      "Iteration 200, loss = 0.63364530\n",
      "Iteration 201, loss = 0.63320653\n",
      "Iteration 202, loss = 0.63280055\n",
      "Iteration 203, loss = 0.63240727\n",
      "Iteration 204, loss = 0.63202072\n",
      "Iteration 205, loss = 0.63164039\n",
      "Iteration 206, loss = 0.63127486\n",
      "Iteration 207, loss = 0.63087125\n",
      "Iteration 208, loss = 0.63050170\n",
      "Iteration 209, loss = 0.63008681\n",
      "Iteration 210, loss = 0.62970909\n",
      "Iteration 211, loss = 0.62930246\n",
      "Iteration 212, loss = 0.62890695\n",
      "Iteration 213, loss = 0.62852461\n",
      "Iteration 214, loss = 0.62819677\n",
      "Iteration 215, loss = 0.62771748\n",
      "Iteration 216, loss = 0.62726800\n",
      "Iteration 217, loss = 0.62688757\n",
      "Iteration 218, loss = 0.62647719\n",
      "Iteration 219, loss = 0.62610296\n",
      "Iteration 220, loss = 0.62574154\n",
      "Iteration 221, loss = 0.62528391\n",
      "Iteration 222, loss = 0.62485879\n",
      "Iteration 223, loss = 0.62445397\n",
      "Iteration 224, loss = 0.62404995\n",
      "Iteration 225, loss = 0.62364051\n",
      "Iteration 226, loss = 0.62325166\n",
      "Iteration 227, loss = 0.62285731\n",
      "Iteration 228, loss = 0.62247311\n",
      "Iteration 229, loss = 0.62204039\n",
      "Iteration 230, loss = 0.62164497\n",
      "Iteration 231, loss = 0.62121832\n",
      "Iteration 232, loss = 0.62080721\n",
      "Iteration 233, loss = 0.62041133\n",
      "Iteration 234, loss = 0.62006590\n",
      "Iteration 235, loss = 0.61961370\n",
      "Iteration 236, loss = 0.61911846\n",
      "Iteration 237, loss = 0.61873162\n",
      "Iteration 238, loss = 0.61835785\n",
      "Iteration 239, loss = 0.61787329\n",
      "Iteration 240, loss = 0.61741364\n",
      "Iteration 241, loss = 0.61699008\n",
      "Iteration 242, loss = 0.61656917\n",
      "Iteration 243, loss = 0.61616055\n",
      "Iteration 244, loss = 0.61573131\n",
      "Iteration 245, loss = 0.61531376\n",
      "Iteration 246, loss = 0.61488978\n",
      "Iteration 247, loss = 0.61447375\n",
      "Iteration 248, loss = 0.61403063\n",
      "Iteration 249, loss = 0.61361090\n",
      "Iteration 250, loss = 0.61317042\n",
      "Iteration 251, loss = 0.61276651\n",
      "Iteration 252, loss = 0.61239164\n",
      "Iteration 253, loss = 0.61190382\n",
      "Iteration 254, loss = 0.61141547\n",
      "Iteration 255, loss = 0.61098197\n",
      "Iteration 256, loss = 0.61058177\n",
      "Iteration 257, loss = 0.61014974\n",
      "Iteration 258, loss = 0.60974731\n",
      "Iteration 259, loss = 0.60935731\n",
      "Iteration 260, loss = 0.60898364\n",
      "Iteration 261, loss = 0.60849606\n",
      "Iteration 262, loss = 0.60793244\n",
      "Iteration 263, loss = 0.60752593\n",
      "Iteration 264, loss = 0.60712286\n",
      "Iteration 265, loss = 0.60662769\n",
      "Iteration 266, loss = 0.60619085\n",
      "Iteration 267, loss = 0.60576751\n",
      "Iteration 268, loss = 0.60538541\n",
      "Iteration 269, loss = 0.60487873\n",
      "Iteration 270, loss = 0.60436550\n",
      "Iteration 271, loss = 0.60393748\n",
      "Iteration 272, loss = 0.60354670\n",
      "Iteration 273, loss = 0.60303815\n",
      "Iteration 274, loss = 0.60259405\n",
      "Iteration 275, loss = 0.60210816\n",
      "Iteration 276, loss = 0.60169500\n",
      "Iteration 277, loss = 0.60119584\n",
      "Iteration 278, loss = 0.60073697\n",
      "Iteration 279, loss = 0.60030870\n",
      "Iteration 280, loss = 0.59991582\n",
      "Iteration 281, loss = 0.59941536\n",
      "Iteration 282, loss = 0.59888445\n",
      "Iteration 283, loss = 0.59837217\n",
      "Iteration 284, loss = 0.59789340\n",
      "Iteration 285, loss = 0.59742066\n",
      "Iteration 286, loss = 0.59697293\n",
      "Iteration 287, loss = 0.59652596\n",
      "Iteration 288, loss = 0.59606866\n",
      "Iteration 289, loss = 0.59563483\n",
      "Iteration 290, loss = 0.59514220\n",
      "Iteration 291, loss = 0.59472318\n",
      "Iteration 292, loss = 0.59428011\n",
      "Iteration 293, loss = 0.59378607\n",
      "Iteration 294, loss = 0.59326221\n",
      "Iteration 295, loss = 0.59270098\n",
      "Iteration 296, loss = 0.59224947\n",
      "Iteration 297, loss = 0.59180060\n",
      "Iteration 298, loss = 0.59129796\n",
      "Iteration 299, loss = 0.59079408\n",
      "Iteration 300, loss = 0.59037377\n",
      "Iteration 1, loss = 0.70123886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69717832\n",
      "Iteration 3, loss = 0.69726583\n",
      "Iteration 4, loss = 0.69620960\n",
      "Iteration 5, loss = 0.69576956\n",
      "Iteration 6, loss = 0.69562503\n",
      "Iteration 7, loss = 0.69540056\n",
      "Iteration 8, loss = 0.69518792\n",
      "Iteration 9, loss = 0.69502871\n",
      "Iteration 10, loss = 0.69486532\n",
      "Iteration 11, loss = 0.69464308\n",
      "Iteration 12, loss = 0.69449949\n",
      "Iteration 13, loss = 0.69422080\n",
      "Iteration 14, loss = 0.69405206\n",
      "Iteration 15, loss = 0.69376447\n",
      "Iteration 16, loss = 0.69350854\n",
      "Iteration 17, loss = 0.69333526\n",
      "Iteration 18, loss = 0.69322448\n",
      "Iteration 19, loss = 0.69282280\n",
      "Iteration 20, loss = 0.69255683\n",
      "Iteration 21, loss = 0.69232276\n",
      "Iteration 22, loss = 0.69222108\n",
      "Iteration 23, loss = 0.69178629\n",
      "Iteration 24, loss = 0.69151873\n",
      "Iteration 25, loss = 0.69131363\n",
      "Iteration 26, loss = 0.69119452\n",
      "Iteration 27, loss = 0.69072619\n",
      "Iteration 28, loss = 0.69043944\n",
      "Iteration 29, loss = 0.69017503\n",
      "Iteration 30, loss = 0.68991186\n",
      "Iteration 31, loss = 0.68966548\n",
      "Iteration 32, loss = 0.68941883\n",
      "Iteration 33, loss = 0.68931591\n",
      "Iteration 34, loss = 0.68894148\n",
      "Iteration 35, loss = 0.68855284\n",
      "Iteration 36, loss = 0.68824272\n",
      "Iteration 37, loss = 0.68796526\n",
      "Iteration 38, loss = 0.68768849\n",
      "Iteration 39, loss = 0.68740656\n",
      "Iteration 40, loss = 0.68712867\n",
      "Iteration 41, loss = 0.68685205\n",
      "Iteration 42, loss = 0.68656987\n",
      "Iteration 43, loss = 0.68628954\n",
      "Iteration 44, loss = 0.68600895\n",
      "Iteration 45, loss = 0.68572797\n",
      "Iteration 46, loss = 0.68544682\n",
      "Iteration 47, loss = 0.68516540\n",
      "Iteration 48, loss = 0.68488368\n",
      "Iteration 49, loss = 0.68460285\n",
      "Iteration 50, loss = 0.68432202\n",
      "Iteration 51, loss = 0.68404488\n",
      "Iteration 52, loss = 0.68375664\n",
      "Iteration 53, loss = 0.68347460\n",
      "Iteration 54, loss = 0.68319629\n",
      "Iteration 55, loss = 0.68291273\n",
      "Iteration 56, loss = 0.68265123\n",
      "Iteration 57, loss = 0.68237718\n",
      "Iteration 58, loss = 0.68214872\n",
      "Iteration 59, loss = 0.68178712\n",
      "Iteration 60, loss = 0.68151273\n",
      "Iteration 61, loss = 0.68125029\n",
      "Iteration 62, loss = 0.68101790\n",
      "Iteration 63, loss = 0.68090301\n",
      "Iteration 64, loss = 0.68054401\n",
      "Iteration 65, loss = 0.68026504\n",
      "Iteration 66, loss = 0.67997326\n",
      "Iteration 67, loss = 0.67965985\n",
      "Iteration 68, loss = 0.67929242\n",
      "Iteration 69, loss = 0.67902287\n",
      "Iteration 70, loss = 0.67878773\n",
      "Iteration 71, loss = 0.67838548\n",
      "Iteration 72, loss = 0.67812193\n",
      "Iteration 73, loss = 0.67788309\n",
      "Iteration 74, loss = 0.67753894\n",
      "Iteration 75, loss = 0.67725721\n",
      "Iteration 76, loss = 0.67699220\n",
      "Iteration 77, loss = 0.67676416\n",
      "Iteration 78, loss = 0.67667863\n",
      "Iteration 79, loss = 0.67630210\n",
      "Iteration 80, loss = 0.67603032\n",
      "Iteration 81, loss = 0.67572560\n",
      "Iteration 82, loss = 0.67541062\n",
      "Iteration 83, loss = 0.67508387\n",
      "Iteration 84, loss = 0.67473311\n",
      "Iteration 85, loss = 0.67450477\n",
      "Iteration 86, loss = 0.67438126\n",
      "Iteration 87, loss = 0.67393700\n",
      "Iteration 88, loss = 0.67362281\n",
      "Iteration 89, loss = 0.67328611\n",
      "Iteration 90, loss = 0.67302907\n",
      "Iteration 91, loss = 0.67277056\n",
      "Iteration 92, loss = 0.67240796\n",
      "Iteration 93, loss = 0.67215898\n",
      "Iteration 94, loss = 0.67189188\n",
      "Iteration 95, loss = 0.67156311\n",
      "Iteration 96, loss = 0.67126951\n",
      "Iteration 97, loss = 0.67096368\n",
      "Iteration 98, loss = 0.67068243\n",
      "Iteration 99, loss = 0.67041812\n",
      "Iteration 100, loss = 0.67010216\n",
      "Iteration 101, loss = 0.66981036\n",
      "Iteration 102, loss = 0.66951382\n",
      "Iteration 103, loss = 0.66922041\n",
      "Iteration 104, loss = 0.66893651\n",
      "Iteration 105, loss = 0.66865277\n",
      "Iteration 106, loss = 0.66835761\n",
      "Iteration 107, loss = 0.66806450\n",
      "Iteration 108, loss = 0.66777362\n",
      "Iteration 109, loss = 0.66748129\n",
      "Iteration 110, loss = 0.66718962\n",
      "Iteration 111, loss = 0.66689550\n",
      "Iteration 112, loss = 0.66660156\n",
      "Iteration 113, loss = 0.66630739\n",
      "Iteration 114, loss = 0.66602172\n",
      "Iteration 115, loss = 0.66572622\n",
      "Iteration 116, loss = 0.66542719\n",
      "Iteration 117, loss = 0.66513037\n",
      "Iteration 118, loss = 0.66483612\n",
      "Iteration 119, loss = 0.66454134\n",
      "Iteration 120, loss = 0.66424526\n",
      "Iteration 121, loss = 0.66395386\n",
      "Iteration 122, loss = 0.66365375\n",
      "Iteration 123, loss = 0.66335876\n",
      "Iteration 124, loss = 0.66305775\n",
      "Iteration 125, loss = 0.66277411\n",
      "Iteration 126, loss = 0.66246641\n",
      "Iteration 127, loss = 0.66216199\n",
      "Iteration 128, loss = 0.66186448\n",
      "Iteration 129, loss = 0.66156438\n",
      "Iteration 130, loss = 0.66126271\n",
      "Iteration 131, loss = 0.66095982\n",
      "Iteration 132, loss = 0.66065984\n",
      "Iteration 133, loss = 0.66035727\n",
      "Iteration 134, loss = 0.66005416\n",
      "Iteration 135, loss = 0.65975242\n",
      "Iteration 136, loss = 0.65945278\n",
      "Iteration 137, loss = 0.65914527\n",
      "Iteration 138, loss = 0.65884073\n",
      "Iteration 139, loss = 0.65854393\n",
      "Iteration 140, loss = 0.65823166\n",
      "Iteration 141, loss = 0.65793068\n",
      "Iteration 142, loss = 0.65762373\n",
      "Iteration 143, loss = 0.65731510\n",
      "Iteration 144, loss = 0.65700694\n",
      "Iteration 145, loss = 0.65670097\n",
      "Iteration 146, loss = 0.65639596\n",
      "Iteration 147, loss = 0.65608850\n",
      "Iteration 148, loss = 0.65577679\n",
      "Iteration 149, loss = 0.65546650\n",
      "Iteration 150, loss = 0.65516076\n",
      "Iteration 151, loss = 0.65484961\n",
      "Iteration 152, loss = 0.65453122\n",
      "Iteration 153, loss = 0.65422360\n",
      "Iteration 154, loss = 0.65391104\n",
      "Iteration 155, loss = 0.65359707\n",
      "Iteration 156, loss = 0.65328559\n",
      "Iteration 157, loss = 0.65296893\n",
      "Iteration 158, loss = 0.65265650\n",
      "Iteration 159, loss = 0.65233970\n",
      "Iteration 160, loss = 0.65202370\n",
      "Iteration 161, loss = 0.65170207\n",
      "Iteration 162, loss = 0.65138516\n",
      "Iteration 163, loss = 0.65105609\n",
      "Iteration 164, loss = 0.65070813\n",
      "Iteration 165, loss = 0.65021747\n",
      "Iteration 166, loss = 0.64943419\n",
      "Iteration 167, loss = 0.64919742\n",
      "Iteration 168, loss = 0.64879559\n",
      "Iteration 169, loss = 0.64840965\n",
      "Iteration 170, loss = 0.64806324\n",
      "Iteration 171, loss = 0.64771058\n",
      "Iteration 172, loss = 0.64732873\n",
      "Iteration 173, loss = 0.64694824\n",
      "Iteration 174, loss = 0.64657530\n",
      "Iteration 175, loss = 0.64620120\n",
      "Iteration 176, loss = 0.64582561\n",
      "Iteration 177, loss = 0.64544720\n",
      "Iteration 178, loss = 0.64506559\n",
      "Iteration 179, loss = 0.64468346\n",
      "Iteration 180, loss = 0.64429945\n",
      "Iteration 181, loss = 0.64391236\n",
      "Iteration 182, loss = 0.64352509\n",
      "Iteration 183, loss = 0.64313627\n",
      "Iteration 184, loss = 0.64274463\n",
      "Iteration 185, loss = 0.64235338\n",
      "Iteration 186, loss = 0.64196149\n",
      "Iteration 187, loss = 0.64157630\n",
      "Iteration 188, loss = 0.64118724\n",
      "Iteration 189, loss = 0.64078344\n",
      "Iteration 190, loss = 0.64040061\n",
      "Iteration 191, loss = 0.63998903\n",
      "Iteration 192, loss = 0.63960783\n",
      "Iteration 193, loss = 0.63918623\n",
      "Iteration 194, loss = 0.63877766\n",
      "Iteration 195, loss = 0.63838633\n",
      "Iteration 196, loss = 0.63798325\n",
      "Iteration 197, loss = 0.63758562\n",
      "Iteration 198, loss = 0.63718147\n",
      "Iteration 199, loss = 0.63676812\n",
      "Iteration 200, loss = 0.63637609\n",
      "Iteration 201, loss = 0.63594776\n",
      "Iteration 202, loss = 0.63553275\n",
      "Iteration 203, loss = 0.63513293\n",
      "Iteration 204, loss = 0.63472724\n",
      "Iteration 205, loss = 0.63434471\n",
      "Iteration 206, loss = 0.63393277\n",
      "Iteration 207, loss = 0.63348326\n",
      "Iteration 208, loss = 0.63306673\n",
      "Iteration 209, loss = 0.63265238\n",
      "Iteration 210, loss = 0.63225174\n",
      "Iteration 211, loss = 0.63182723\n",
      "Iteration 212, loss = 0.63142772\n",
      "Iteration 213, loss = 0.63098773\n",
      "Iteration 214, loss = 0.63055719\n",
      "Iteration 215, loss = 0.63013852\n",
      "Iteration 216, loss = 0.62972194\n",
      "Iteration 217, loss = 0.62931890\n",
      "Iteration 218, loss = 0.62888503\n",
      "Iteration 219, loss = 0.62845626\n",
      "Iteration 220, loss = 0.62804580\n",
      "Iteration 221, loss = 0.62761832\n",
      "Iteration 222, loss = 0.62718434\n",
      "Iteration 223, loss = 0.62678381\n",
      "Iteration 224, loss = 0.62633583\n",
      "Iteration 225, loss = 0.62589462\n",
      "Iteration 226, loss = 0.62548143\n",
      "Iteration 227, loss = 0.62505509\n",
      "Iteration 228, loss = 0.62463112\n",
      "Iteration 229, loss = 0.62417863\n",
      "Iteration 230, loss = 0.62373782\n",
      "Iteration 231, loss = 0.62330708\n",
      "Iteration 232, loss = 0.62288914\n",
      "Iteration 233, loss = 0.62244333\n",
      "Iteration 234, loss = 0.62199487\n",
      "Iteration 235, loss = 0.62155758\n",
      "Iteration 236, loss = 0.62112942\n",
      "Iteration 237, loss = 0.62069840\n",
      "Iteration 238, loss = 0.62025374\n",
      "Iteration 239, loss = 0.61980102\n",
      "Iteration 240, loss = 0.61936639\n",
      "Iteration 241, loss = 0.61893297\n",
      "Iteration 242, loss = 0.61850769\n",
      "Iteration 243, loss = 0.61807948\n",
      "Iteration 244, loss = 0.61760072\n",
      "Iteration 245, loss = 0.61713135\n",
      "Iteration 246, loss = 0.61669378\n",
      "Iteration 247, loss = 0.61623978\n",
      "Iteration 248, loss = 0.61577816\n",
      "Iteration 249, loss = 0.61533951\n",
      "Iteration 250, loss = 0.61488878\n",
      "Iteration 251, loss = 0.61443361\n",
      "Iteration 252, loss = 0.61399737\n",
      "Iteration 253, loss = 0.61353312\n",
      "Iteration 254, loss = 0.61306762\n",
      "Iteration 255, loss = 0.61260642\n",
      "Iteration 256, loss = 0.61214526\n",
      "Iteration 257, loss = 0.61168942\n",
      "Iteration 258, loss = 0.61123523\n",
      "Iteration 259, loss = 0.61076274\n",
      "Iteration 260, loss = 0.61029482\n",
      "Iteration 261, loss = 0.60984062\n",
      "Iteration 262, loss = 0.60937229\n",
      "Iteration 263, loss = 0.60890237\n",
      "Iteration 264, loss = 0.60843313\n",
      "Iteration 265, loss = 0.60796271\n",
      "Iteration 266, loss = 0.60748825\n",
      "Iteration 267, loss = 0.60703959\n",
      "Iteration 268, loss = 0.60656572\n",
      "Iteration 269, loss = 0.60608535\n",
      "Iteration 270, loss = 0.60560351\n",
      "Iteration 271, loss = 0.60512501\n",
      "Iteration 272, loss = 0.60464591\n",
      "Iteration 273, loss = 0.60416424\n",
      "Iteration 274, loss = 0.60371273\n",
      "Iteration 275, loss = 0.60320916\n",
      "Iteration 276, loss = 0.60272756\n",
      "Iteration 277, loss = 0.60224450\n",
      "Iteration 278, loss = 0.60175982\n",
      "Iteration 279, loss = 0.60127346\n",
      "Iteration 280, loss = 0.60078542\n",
      "Iteration 281, loss = 0.60029573\n",
      "Iteration 282, loss = 0.59980617\n",
      "Iteration 283, loss = 0.59931898\n",
      "Iteration 284, loss = 0.59882597\n",
      "Iteration 285, loss = 0.59833109\n",
      "Iteration 286, loss = 0.59783641\n",
      "Iteration 287, loss = 0.59734039\n",
      "Iteration 288, loss = 0.59684575\n",
      "Iteration 289, loss = 0.59634690\n",
      "Iteration 290, loss = 0.59584597\n",
      "Iteration 291, loss = 0.59534619\n",
      "Iteration 292, loss = 0.59484348\n",
      "Iteration 293, loss = 0.59434515\n",
      "Iteration 294, loss = 0.59386485\n",
      "Iteration 295, loss = 0.59336936\n",
      "Iteration 296, loss = 0.59285001\n",
      "Iteration 297, loss = 0.59236955\n",
      "Iteration 298, loss = 0.59190966\n",
      "Iteration 299, loss = 0.59138865\n",
      "Iteration 300, loss = 0.59084911\n",
      "Iteration 1, loss = 0.69941293\n",
      "Iteration 2, loss = 3.61367848\n",
      "Iteration 3, loss = 1.41837408\n",
      "Iteration 4, loss = 0.75507088\n",
      "Iteration 5, loss = 0.75503147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.75479845\n",
      "Iteration 7, loss = 0.75443490\n",
      "Iteration 8, loss = 0.75397645\n",
      "Iteration 9, loss = 0.75344607\n",
      "Iteration 10, loss = 0.75285981\n",
      "Iteration 11, loss = 0.75222940\n",
      "Iteration 12, loss = 0.75156378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70286440\n",
      "Iteration 2, loss = 3.57261481\n",
      "Iteration 3, loss = 1.43837088\n",
      "Iteration 4, loss = 0.75975201\n",
      "Iteration 5, loss = 0.75969152\n",
      "Iteration 6, loss = 0.75943332\n",
      "Iteration 7, loss = 0.75904119\n",
      "Iteration 8, loss = 0.75855158\n",
      "Iteration 9, loss = 0.75798815\n",
      "Iteration 10, loss = 0.75736741\n",
      "Iteration 11, loss = 0.75670144\n",
      "Iteration 12, loss = 0.75599945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70123886\n",
      "Iteration 2, loss = 3.59652467\n",
      "Iteration 3, loss = 1.42939395\n",
      "Iteration 4, loss = 0.75738745\n",
      "Iteration 5, loss = 0.75733248\n",
      "Iteration 6, loss = 0.75708304\n",
      "Iteration 7, loss = 0.75670195\n",
      "Iteration 8, loss = 0.75622509\n",
      "Iteration 9, loss = 0.75567569\n",
      "Iteration 10, loss = 0.75506998\n",
      "Iteration 11, loss = 0.75441983\n",
      "Iteration 12, loss = 0.75373427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79410679\n",
      "Iteration 2, loss = 0.71028231\n",
      "Iteration 3, loss = 0.70672082\n",
      "Iteration 4, loss = 0.70291187\n",
      "Iteration 5, loss = 0.69845434\n",
      "Iteration 6, loss = 0.69559204\n",
      "Iteration 7, loss = 0.69449092\n",
      "Iteration 8, loss = 0.69390515\n",
      "Iteration 9, loss = 0.69387605\n",
      "Iteration 10, loss = 0.69416723\n",
      "Iteration 11, loss = 0.69461344\n",
      "Iteration 12, loss = 0.69507063\n",
      "Iteration 13, loss = 0.69544184\n",
      "Iteration 14, loss = 0.69566284\n",
      "Iteration 15, loss = 0.69570993\n",
      "Iteration 16, loss = 0.69559946\n",
      "Iteration 17, loss = 0.69536617\n",
      "Iteration 18, loss = 0.69505836\n",
      "Iteration 19, loss = 0.69472384\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79833060\n",
      "Iteration 2, loss = 0.71111906\n",
      "Iteration 3, loss = 0.70826049\n",
      "Iteration 4, loss = 0.70380645\n",
      "Iteration 5, loss = 0.69876310\n",
      "Iteration 6, loss = 0.69516700\n",
      "Iteration 7, loss = 0.69406023\n",
      "Iteration 8, loss = 0.69328366\n",
      "Iteration 9, loss = 0.69309196\n",
      "Iteration 10, loss = 0.69335845\n",
      "Iteration 11, loss = 0.69383997\n",
      "Iteration 12, loss = 0.69436132\n",
      "Iteration 13, loss = 0.69479522\n",
      "Iteration 14, loss = 0.69507420\n",
      "Iteration 15, loss = 0.69517012\n",
      "Iteration 16, loss = 0.69508173\n",
      "Iteration 17, loss = 0.69484624\n",
      "Iteration 18, loss = 0.69451462\n",
      "Iteration 19, loss = 0.69414443\n",
      "Iteration 20, loss = 0.69378588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79565985\n",
      "Iteration 2, loss = 0.71058728\n",
      "Iteration 3, loss = 0.70819919\n",
      "Iteration 4, loss = 0.70464906\n",
      "Iteration 5, loss = 0.70003979\n",
      "Iteration 6, loss = 0.69632324\n",
      "Iteration 7, loss = 0.69463536\n",
      "Iteration 8, loss = 0.69391144\n",
      "Iteration 9, loss = 0.69366561\n",
      "Iteration 10, loss = 0.69382869\n",
      "Iteration 11, loss = 0.69423389\n",
      "Iteration 12, loss = 0.69471033\n",
      "Iteration 13, loss = 0.69513767\n",
      "Iteration 14, loss = 0.69542323\n",
      "Iteration 15, loss = 0.69553494\n",
      "Iteration 16, loss = 0.69547600\n",
      "Iteration 17, loss = 0.69527956\n",
      "Iteration 18, loss = 0.69499057\n",
      "Iteration 19, loss = 0.69465674\n",
      "Iteration 20, loss = 0.69432020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79410679\n",
      "Iteration 2, loss = 0.85417323\n",
      "Iteration 3, loss = 0.75142498\n",
      "Iteration 4, loss = 0.69557172\n",
      "Iteration 5, loss = 0.69321131\n",
      "Iteration 6, loss = 0.69640854\n",
      "Iteration 7, loss = 0.70030874\n",
      "Iteration 8, loss = 0.70125637\n",
      "Iteration 9, loss = 0.69890143\n",
      "Iteration 10, loss = 0.69533893\n",
      "Iteration 11, loss = 0.69301497\n",
      "Iteration 12, loss = 0.69301628\n",
      "Iteration 13, loss = 0.69461865\n",
      "Iteration 14, loss = 0.69614497\n",
      "Iteration 15, loss = 0.69632530\n",
      "Iteration 16, loss = 0.69509663\n",
      "Iteration 17, loss = 0.69340187\n",
      "Iteration 18, loss = 0.69234587\n",
      "Iteration 19, loss = 0.69239872\n",
      "Iteration 20, loss = 0.69317305\n",
      "Iteration 21, loss = 0.69384283\n",
      "Iteration 22, loss = 0.69380796\n",
      "Iteration 23, loss = 0.69308564\n",
      "Iteration 24, loss = 0.69219470\n",
      "Iteration 25, loss = 0.69169827\n",
      "Iteration 26, loss = 0.69178116\n",
      "Iteration 27, loss = 0.69216616\n",
      "Iteration 28, loss = 0.69239303\n",
      "Iteration 29, loss = 0.69220068\n",
      "Iteration 30, loss = 0.69169931\n",
      "Iteration 31, loss = 0.69122638\n",
      "Iteration 32, loss = 0.69104448\n",
      "Iteration 33, loss = 0.69114263\n",
      "Iteration 34, loss = 0.69128653\n",
      "Iteration 35, loss = 0.69124366\n",
      "Iteration 36, loss = 0.69097415\n",
      "Iteration 37, loss = 0.69063427\n",
      "Iteration 38, loss = 0.69041551\n",
      "Iteration 39, loss = 0.69037633\n",
      "Iteration 40, loss = 0.69041135\n",
      "Iteration 41, loss = 0.69036743\n",
      "Iteration 42, loss = 0.69018568\n",
      "Iteration 43, loss = 0.68993945\n",
      "Iteration 44, loss = 0.68974807\n",
      "Iteration 45, loss = 0.68966154\n",
      "Iteration 46, loss = 0.68962539\n",
      "Iteration 47, loss = 0.68954703\n",
      "Iteration 48, loss = 0.68938784\n",
      "Iteration 49, loss = 0.68919103\n",
      "Iteration 50, loss = 0.68902797\n",
      "Iteration 51, loss = 0.68892532\n",
      "Iteration 52, loss = 0.68884600\n",
      "Iteration 53, loss = 0.68873524\n",
      "Iteration 54, loss = 0.68857741\n",
      "Iteration 55, loss = 0.68840550\n",
      "Iteration 56, loss = 0.68826021\n",
      "Iteration 57, loss = 0.68814747\n",
      "Iteration 58, loss = 0.68803777\n",
      "Iteration 59, loss = 0.68790218\n",
      "Iteration 60, loss = 0.68774234\n",
      "Iteration 61, loss = 0.68758386\n",
      "Iteration 62, loss = 0.68744541\n",
      "Iteration 63, loss = 0.68731981\n",
      "Iteration 64, loss = 0.68718584\n",
      "Iteration 65, loss = 0.68703349\n",
      "Iteration 66, loss = 0.68687315\n",
      "Iteration 67, loss = 0.68672084\n",
      "Iteration 68, loss = 0.68657941\n",
      "Iteration 69, loss = 0.68643747\n",
      "Iteration 70, loss = 0.68628448\n",
      "Iteration 71, loss = 0.68612266\n",
      "Iteration 72, loss = 0.68596235\n",
      "Iteration 73, loss = 0.68580868\n",
      "Iteration 74, loss = 0.68565640\n",
      "Iteration 75, loss = 0.68549768\n",
      "Iteration 76, loss = 0.68533183\n",
      "Iteration 77, loss = 0.68516493\n",
      "Iteration 78, loss = 0.68500139\n",
      "Iteration 79, loss = 0.68483880\n",
      "Iteration 80, loss = 0.68467198\n",
      "Iteration 81, loss = 0.68449977\n",
      "Iteration 82, loss = 0.68432582\n",
      "Iteration 83, loss = 0.68415318\n",
      "Iteration 84, loss = 0.68398052\n",
      "Iteration 85, loss = 0.68380449\n",
      "Iteration 86, loss = 0.68362432\n",
      "Iteration 87, loss = 0.68344236\n",
      "Iteration 88, loss = 0.68326047\n",
      "Iteration 89, loss = 0.68307762\n",
      "Iteration 90, loss = 0.68289167\n",
      "Iteration 91, loss = 0.68270238\n",
      "Iteration 92, loss = 0.68251137\n",
      "Iteration 93, loss = 0.68231958\n",
      "Iteration 94, loss = 0.68212606\n",
      "Iteration 95, loss = 0.68192952\n",
      "Iteration 96, loss = 0.68173013\n",
      "Iteration 97, loss = 0.68152902\n",
      "Iteration 98, loss = 0.68132643\n",
      "Iteration 99, loss = 0.68112151\n",
      "Iteration 100, loss = 0.68091363\n",
      "Iteration 101, loss = 0.68070318\n",
      "Iteration 102, loss = 0.68049082\n",
      "Iteration 103, loss = 0.68027637\n",
      "Iteration 104, loss = 0.68005916\n",
      "Iteration 105, loss = 0.67983905\n",
      "Iteration 106, loss = 0.67961647\n",
      "Iteration 107, loss = 0.67939162\n",
      "Iteration 108, loss = 0.67916412\n",
      "Iteration 109, loss = 0.67893361\n",
      "Iteration 110, loss = 0.67870024\n",
      "Iteration 111, loss = 0.67846430\n",
      "Iteration 112, loss = 0.67822553\n",
      "Iteration 113, loss = 0.67798374\n",
      "Iteration 114, loss = 0.67773882\n",
      "Iteration 115, loss = 0.67749094\n",
      "Iteration 116, loss = 0.67724010\n",
      "Iteration 117, loss = 0.67698605\n",
      "Iteration 118, loss = 0.67672865\n",
      "Iteration 119, loss = 0.67646797\n",
      "Iteration 120, loss = 0.67620405\n",
      "Iteration 121, loss = 0.67593670\n",
      "Iteration 122, loss = 0.67566578\n",
      "Iteration 123, loss = 0.67539130\n",
      "Iteration 124, loss = 0.67511325\n",
      "Iteration 125, loss = 0.67483153\n",
      "Iteration 126, loss = 0.67454596\n",
      "Iteration 127, loss = 0.67425654\n",
      "Iteration 128, loss = 0.67396324\n",
      "Iteration 129, loss = 0.67366594\n",
      "Iteration 130, loss = 0.67336452\n",
      "Iteration 131, loss = 0.67305893\n",
      "Iteration 132, loss = 0.67274912\n",
      "Iteration 133, loss = 0.67243499\n",
      "Iteration 134, loss = 0.67211640\n",
      "Iteration 135, loss = 0.67179330\n",
      "Iteration 136, loss = 0.67146562\n",
      "Iteration 137, loss = 0.67113325\n",
      "Iteration 138, loss = 0.67079606\n",
      "Iteration 139, loss = 0.67045400\n",
      "Iteration 140, loss = 0.67010697\n",
      "Iteration 141, loss = 0.66975485\n",
      "Iteration 142, loss = 0.66939754\n",
      "Iteration 143, loss = 0.66903493\n",
      "Iteration 144, loss = 0.66866694\n",
      "Iteration 145, loss = 0.66829344\n",
      "Iteration 146, loss = 0.66791432\n",
      "Iteration 147, loss = 0.66752947\n",
      "Iteration 148, loss = 0.66713879\n",
      "Iteration 149, loss = 0.66674213\n",
      "Iteration 150, loss = 0.66633940\n",
      "Iteration 151, loss = 0.66593047\n",
      "Iteration 152, loss = 0.66551521\n",
      "Iteration 153, loss = 0.66509350\n",
      "Iteration 154, loss = 0.66466521\n",
      "Iteration 155, loss = 0.66423022\n",
      "Iteration 156, loss = 0.66378837\n",
      "Iteration 157, loss = 0.66333955\n",
      "Iteration 158, loss = 0.66288361\n",
      "Iteration 159, loss = 0.66242041\n",
      "Iteration 160, loss = 0.66194982\n",
      "Iteration 161, loss = 0.66147168\n",
      "Iteration 162, loss = 0.66098585\n",
      "Iteration 163, loss = 0.66049216\n",
      "Iteration 164, loss = 0.69139561\n",
      "Iteration 165, loss = 0.66840190\n",
      "Iteration 166, loss = 0.66923556\n",
      "Iteration 167, loss = 0.65911104\n",
      "Iteration 168, loss = 0.66264251\n",
      "Iteration 169, loss = 0.66618768\n",
      "Iteration 170, loss = 0.65736509\n",
      "Iteration 171, loss = 0.66101572\n",
      "Iteration 172, loss = 0.66308103\n",
      "Iteration 173, loss = 0.65651058\n",
      "Iteration 174, loss = 0.65729349\n",
      "Iteration 175, loss = 0.66013102\n",
      "Iteration 176, loss = 0.65503810\n",
      "Iteration 177, loss = 0.65499867\n",
      "Iteration 178, loss = 0.65717337\n",
      "Iteration 179, loss = 0.65365758\n",
      "Iteration 180, loss = 0.65274168\n",
      "Iteration 181, loss = 0.65457751\n",
      "Iteration 182, loss = 0.65179262\n",
      "Iteration 183, loss = 0.65089655\n",
      "Iteration 184, loss = 0.65204146\n",
      "Iteration 185, loss = 0.64991355\n",
      "Iteration 186, loss = 0.64899602\n",
      "Iteration 187, loss = 0.64972083\n",
      "Iteration 188, loss = 0.64779070\n",
      "Iteration 189, loss = 0.64718221\n",
      "Iteration 190, loss = 0.64738075\n",
      "Iteration 191, loss = 0.64568591\n",
      "Iteration 192, loss = 0.64524002\n",
      "Iteration 193, loss = 0.64506095\n",
      "Iteration 194, loss = 0.64350658\n",
      "Iteration 195, loss = 0.64326472\n",
      "Iteration 196, loss = 0.64261470\n",
      "Iteration 197, loss = 0.64133439\n",
      "Iteration 198, loss = 0.64112290\n",
      "Iteration 199, loss = 0.64007654\n",
      "Iteration 200, loss = 0.63919450\n",
      "Iteration 201, loss = 0.63877120\n",
      "Iteration 202, loss = 0.63755338\n",
      "Iteration 203, loss = 0.63698143\n",
      "Iteration 204, loss = 0.63614090\n",
      "Iteration 205, loss = 0.63510883\n",
      "Iteration 206, loss = 0.63452720\n",
      "Iteration 207, loss = 0.63340512\n",
      "Iteration 208, loss = 0.63271677\n",
      "Iteration 209, loss = 0.63170997\n",
      "Iteration 210, loss = 0.63083943\n",
      "Iteration 211, loss = 0.62997063\n",
      "Iteration 212, loss = 0.62894159\n",
      "Iteration 213, loss = 0.62809716\n",
      "Iteration 214, loss = 0.62704264\n",
      "Iteration 215, loss = 0.62615985\n",
      "Iteration 216, loss = 0.62510659\n",
      "Iteration 217, loss = 0.62416933\n",
      "Iteration 218, loss = 0.62312415\n",
      "Iteration 219, loss = 0.62214007\n",
      "Iteration 220, loss = 0.62109017\n",
      "Iteration 221, loss = 0.62004743\n",
      "Iteration 222, loss = 0.61899028\n",
      "Iteration 223, loss = 0.61789334\n",
      "Iteration 224, loss = 0.61681406\n",
      "Iteration 225, loss = 0.61567814\n",
      "Iteration 226, loss = 0.61457444\n",
      "Iteration 227, loss = 0.61340056\n",
      "Iteration 228, loss = 0.61224177\n",
      "Iteration 229, loss = 0.61104739\n",
      "Iteration 230, loss = 0.60983061\n",
      "Iteration 231, loss = 0.60861626\n",
      "Iteration 232, loss = 0.60732510\n",
      "Iteration 233, loss = 0.60606345\n",
      "Iteration 234, loss = 0.60478828\n",
      "Iteration 235, loss = 0.60338513\n",
      "Iteration 236, loss = 0.60203279\n",
      "Iteration 237, loss = 0.60068678\n",
      "Iteration 238, loss = 0.59927171\n",
      "Iteration 239, loss = 0.59783008\n",
      "Iteration 240, loss = 0.59638384\n",
      "Iteration 241, loss = 0.59464362\n",
      "Iteration 242, loss = 0.59301228\n",
      "Iteration 243, loss = 0.59143653\n",
      "Iteration 244, loss = 0.58977418\n",
      "Iteration 245, loss = 0.58806147\n",
      "Iteration 246, loss = 0.58631462\n",
      "Iteration 247, loss = 0.58459886\n",
      "Iteration 248, loss = 0.58285065\n",
      "Iteration 249, loss = 0.58111779\n",
      "Iteration 250, loss = 0.57942104\n",
      "Iteration 251, loss = 0.57771651\n",
      "Iteration 252, loss = 0.57614539\n",
      "Iteration 253, loss = 0.57518475\n",
      "Iteration 254, loss = 0.57870468\n",
      "Iteration 255, loss = 0.60406137\n",
      "Iteration 256, loss = 0.59783536\n",
      "Iteration 257, loss = 0.59430562\n",
      "Iteration 258, loss = 0.56704442\n",
      "Iteration 259, loss = 0.60197059\n",
      "Iteration 260, loss = 0.61334356\n",
      "Iteration 261, loss = 0.57411471\n",
      "Iteration 262, loss = 0.63601225\n",
      "Iteration 263, loss = 0.56323878\n",
      "Iteration 264, loss = 0.60006171\n",
      "Iteration 265, loss = 0.55943789\n",
      "Iteration 266, loss = 0.60026341\n",
      "Iteration 267, loss = 0.57160576\n",
      "Iteration 268, loss = 0.58463398\n",
      "Iteration 269, loss = 0.56179961\n",
      "Iteration 270, loss = 0.57445886\n",
      "Iteration 271, loss = 0.57145077\n",
      "Iteration 272, loss = 0.57163780\n",
      "Iteration 273, loss = 0.56152057\n",
      "Iteration 274, loss = 0.56119116\n",
      "Iteration 275, loss = 0.56282424\n",
      "Iteration 276, loss = 0.55974976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79833060\n",
      "Iteration 2, loss = 0.84621598\n",
      "Iteration 3, loss = 0.75087530\n",
      "Iteration 4, loss = 0.69597706\n",
      "Iteration 5, loss = 0.69288724\n",
      "Iteration 6, loss = 0.69503853\n",
      "Iteration 7, loss = 0.69859897\n",
      "Iteration 8, loss = 0.70004906\n",
      "Iteration 9, loss = 0.69850543\n",
      "Iteration 10, loss = 0.69545552\n",
      "Iteration 11, loss = 0.69308356\n",
      "Iteration 12, loss = 0.69265089\n",
      "Iteration 13, loss = 0.69384540\n",
      "Iteration 14, loss = 0.69530768\n",
      "Iteration 15, loss = 0.69577812\n",
      "Iteration 16, loss = 0.69494770\n",
      "Iteration 17, loss = 0.69348056\n",
      "Iteration 18, loss = 0.69237090\n",
      "Iteration 19, loss = 0.69219566\n",
      "Iteration 20, loss = 0.69278091\n",
      "Iteration 21, loss = 0.69344909\n",
      "Iteration 22, loss = 0.69358643\n",
      "Iteration 23, loss = 0.69307220\n",
      "Iteration 24, loss = 0.69228362\n",
      "Iteration 25, loss = 0.69174176\n",
      "Iteration 26, loss = 0.69170539\n",
      "Iteration 27, loss = 0.69201577\n",
      "Iteration 28, loss = 0.69227854\n",
      "Iteration 29, loss = 0.69220202\n",
      "Iteration 30, loss = 0.69180765\n",
      "Iteration 31, loss = 0.69136481\n",
      "Iteration 32, loss = 0.69113991\n",
      "Iteration 33, loss = 0.69118389\n",
      "Iteration 34, loss = 0.69132347\n",
      "Iteration 35, loss = 0.69133633\n",
      "Iteration 36, loss = 0.69114144\n",
      "Iteration 37, loss = 0.69084472\n",
      "Iteration 38, loss = 0.69062170\n",
      "Iteration 39, loss = 0.69055780\n",
      "Iteration 40, loss = 0.69058810\n",
      "Iteration 41, loss = 0.69057612\n",
      "Iteration 42, loss = 0.69044463\n",
      "Iteration 43, loss = 0.69023520\n",
      "Iteration 44, loss = 0.69005265\n",
      "Iteration 45, loss = 0.68996033\n",
      "Iteration 46, loss = 0.68992845\n",
      "Iteration 47, loss = 0.68987586\n",
      "Iteration 48, loss = 0.68975355\n",
      "Iteration 49, loss = 0.68958568\n",
      "Iteration 50, loss = 0.68943532\n",
      "Iteration 51, loss = 0.68933788\n",
      "Iteration 52, loss = 0.68927107\n",
      "Iteration 53, loss = 0.68918550\n",
      "Iteration 54, loss = 0.68905767\n",
      "Iteration 55, loss = 0.68890929\n",
      "Iteration 56, loss = 0.68877836\n",
      "Iteration 57, loss = 0.68867801\n",
      "Iteration 58, loss = 0.68858699\n",
      "Iteration 59, loss = 0.68847697\n",
      "Iteration 60, loss = 0.68834309\n",
      "Iteration 61, loss = 0.68820532\n",
      "Iteration 62, loss = 0.68808340\n",
      "Iteration 63, loss = 0.68797576\n",
      "Iteration 64, loss = 0.68786454\n",
      "Iteration 65, loss = 0.68773760\n",
      "Iteration 66, loss = 0.68760084\n",
      "Iteration 67, loss = 0.68746879\n",
      "Iteration 68, loss = 0.68734707\n",
      "Iteration 69, loss = 0.68722745\n",
      "Iteration 70, loss = 0.68709939\n",
      "Iteration 71, loss = 0.68696239\n",
      "Iteration 72, loss = 0.68682491\n",
      "Iteration 73, loss = 0.68669316\n",
      "Iteration 74, loss = 0.68656415\n",
      "Iteration 75, loss = 0.68643070\n",
      "Iteration 76, loss = 0.68629058\n",
      "Iteration 77, loss = 0.68614838\n",
      "Iteration 78, loss = 0.68600884\n",
      "Iteration 79, loss = 0.68587107\n",
      "Iteration 80, loss = 0.68573051\n",
      "Iteration 81, loss = 0.68558513\n",
      "Iteration 82, loss = 0.68543752\n",
      "Iteration 83, loss = 0.68529085\n",
      "Iteration 84, loss = 0.68514480\n",
      "Iteration 85, loss = 0.68499647\n",
      "Iteration 86, loss = 0.68484451\n",
      "Iteration 87, loss = 0.68469056\n",
      "Iteration 88, loss = 0.68453663\n",
      "Iteration 89, loss = 0.68438235\n",
      "Iteration 90, loss = 0.68422585\n",
      "Iteration 91, loss = 0.68406643\n",
      "Iteration 92, loss = 0.68390528\n",
      "Iteration 93, loss = 0.68374355\n",
      "Iteration 94, loss = 0.68358075\n",
      "Iteration 95, loss = 0.68341564\n",
      "Iteration 96, loss = 0.68324807\n",
      "Iteration 97, loss = 0.68307891\n",
      "Iteration 98, loss = 0.68290869\n",
      "Iteration 99, loss = 0.68273682\n",
      "Iteration 100, loss = 0.68256261\n",
      "Iteration 101, loss = 0.68238619\n",
      "Iteration 102, loss = 0.68220819\n",
      "Iteration 103, loss = 0.68202865\n",
      "Iteration 104, loss = 0.68184706\n",
      "Iteration 105, loss = 0.68166312\n",
      "Iteration 106, loss = 0.68147711\n",
      "Iteration 107, loss = 0.68128934\n",
      "Iteration 108, loss = 0.68109960\n",
      "Iteration 109, loss = 0.68090754\n",
      "Iteration 110, loss = 0.68071314\n",
      "Iteration 111, loss = 0.68051665\n",
      "Iteration 112, loss = 0.68031809\n",
      "Iteration 113, loss = 0.68011720\n",
      "Iteration 114, loss = 0.67991382\n",
      "Iteration 115, loss = 0.67970809\n",
      "Iteration 116, loss = 0.67950008\n",
      "Iteration 117, loss = 0.67928964\n",
      "Iteration 118, loss = 0.67907659\n",
      "Iteration 119, loss = 0.67886096\n",
      "Iteration 120, loss = 0.67864282\n",
      "Iteration 121, loss = 0.67842208\n",
      "Iteration 122, loss = 0.67819860\n",
      "Iteration 123, loss = 0.67797233\n",
      "Iteration 124, loss = 0.67774332\n",
      "Iteration 125, loss = 0.67751151\n",
      "Iteration 126, loss = 0.67727677\n",
      "Iteration 127, loss = 0.67703904\n",
      "Iteration 128, loss = 0.67679833\n",
      "Iteration 129, loss = 0.67655459\n",
      "Iteration 130, loss = 0.67630770\n",
      "Iteration 131, loss = 0.75136193\n",
      "Iteration 132, loss = 0.68137502\n",
      "Iteration 133, loss = 0.68578393\n",
      "Iteration 134, loss = 0.68035451\n",
      "Iteration 135, loss = 0.67524161\n",
      "Iteration 136, loss = 0.67830081\n",
      "Iteration 137, loss = 0.68148215\n",
      "Iteration 138, loss = 0.67805374\n",
      "Iteration 139, loss = 0.67438460\n",
      "Iteration 140, loss = 0.67617116\n",
      "Iteration 141, loss = 0.67831931\n",
      "Iteration 142, loss = 0.67605246\n",
      "Iteration 143, loss = 0.67345715\n",
      "Iteration 144, loss = 0.67453466\n",
      "Iteration 145, loss = 0.67589298\n",
      "Iteration 146, loss = 0.67423938\n",
      "Iteration 147, loss = 0.67245142\n",
      "Iteration 148, loss = 0.67317177\n",
      "Iteration 149, loss = 0.67390799\n",
      "Iteration 150, loss = 0.67258655\n",
      "Iteration 151, loss = 0.67139232\n",
      "Iteration 152, loss = 0.67190547\n",
      "Iteration 153, loss = 0.67216465\n",
      "Iteration 154, loss = 0.67104016\n",
      "Iteration 155, loss = 0.67029409\n",
      "Iteration 156, loss = 0.67064969\n",
      "Iteration 157, loss = 0.67053246\n",
      "Iteration 158, loss = 0.66957670\n",
      "Iteration 159, loss = 0.66915858\n",
      "Iteration 160, loss = 0.66933512\n",
      "Iteration 161, loss = 0.66893786\n",
      "Iteration 162, loss = 0.66817130\n",
      "Iteration 163, loss = 0.66796697\n",
      "Iteration 164, loss = 0.66792071\n",
      "Iteration 165, loss = 0.70038903\n",
      "Iteration 166, loss = 0.67228855\n",
      "Iteration 167, loss = 0.67744843\n",
      "Iteration 168, loss = 0.67011424\n",
      "Iteration 169, loss = 0.66618459\n",
      "Iteration 170, loss = 0.67166973\n",
      "Iteration 171, loss = 0.67169361\n",
      "Iteration 172, loss = 0.66562721\n",
      "Iteration 173, loss = 0.66622838\n",
      "Iteration 174, loss = 0.66963616\n",
      "Iteration 175, loss = 0.66666907\n",
      "Iteration 176, loss = 0.66359531\n",
      "Iteration 177, loss = 0.66587010\n",
      "Iteration 178, loss = 0.66633818\n",
      "Iteration 179, loss = 0.66309831\n",
      "Iteration 180, loss = 0.66280395\n",
      "Iteration 181, loss = 0.66443607\n",
      "Iteration 182, loss = 0.66283430\n",
      "Iteration 183, loss = 0.66105658\n",
      "Iteration 184, loss = 0.66205591\n",
      "Iteration 185, loss = 0.66191677\n",
      "Iteration 186, loss = 0.66003011\n",
      "Iteration 187, loss = 0.65996817\n",
      "Iteration 188, loss = 0.66041642\n",
      "Iteration 189, loss = 0.65907136\n",
      "Iteration 190, loss = 0.65830416\n",
      "Iteration 191, loss = 0.65869307\n",
      "Iteration 192, loss = 0.65790436\n",
      "Iteration 193, loss = 0.65687603\n",
      "Iteration 194, loss = 0.65697468\n",
      "Iteration 195, loss = 0.65651763\n",
      "Iteration 196, loss = 0.65549266\n",
      "Iteration 197, loss = 0.65532127\n",
      "Iteration 198, loss = 0.65498844\n",
      "Iteration 199, loss = 0.65405243\n",
      "Iteration 200, loss = 0.65370358\n",
      "Iteration 201, loss = 0.65337271\n",
      "Iteration 202, loss = 0.65252176\n",
      "Iteration 203, loss = 0.65207931\n",
      "Iteration 204, loss = 0.65169603\n",
      "Iteration 205, loss = 0.65090050\n",
      "Iteration 206, loss = 0.65041460\n",
      "Iteration 207, loss = 0.64995816\n",
      "Iteration 208, loss = 0.64919414\n",
      "Iteration 209, loss = 0.64868949\n",
      "Iteration 210, loss = 0.64815264\n",
      "Iteration 211, loss = 0.64740975\n",
      "Iteration 212, loss = 0.64688902\n",
      "Iteration 213, loss = 0.64626990\n",
      "Iteration 214, loss = 0.64555064\n",
      "Iteration 215, loss = 0.64500074\n",
      "Iteration 216, loss = 0.64430710\n",
      "Iteration 217, loss = 0.64361645\n",
      "Iteration 218, loss = 0.64301111\n",
      "Iteration 219, loss = 0.64226641\n",
      "Iteration 220, loss = 0.64159779\n",
      "Iteration 221, loss = 0.64091331\n",
      "Iteration 222, loss = 0.64015424\n",
      "Iteration 223, loss = 0.63947492\n",
      "Iteration 224, loss = 0.63871304\n",
      "Iteration 225, loss = 0.63796593\n",
      "Iteration 226, loss = 0.63723569\n",
      "Iteration 227, loss = 0.63643197\n",
      "Iteration 228, loss = 0.63568795\n",
      "Iteration 229, loss = 0.63486473\n",
      "Iteration 230, loss = 0.63408421\n",
      "Iteration 231, loss = 0.63326504\n",
      "Iteration 232, loss = 0.63244362\n",
      "Iteration 233, loss = 0.63161690\n",
      "Iteration 234, loss = 0.63075971\n",
      "Iteration 235, loss = 0.62991692\n",
      "Iteration 236, loss = 0.62903272\n",
      "Iteration 237, loss = 0.62816121\n",
      "Iteration 238, loss = 0.62726480\n",
      "Iteration 239, loss = 0.62635944\n",
      "Iteration 240, loss = 0.62545098\n",
      "Iteration 241, loss = 0.62453024\n",
      "Iteration 242, loss = 0.62359816\n",
      "Iteration 243, loss = 0.62265109\n",
      "Iteration 244, loss = 0.62169039\n",
      "Iteration 245, loss = 0.62071299\n",
      "Iteration 246, loss = 0.61972044\n",
      "Iteration 247, loss = 0.61870706\n",
      "Iteration 248, loss = 0.61767663\n",
      "Iteration 249, loss = 0.61662146\n",
      "Iteration 250, loss = 0.61554533\n",
      "Iteration 251, loss = 0.61443966\n",
      "Iteration 252, loss = 0.61331640\n",
      "Iteration 253, loss = 0.61216078\n",
      "Iteration 254, loss = 0.61094571\n",
      "Iteration 255, loss = 0.60972835\n",
      "Iteration 256, loss = 0.60843660\n",
      "Iteration 257, loss = 0.60714967\n",
      "Iteration 258, loss = 0.60579466\n",
      "Iteration 259, loss = 0.60441918\n",
      "Iteration 260, loss = 0.60303980\n",
      "Iteration 261, loss = 0.60163346\n",
      "Iteration 262, loss = 0.60022164\n",
      "Iteration 263, loss = 0.59880193\n",
      "Iteration 264, loss = 0.59741128\n",
      "Iteration 265, loss = 0.59617436\n",
      "Iteration 266, loss = 0.59487057\n",
      "Iteration 267, loss = 0.59351727\n",
      "Iteration 268, loss = 0.59204551\n",
      "Iteration 269, loss = 0.59061174\n",
      "Iteration 270, loss = 0.58921148\n",
      "Iteration 271, loss = 0.58780199\n",
      "Iteration 272, loss = 0.58651691\n",
      "Iteration 273, loss = 0.58522439\n",
      "Iteration 274, loss = 0.58408676\n",
      "Iteration 275, loss = 0.58363878\n",
      "Iteration 276, loss = 0.58488327\n",
      "Iteration 277, loss = 0.59750491\n",
      "Iteration 278, loss = 0.60288214\n",
      "Iteration 279, loss = 0.61633072\n",
      "Iteration 280, loss = 0.57519641\n",
      "Iteration 281, loss = 0.60896227\n",
      "Iteration 282, loss = 0.63083371\n",
      "Iteration 283, loss = 0.58059361\n",
      "Iteration 284, loss = 0.66058986\n",
      "Iteration 285, loss = 0.57102359\n",
      "Iteration 286, loss = 0.62077154\n",
      "Iteration 287, loss = 0.56798003\n",
      "Iteration 288, loss = 0.62523102\n",
      "Iteration 289, loss = 0.57432821\n",
      "Iteration 290, loss = 0.60377854\n",
      "Iteration 291, loss = 0.56604081\n",
      "Iteration 292, loss = 0.60484399\n",
      "Iteration 293, loss = 0.56980046\n",
      "Iteration 294, loss = 0.59131427\n",
      "Iteration 295, loss = 0.56243919\n",
      "Iteration 296, loss = 0.59169048\n",
      "Iteration 297, loss = 0.56545930\n",
      "Iteration 298, loss = 0.58168145\n",
      "Iteration 299, loss = 0.55891146\n",
      "Iteration 300, loss = 0.58163473\n",
      "Iteration 1, loss = 0.79565985\n",
      "Iteration 2, loss = 0.85044356\n",
      "Iteration 3, loss = 0.75143004\n",
      "Iteration 4, loss = 0.69584455\n",
      "Iteration 5, loss = 0.69320295\n",
      "Iteration 6, loss = 0.69594828\n",
      "Iteration 7, loss = 0.69968646\n",
      "Iteration 8, loss = 0.70084064\n",
      "Iteration 9, loss = 0.69884091\n",
      "Iteration 10, loss = 0.69551848\n",
      "Iteration 11, loss = 0.69319637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.69302578\n",
      "Iteration 13, loss = 0.69445589\n",
      "Iteration 14, loss = 0.69595034\n",
      "Iteration 15, loss = 0.69625159\n",
      "Iteration 16, loss = 0.69519704\n",
      "Iteration 17, loss = 0.69361069\n",
      "Iteration 18, loss = 0.69254576\n",
      "Iteration 19, loss = 0.69251238\n",
      "Iteration 20, loss = 0.69321115\n",
      "Iteration 21, loss = 0.69388050\n",
      "Iteration 22, loss = 0.69391880\n",
      "Iteration 23, loss = 0.69328875\n",
      "Iteration 24, loss = 0.69245009\n",
      "Iteration 25, loss = 0.69194549\n",
      "Iteration 26, loss = 0.69198709\n",
      "Iteration 27, loss = 0.69234563\n",
      "Iteration 28, loss = 0.69258985\n",
      "Iteration 29, loss = 0.69244900\n",
      "Iteration 30, loss = 0.69199843\n",
      "Iteration 31, loss = 0.69154691\n",
      "Iteration 32, loss = 0.69135631\n",
      "Iteration 33, loss = 0.69143915\n",
      "Iteration 34, loss = 0.69158609\n",
      "Iteration 35, loss = 0.69157045\n",
      "Iteration 36, loss = 0.69133739\n",
      "Iteration 37, loss = 0.69102308\n",
      "Iteration 38, loss = 0.69081129\n",
      "Iteration 39, loss = 0.69076984\n",
      "Iteration 40, loss = 0.69080924\n",
      "Iteration 41, loss = 0.69078396\n",
      "Iteration 42, loss = 0.69062906\n",
      "Iteration 43, loss = 0.69040555\n",
      "Iteration 44, loss = 0.69022627\n",
      "Iteration 45, loss = 0.69014548\n",
      "Iteration 46, loss = 0.69011824\n",
      "Iteration 47, loss = 0.69005707\n",
      "Iteration 48, loss = 0.68992005\n",
      "Iteration 49, loss = 0.68974310\n",
      "Iteration 50, loss = 0.68959376\n",
      "Iteration 51, loss = 0.68950154\n",
      "Iteration 52, loss = 0.68943508\n",
      "Iteration 53, loss = 0.68934219\n",
      "Iteration 54, loss = 0.68920462\n",
      "Iteration 55, loss = 0.68905088\n",
      "Iteration 56, loss = 0.68892024\n",
      "Iteration 57, loss = 0.68882115\n",
      "Iteration 58, loss = 0.68872745\n",
      "Iteration 59, loss = 0.68861077\n",
      "Iteration 60, loss = 0.68847036\n",
      "Iteration 61, loss = 0.68832946\n",
      "Iteration 62, loss = 0.68820691\n",
      "Iteration 63, loss = 0.68809772\n",
      "Iteration 64, loss = 0.68798215\n",
      "Iteration 65, loss = 0.68784951\n",
      "Iteration 66, loss = 0.68770836\n",
      "Iteration 67, loss = 0.68757400\n",
      "Iteration 68, loss = 0.68745032\n",
      "Iteration 69, loss = 0.68732726\n",
      "Iteration 70, loss = 0.68719440\n",
      "Iteration 71, loss = 0.68705286\n",
      "Iteration 72, loss = 0.68691212\n",
      "Iteration 73, loss = 0.68677772\n",
      "Iteration 74, loss = 0.68664538\n",
      "Iteration 75, loss = 0.68650762\n",
      "Iteration 76, loss = 0.68636308\n",
      "Iteration 77, loss = 0.68621718\n",
      "Iteration 78, loss = 0.68607443\n",
      "Iteration 79, loss = 0.68593314\n",
      "Iteration 80, loss = 0.68578841\n",
      "Iteration 81, loss = 0.68563871\n",
      "Iteration 82, loss = 0.68548716\n",
      "Iteration 83, loss = 0.68533689\n",
      "Iteration 84, loss = 0.68518704\n",
      "Iteration 85, loss = 0.68503450\n",
      "Iteration 86, loss = 0.68487821\n",
      "Iteration 87, loss = 0.68472016\n",
      "Iteration 88, loss = 0.68456229\n",
      "Iteration 89, loss = 0.68440394\n",
      "Iteration 90, loss = 0.68424309\n",
      "Iteration 91, loss = 0.68407925\n",
      "Iteration 92, loss = 0.68391382\n",
      "Iteration 93, loss = 0.68374787\n",
      "Iteration 94, loss = 0.68358071\n",
      "Iteration 95, loss = 0.68341107\n",
      "Iteration 96, loss = 0.68323892\n",
      "Iteration 97, loss = 0.68306527\n",
      "Iteration 98, loss = 0.68289054\n",
      "Iteration 99, loss = 0.68271403\n",
      "Iteration 100, loss = 0.68253505\n",
      "Iteration 101, loss = 0.68235387\n",
      "Iteration 102, loss = 0.68217110\n",
      "Iteration 103, loss = 0.68198674\n",
      "Iteration 104, loss = 0.68180021\n",
      "Iteration 105, loss = 0.68161125\n",
      "Iteration 106, loss = 0.68142022\n",
      "Iteration 107, loss = 0.68122738\n",
      "Iteration 108, loss = 0.68103248\n",
      "Iteration 109, loss = 0.68083516\n",
      "Iteration 110, loss = 0.68063545\n",
      "Iteration 111, loss = 0.68043361\n",
      "Iteration 112, loss = 0.68022972\n",
      "Iteration 113, loss = 0.68002321\n",
      "Iteration 114, loss = 0.67981424\n",
      "Iteration 115, loss = 0.67960285\n",
      "Iteration 116, loss = 0.67938912\n",
      "Iteration 117, loss = 0.67917286\n",
      "Iteration 118, loss = 0.67895391\n",
      "Iteration 119, loss = 0.67873229\n",
      "Iteration 120, loss = 0.67850809\n",
      "Iteration 121, loss = 0.67828120\n",
      "Iteration 122, loss = 0.67805145\n",
      "Iteration 123, loss = 0.67781883\n",
      "Iteration 124, loss = 0.67758337\n",
      "Iteration 125, loss = 0.67734501\n",
      "Iteration 126, loss = 0.67710361\n",
      "Iteration 127, loss = 0.67685911\n",
      "Iteration 128, loss = 0.67661152\n",
      "Iteration 129, loss = 0.67636080\n",
      "Iteration 130, loss = 0.67610681\n",
      "Iteration 131, loss = 0.67584949\n",
      "Iteration 132, loss = 0.67558883\n",
      "Iteration 133, loss = 0.67532476\n",
      "Iteration 134, loss = 0.67505718\n",
      "Iteration 135, loss = 0.67478602\n",
      "Iteration 136, loss = 0.67451123\n",
      "Iteration 137, loss = 0.67423276\n",
      "Iteration 138, loss = 0.67395050\n",
      "Iteration 139, loss = 0.67366437\n",
      "Iteration 140, loss = 0.67337433\n",
      "Iteration 141, loss = 0.67308030\n",
      "Iteration 142, loss = 0.67278217\n",
      "Iteration 143, loss = 0.67247987\n",
      "Iteration 144, loss = 0.67217334\n",
      "Iteration 145, loss = 0.67186249\n",
      "Iteration 146, loss = 0.67154721\n",
      "Iteration 147, loss = 0.67122743\n",
      "Iteration 148, loss = 0.67090308\n",
      "Iteration 149, loss = 0.67057404\n",
      "Iteration 150, loss = 0.67024022\n",
      "Iteration 151, loss = 0.66990155\n",
      "Iteration 152, loss = 0.66955792\n",
      "Iteration 153, loss = 0.66920923\n",
      "Iteration 154, loss = 0.66885538\n",
      "Iteration 155, loss = 0.66849627\n",
      "Iteration 156, loss = 0.66813180\n",
      "Iteration 157, loss = 0.66776187\n",
      "Iteration 158, loss = 0.66738636\n",
      "Iteration 159, loss = 0.66700517\n",
      "Iteration 160, loss = 0.66661819\n",
      "Iteration 161, loss = 0.66622530\n",
      "Iteration 162, loss = 0.66582639\n",
      "Iteration 163, loss = 0.66542134\n",
      "Iteration 164, loss = 0.66501001\n",
      "Iteration 165, loss = 0.69492492\n",
      "Iteration 166, loss = 0.67254232\n",
      "Iteration 167, loss = 0.67401406\n",
      "Iteration 168, loss = 0.66428143\n",
      "Iteration 169, loss = 0.66653289\n",
      "Iteration 170, loss = 0.67122309\n",
      "Iteration 171, loss = 0.66369782\n",
      "Iteration 172, loss = 0.66354007\n",
      "Iteration 173, loss = 0.66779909\n",
      "Iteration 174, loss = 0.66348655\n",
      "Iteration 175, loss = 0.66095994\n",
      "Iteration 176, loss = 0.66442513\n",
      "Iteration 177, loss = 0.66259993\n",
      "Iteration 178, loss = 0.65935207\n",
      "Iteration 179, loss = 0.66147119\n",
      "Iteration 180, loss = 0.66110570\n",
      "Iteration 181, loss = 0.65807802\n",
      "Iteration 182, loss = 0.65904640\n",
      "Iteration 183, loss = 0.65929115\n",
      "Iteration 184, loss = 0.65678499\n",
      "Iteration 185, loss = 0.65701143\n",
      "Iteration 186, loss = 0.65736281\n",
      "Iteration 187, loss = 0.65534672\n",
      "Iteration 188, loss = 0.65520170\n",
      "Iteration 189, loss = 0.65542146\n",
      "Iteration 190, loss = 0.65376437\n",
      "Iteration 191, loss = 0.65347836\n",
      "Iteration 192, loss = 0.65348235\n",
      "Iteration 193, loss = 0.65206319\n",
      "Iteration 194, loss = 0.65176432\n",
      "Iteration 195, loss = 0.65152527\n",
      "Iteration 196, loss = 0.65027942\n",
      "Iteration 197, loss = 0.65000733\n",
      "Iteration 198, loss = 0.64951945\n",
      "Iteration 199, loss = 0.64843718\n",
      "Iteration 200, loss = 0.64817074\n",
      "Iteration 201, loss = 0.64744924\n",
      "Iteration 202, loss = 0.64655353\n",
      "Iteration 203, loss = 0.64621380\n",
      "Iteration 204, loss = 0.64531989\n",
      "Iteration 205, loss = 0.64462122\n",
      "Iteration 206, loss = 0.64410529\n",
      "Iteration 207, loss = 0.64316341\n",
      "Iteration 208, loss = 0.64259502\n",
      "Iteration 209, loss = 0.64184732\n",
      "Iteration 210, loss = 0.64100185\n",
      "Iteration 211, loss = 0.64040368\n",
      "Iteration 212, loss = 0.63950287\n",
      "Iteration 213, loss = 0.63879380\n",
      "Iteration 214, loss = 0.63801601\n",
      "Iteration 215, loss = 0.63714421\n",
      "Iteration 216, loss = 0.63642894\n",
      "Iteration 217, loss = 0.63551197\n",
      "Iteration 218, loss = 0.63473573\n",
      "Iteration 219, loss = 0.63386053\n",
      "Iteration 220, loss = 0.63298430\n",
      "Iteration 221, loss = 0.63214451\n",
      "Iteration 222, loss = 0.63120127\n",
      "Iteration 223, loss = 0.63035051\n",
      "Iteration 224, loss = 0.62938269\n",
      "Iteration 225, loss = 0.62849128\n",
      "Iteration 226, loss = 0.62750539\n",
      "Iteration 227, loss = 0.62657298\n",
      "Iteration 228, loss = 0.62557467\n",
      "Iteration 229, loss = 0.62463379\n",
      "Iteration 230, loss = 0.62359159\n",
      "Iteration 231, loss = 0.62263745\n",
      "Iteration 232, loss = 0.62156828\n",
      "Iteration 233, loss = 0.62055832\n",
      "Iteration 234, loss = 0.61947272\n",
      "Iteration 235, loss = 0.61843193\n",
      "Iteration 236, loss = 0.61731933\n",
      "Iteration 237, loss = 0.61623406\n",
      "Iteration 238, loss = 0.61514538\n",
      "Iteration 239, loss = 0.61397872\n",
      "Iteration 240, loss = 0.61285328\n",
      "Iteration 241, loss = 0.61169287\n",
      "Iteration 242, loss = 0.61050746\n",
      "Iteration 243, loss = 0.60934775\n",
      "Iteration 244, loss = 0.60812564\n",
      "Iteration 245, loss = 0.60686032\n",
      "Iteration 246, loss = 0.60562612\n",
      "Iteration 247, loss = 0.60437026\n",
      "Iteration 248, loss = 0.60302896\n",
      "Iteration 249, loss = 0.60167193\n",
      "Iteration 250, loss = 0.60029386\n",
      "Iteration 251, loss = 0.59887341\n",
      "Iteration 252, loss = 0.59742979\n",
      "Iteration 253, loss = 0.59597786\n",
      "Iteration 254, loss = 0.59455397\n",
      "Iteration 255, loss = 0.59324682\n",
      "Iteration 256, loss = 0.59210404\n",
      "Iteration 257, loss = 0.59171880\n",
      "Iteration 258, loss = 0.59331784\n",
      "Iteration 259, loss = 0.59383196\n",
      "Iteration 260, loss = 0.60937448\n",
      "Iteration 261, loss = 0.58977802\n",
      "Iteration 262, loss = 0.58266269\n",
      "Iteration 263, loss = 0.58244557\n",
      "Iteration 264, loss = 0.58675799\n",
      "Iteration 265, loss = 0.60146189\n",
      "Iteration 266, loss = 0.58032901\n",
      "Iteration 267, loss = 0.57768699\n",
      "Iteration 268, loss = 0.58748383\n",
      "Iteration 269, loss = 0.57778516\n",
      "Iteration 270, loss = 0.57190427\n",
      "Iteration 271, loss = 0.57418642\n",
      "Iteration 272, loss = 0.57707712\n",
      "Iteration 273, loss = 0.58088513\n",
      "Iteration 274, loss = 0.56746201\n",
      "Iteration 275, loss = 0.57203638\n",
      "Iteration 276, loss = 0.58977881\n",
      "Iteration 277, loss = 0.56424452\n",
      "Iteration 278, loss = 0.57486227\n",
      "Iteration 279, loss = 0.60009680\n",
      "Iteration 280, loss = 0.55923655\n",
      "Iteration 281, loss = 0.60242006\n",
      "Iteration 282, loss = 0.58978904\n",
      "Iteration 283, loss = 0.57200788\n",
      "Iteration 284, loss = 0.60672421\n",
      "Iteration 285, loss = 0.55499692\n",
      "Iteration 286, loss = 0.58289796\n",
      "Iteration 287, loss = 0.55322386\n",
      "Iteration 288, loss = 0.57748031\n",
      "Iteration 289, loss = 0.56839944\n",
      "Iteration 290, loss = 0.56304016\n",
      "Iteration 291, loss = 0.57084939\n",
      "Iteration 292, loss = 0.54678154\n",
      "Iteration 293, loss = 0.55967167\n",
      "Iteration 294, loss = 0.54462621\n",
      "Iteration 295, loss = 0.55855711\n",
      "Iteration 296, loss = 0.55071390\n",
      "Iteration 297, loss = 0.54859721\n",
      "Iteration 298, loss = 0.55097127\n",
      "Iteration 299, loss = 0.53908871\n",
      "Iteration 300, loss = 0.54442268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69446993\n",
      "Iteration 2, loss = 0.69423988\n",
      "Iteration 3, loss = 0.69359636\n",
      "Iteration 4, loss = 0.69356308\n",
      "Iteration 5, loss = 0.69400445\n",
      "Iteration 6, loss = 0.69443100\n",
      "Iteration 7, loss = 0.69509529\n",
      "Iteration 8, loss = 0.69637884\n",
      "Iteration 9, loss = 0.69972025\n",
      "Iteration 10, loss = 0.70461122\n",
      "Iteration 11, loss = 0.69162668\n",
      "Iteration 12, loss = 0.69345060\n",
      "Iteration 13, loss = 0.69298666\n",
      "Iteration 14, loss = 0.69080696\n",
      "Iteration 15, loss = 0.69039780\n",
      "Iteration 16, loss = 0.69003676\n",
      "Iteration 17, loss = 0.68973333\n",
      "Iteration 18, loss = 0.68942183\n",
      "Iteration 19, loss = 0.68910531\n",
      "Iteration 20, loss = 0.68879457\n",
      "Iteration 21, loss = 0.68845779\n",
      "Iteration 22, loss = 0.68812373\n",
      "Iteration 23, loss = 0.68778615\n",
      "Iteration 24, loss = 0.68744334\n",
      "Iteration 25, loss = 0.68709550\n",
      "Iteration 26, loss = 0.68674253\n",
      "Iteration 27, loss = 0.68638439\n",
      "Iteration 28, loss = 0.68602097\n",
      "Iteration 29, loss = 0.68565217\n",
      "Iteration 30, loss = 0.68527784\n",
      "Iteration 31, loss = 0.68489783\n",
      "Iteration 32, loss = 0.68451194\n",
      "Iteration 33, loss = 0.68411997\n",
      "Iteration 34, loss = 0.68372169\n",
      "Iteration 35, loss = 0.68331685\n",
      "Iteration 36, loss = 0.68290518\n",
      "Iteration 37, loss = 0.68248639\n",
      "Iteration 38, loss = 0.68206020\n",
      "Iteration 39, loss = 0.68162628\n",
      "Iteration 40, loss = 0.68118429\n",
      "Iteration 41, loss = 0.68073388\n",
      "Iteration 42, loss = 0.68027468\n",
      "Iteration 43, loss = 0.67980632\n",
      "Iteration 44, loss = 0.67932838\n",
      "Iteration 45, loss = 0.67884046\n",
      "Iteration 46, loss = 0.67834211\n",
      "Iteration 47, loss = 0.67783289\n",
      "Iteration 48, loss = 0.67731233\n",
      "Iteration 49, loss = 0.67677995\n",
      "Iteration 50, loss = 0.67623524\n",
      "Iteration 51, loss = 0.67567768\n",
      "Iteration 52, loss = 0.67510674\n",
      "Iteration 53, loss = 0.67452184\n",
      "Iteration 54, loss = 0.67392240\n",
      "Iteration 55, loss = 0.67330784\n",
      "Iteration 56, loss = 0.67267751\n",
      "Iteration 57, loss = 0.67203078\n",
      "Iteration 58, loss = 0.67136697\n",
      "Iteration 59, loss = 0.67068539\n",
      "Iteration 60, loss = 0.66998531\n",
      "Iteration 61, loss = 0.66926599\n",
      "Iteration 62, loss = 0.66852666\n",
      "Iteration 63, loss = 0.66776650\n",
      "Iteration 64, loss = 0.66698468\n",
      "Iteration 65, loss = 0.66618033\n",
      "Iteration 66, loss = 0.66535256\n",
      "Iteration 67, loss = 0.66450043\n",
      "Iteration 68, loss = 0.66362297\n",
      "Iteration 69, loss = 0.66271918\n",
      "Iteration 70, loss = 0.66178800\n",
      "Iteration 71, loss = 0.66082836\n",
      "Iteration 72, loss = 0.65983913\n",
      "Iteration 73, loss = 0.65881914\n",
      "Iteration 74, loss = 0.65776718\n",
      "Iteration 75, loss = 0.65668197\n",
      "Iteration 76, loss = 0.65556222\n",
      "Iteration 77, loss = 0.65440656\n",
      "Iteration 78, loss = 0.65321358\n",
      "Iteration 79, loss = 0.65198663\n",
      "Iteration 80, loss = 0.65074265\n",
      "Iteration 81, loss = 0.64957437\n",
      "Iteration 82, loss = 0.64925177\n",
      "Iteration 83, loss = 0.65665269\n",
      "Iteration 84, loss = 0.71541157\n",
      "Iteration 85, loss = 0.94734357\n",
      "Iteration 86, loss = 0.69689903\n",
      "Iteration 87, loss = 0.70070499\n",
      "Iteration 88, loss = 0.70245554\n",
      "Iteration 89, loss = 0.70101769\n",
      "Iteration 90, loss = 0.69743707\n",
      "Iteration 91, loss = 0.69448639\n",
      "Iteration 92, loss = 0.69384245\n",
      "Iteration 93, loss = 0.69468892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69468780\n",
      "Iteration 2, loss = 0.69453757\n",
      "Iteration 3, loss = 0.69412197\n",
      "Iteration 4, loss = 0.69398782\n",
      "Iteration 5, loss = 0.69381609\n",
      "Iteration 6, loss = 0.69363409\n",
      "Iteration 7, loss = 0.69342395\n",
      "Iteration 8, loss = 0.69323319\n",
      "Iteration 9, loss = 0.69312867\n",
      "Iteration 10, loss = 0.69275756\n",
      "Iteration 11, loss = 0.69243211\n",
      "Iteration 12, loss = 0.69214357\n",
      "Iteration 13, loss = 0.69183998\n",
      "Iteration 14, loss = 0.69152821\n",
      "Iteration 15, loss = 0.69120811\n",
      "Iteration 16, loss = 0.69087929\n",
      "Iteration 17, loss = 0.69054351\n",
      "Iteration 18, loss = 0.69020046\n",
      "Iteration 19, loss = 0.68985098\n",
      "Iteration 20, loss = 0.68949378\n",
      "Iteration 21, loss = 0.68913039\n",
      "Iteration 22, loss = 0.68875876\n",
      "Iteration 23, loss = 0.68837472\n",
      "Iteration 24, loss = 0.68797538\n",
      "Iteration 25, loss = 0.68756603\n",
      "Iteration 26, loss = 0.68715526\n",
      "Iteration 27, loss = 0.68674308\n",
      "Iteration 28, loss = 0.68632527\n",
      "Iteration 29, loss = 0.68590815\n",
      "Iteration 30, loss = 0.68548215\n",
      "Iteration 31, loss = 0.68489659\n",
      "Iteration 32, loss = 0.68403440\n",
      "Iteration 33, loss = 0.68412395\n",
      "Iteration 34, loss = 0.68465886\n",
      "Iteration 35, loss = 0.72304211\n",
      "Iteration 36, loss = 0.70107077\n",
      "Iteration 37, loss = 0.69970828\n",
      "Iteration 38, loss = 0.69753176\n",
      "Iteration 39, loss = 0.69504778\n",
      "Iteration 40, loss = 0.69298256\n",
      "Iteration 41, loss = 0.69195991\n",
      "Iteration 42, loss = 0.69214722\n",
      "Iteration 43, loss = 0.69301382\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69475916\n",
      "Iteration 2, loss = 0.69422390\n",
      "Iteration 3, loss = 0.69435093\n",
      "Iteration 4, loss = 0.69474129\n",
      "Iteration 5, loss = 0.69516175\n",
      "Iteration 6, loss = 0.69602229\n",
      "Iteration 7, loss = 0.69741267\n",
      "Iteration 8, loss = 0.69965589\n",
      "Iteration 9, loss = 0.70301005\n",
      "Iteration 10, loss = 0.70774848\n",
      "Iteration 11, loss = 0.69535590\n",
      "Iteration 12, loss = 0.69340557\n",
      "Iteration 13, loss = 0.69202930\n",
      "Iteration 14, loss = 0.69190598\n",
      "Iteration 15, loss = 0.69122048\n",
      "Iteration 16, loss = 0.69086896\n",
      "Iteration 17, loss = 0.69060698\n",
      "Iteration 18, loss = 0.69032997\n",
      "Iteration 19, loss = 0.69005824\n",
      "Iteration 20, loss = 0.68978396\n",
      "Iteration 21, loss = 0.68950303\n",
      "Iteration 22, loss = 0.68921948\n",
      "Iteration 23, loss = 0.68893311\n",
      "Iteration 24, loss = 0.68864144\n",
      "Iteration 25, loss = 0.68834632\n",
      "Iteration 26, loss = 0.68805162\n",
      "Iteration 27, loss = 0.68774605\n",
      "Iteration 28, loss = 0.68744227\n",
      "Iteration 29, loss = 0.68713010\n",
      "Iteration 30, loss = 0.68681727\n",
      "Iteration 31, loss = 0.68649913\n",
      "Iteration 32, loss = 0.68617826\n",
      "Iteration 33, loss = 0.68585152\n",
      "Iteration 34, loss = 0.68552279\n",
      "Iteration 35, loss = 0.68518651\n",
      "Iteration 36, loss = 0.68484700\n",
      "Iteration 37, loss = 0.68450400\n",
      "Iteration 38, loss = 0.68415277\n",
      "Iteration 39, loss = 0.68379786\n",
      "Iteration 40, loss = 0.68343800\n",
      "Iteration 41, loss = 0.68307283\n",
      "Iteration 42, loss = 0.68269853\n",
      "Iteration 43, loss = 0.68231981\n",
      "Iteration 44, loss = 0.68193448\n",
      "Iteration 45, loss = 0.68154225\n",
      "Iteration 46, loss = 0.68114282\n",
      "Iteration 47, loss = 0.68073590\n",
      "Iteration 48, loss = 0.68032115\n",
      "Iteration 49, loss = 0.67989827\n",
      "Iteration 50, loss = 0.67946690\n",
      "Iteration 51, loss = 0.67902670\n",
      "Iteration 52, loss = 0.67857729\n",
      "Iteration 53, loss = 0.67811864\n",
      "Iteration 54, loss = 0.67765146\n",
      "Iteration 55, loss = 0.67717477\n",
      "Iteration 56, loss = 0.67668594\n",
      "Iteration 57, loss = 0.67618152\n",
      "Iteration 58, loss = 0.67566734\n",
      "Iteration 59, loss = 0.67514323\n",
      "Iteration 60, loss = 0.67460495\n",
      "Iteration 61, loss = 0.67405425\n",
      "Iteration 62, loss = 0.67349044\n",
      "Iteration 63, loss = 0.67291127\n",
      "Iteration 64, loss = 0.67231815\n",
      "Iteration 65, loss = 0.67170978\n",
      "Iteration 66, loss = 0.67108556\n",
      "Iteration 67, loss = 0.67044489\n",
      "Iteration 68, loss = 0.66978716\n",
      "Iteration 69, loss = 0.66911173\n",
      "Iteration 70, loss = 0.66841881\n",
      "Iteration 71, loss = 0.66771889\n",
      "Iteration 72, loss = 0.66698925\n",
      "Iteration 73, loss = 0.66623705\n",
      "Iteration 74, loss = 0.66545899\n",
      "Iteration 75, loss = 0.66465470\n",
      "Iteration 76, loss = 0.66384907\n",
      "Iteration 77, loss = 0.66300246\n",
      "Iteration 78, loss = 0.66213242\n",
      "Iteration 79, loss = 0.66123129\n",
      "Iteration 80, loss = 0.66033249\n",
      "Iteration 81, loss = 0.65937782\n",
      "Iteration 82, loss = 0.65840066\n",
      "Iteration 83, loss = 0.65738719\n",
      "Iteration 84, loss = 0.65636566\n",
      "Iteration 85, loss = 0.65530173\n",
      "Iteration 86, loss = 0.65420746\n",
      "Iteration 87, loss = 0.65310021\n",
      "Iteration 88, loss = 0.65212031\n",
      "Iteration 89, loss = 0.65177045\n",
      "Iteration 90, loss = 0.65595294\n",
      "Iteration 91, loss = 0.69585725\n",
      "Iteration 92, loss = 0.67687784\n",
      "Iteration 93, loss = 0.67457004\n",
      "Iteration 94, loss = 0.67246155\n",
      "Iteration 95, loss = 0.67175566\n",
      "Iteration 96, loss = 0.67102734\n",
      "Iteration 97, loss = 0.67027472\n",
      "Iteration 98, loss = 0.66950487\n",
      "Iteration 99, loss = 0.66951442\n",
      "Iteration 100, loss = 0.66740206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69446993\n",
      "Iteration 2, loss = 4.84389217\n",
      "Iteration 3, loss = 9.38564507\n",
      "Iteration 4, loss = 7.79094789\n",
      "Iteration 5, loss = 0.98651450\n",
      "Iteration 6, loss = 6.23415052\n",
      "Iteration 7, loss = 3.34749452\n",
      "Iteration 8, loss = 1.06975166\n",
      "Iteration 9, loss = 0.77630098\n",
      "Iteration 10, loss = 0.74656877\n",
      "Iteration 11, loss = 1.04632462\n",
      "Iteration 12, loss = 1.09726060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69468780\n",
      "Iteration 2, loss = 7.81988097\n",
      "Iteration 3, loss = 1.94765222\n",
      "Iteration 4, loss = 0.70298985\n",
      "Iteration 5, loss = 0.70344044\n",
      "Iteration 6, loss = 0.70362711\n",
      "Iteration 7, loss = 0.70363374\n",
      "Iteration 8, loss = 0.70351179\n",
      "Iteration 9, loss = 0.70329601\n",
      "Iteration 10, loss = 0.70301017\n",
      "Iteration 11, loss = 0.70267006\n",
      "Iteration 12, loss = 0.70228623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69475916\n",
      "Iteration 2, loss = 4.78405148\n",
      "Iteration 3, loss = 9.52899886\n",
      "Iteration 4, loss = 8.07798285\n",
      "Iteration 5, loss = 1.29806957\n",
      "Iteration 6, loss = 6.60597371\n",
      "Iteration 7, loss = 3.82261957\n",
      "Iteration 8, loss = 1.13473652\n",
      "Iteration 9, loss = 0.81900147\n",
      "Iteration 10, loss = 0.72310596\n",
      "Iteration 11, loss = 1.04245757\n",
      "Iteration 12, loss = 1.15068631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69941293\n",
      "Iteration 2, loss = 1.01348725\n",
      "Iteration 3, loss = 0.82661762\n",
      "Iteration 4, loss = 0.81509042\n",
      "Iteration 5, loss = 0.76960866\n",
      "Iteration 6, loss = 0.71558275\n",
      "Iteration 7, loss = 0.69268967\n",
      "Iteration 8, loss = 0.71383337\n",
      "Iteration 9, loss = 0.73214788\n",
      "Iteration 10, loss = 0.72043841\n",
      "Iteration 11, loss = 0.70136185\n",
      "Iteration 12, loss = 0.69302409\n",
      "Iteration 13, loss = 0.69364268\n",
      "Iteration 14, loss = 0.69575265\n",
      "Iteration 15, loss = 0.69567959\n",
      "Iteration 16, loss = 0.69410073\n",
      "Iteration 17, loss = 0.69286915\n",
      "Iteration 18, loss = 0.69262669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70286440\n",
      "Iteration 2, loss = 1.07069443\n",
      "Iteration 3, loss = 0.79456934\n",
      "Iteration 4, loss = 0.79303114\n",
      "Iteration 5, loss = 0.78403768\n",
      "Iteration 6, loss = 0.77083365\n",
      "Iteration 7, loss = 0.75603175\n",
      "Iteration 8, loss = 0.74090085\n",
      "Iteration 9, loss = 0.72585294\n",
      "Iteration 10, loss = 0.71149184\n",
      "Iteration 11, loss = 0.69962524\n",
      "Iteration 12, loss = 0.69318183\n",
      "Iteration 13, loss = 0.69379697\n",
      "Iteration 14, loss = 0.69871955\n",
      "Iteration 15, loss = 0.70228949\n",
      "Iteration 16, loss = 0.70152030\n",
      "Iteration 17, loss = 0.69794400\n",
      "Iteration 18, loss = 0.69448474\n",
      "Iteration 19, loss = 0.69271242\n",
      "Iteration 20, loss = 0.69256123\n",
      "Iteration 21, loss = 0.69318537\n",
      "Iteration 22, loss = 0.69374701\n",
      "Iteration 23, loss = 0.69381302\n",
      "Iteration 24, loss = 0.69340416\n",
      "Iteration 25, loss = 0.69281559\n",
      "Iteration 26, loss = 0.69235596\n",
      "Iteration 27, loss = 0.69215814\n",
      "Iteration 28, loss = 0.69215492\n",
      "Iteration 29, loss = 0.69219318\n",
      "Iteration 30, loss = 0.69216622\n",
      "Iteration 31, loss = 0.69206327\n",
      "Iteration 32, loss = 0.69193256\n",
      "Iteration 33, loss = 0.69182105\n",
      "Iteration 34, loss = 0.69174310\n",
      "Iteration 35, loss = 0.69168572\n",
      "Iteration 36, loss = 0.69162933\n",
      "Iteration 37, loss = 0.69156313\n",
      "Iteration 38, loss = 0.69148784\n",
      "Iteration 39, loss = 0.69140961\n",
      "Iteration 40, loss = 0.69133339\n",
      "Iteration 41, loss = 0.69126004\n",
      "Iteration 42, loss = 0.69118763\n",
      "Iteration 43, loss = 0.69111402\n",
      "Iteration 44, loss = 0.69103835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70123886\n",
      "Iteration 2, loss = 1.03308605\n",
      "Iteration 3, loss = 0.83890358\n",
      "Iteration 4, loss = 0.82086865\n",
      "Iteration 5, loss = 0.76171819\n",
      "Iteration 6, loss = 0.70300417\n",
      "Iteration 7, loss = 0.69950240\n",
      "Iteration 8, loss = 0.73260074\n",
      "Iteration 9, loss = 0.73229414\n",
      "Iteration 10, loss = 0.70723378\n",
      "Iteration 11, loss = 0.69349889\n",
      "Iteration 12, loss = 0.69393410\n",
      "Iteration 13, loss = 0.69625716\n",
      "Iteration 14, loss = 0.69526903\n",
      "Iteration 15, loss = 0.69331940\n",
      "Iteration 16, loss = 0.69267347\n",
      "Iteration 17, loss = 0.69284659\n",
      "Iteration 18, loss = 0.69288836\n",
      "Iteration 19, loss = 0.69275204\n",
      "Iteration 20, loss = 0.69267473\n",
      "Iteration 21, loss = 0.69267581\n",
      "Iteration 22, loss = 0.69268165\n",
      "Iteration 23, loss = 0.69267355\n",
      "Iteration 24, loss = 0.69266599\n",
      "Iteration 25, loss = 0.69266359\n",
      "Iteration 26, loss = 0.69266226\n",
      "Iteration 27, loss = 0.69266012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69941293\n",
      "Iteration 2, loss = 18.13962850\n",
      "Iteration 3, loss = 11.02252840\n",
      "Iteration 4, loss = 0.75065205\n",
      "Iteration 5, loss = 0.74910685\n",
      "Iteration 6, loss = 0.74613999\n",
      "Iteration 7, loss = 0.74231966\n",
      "Iteration 8, loss = 0.73799020\n",
      "Iteration 9, loss = 0.73338950\n",
      "Iteration 10, loss = 0.72869362\n",
      "Iteration 11, loss = 0.72403825\n",
      "Iteration 12, loss = 0.71953023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70286440\n",
      "Iteration 2, loss = 17.90404907\n",
      "Iteration 3, loss = 11.23493210\n",
      "Iteration 4, loss = 0.75516430\n",
      "Iteration 5, loss = 0.75350287\n",
      "Iteration 6, loss = 0.75035779\n",
      "Iteration 7, loss = 0.74632086\n",
      "Iteration 8, loss = 0.74175013\n",
      "Iteration 9, loss = 0.73689228\n",
      "Iteration 10, loss = 0.73192942\n",
      "Iteration 11, loss = 0.72700160\n",
      "Iteration 12, loss = 0.72221898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70123886\n",
      "Iteration 2, loss = 18.02183871\n",
      "Iteration 3, loss = 11.13985748\n",
      "Iteration 4, loss = 0.75291589\n",
      "Iteration 5, loss = 0.75133064\n",
      "Iteration 6, loss = 0.74828556\n",
      "Iteration 7, loss = 0.74436396\n",
      "Iteration 8, loss = 0.73991831\n",
      "Iteration 9, loss = 0.73519156\n",
      "Iteration 10, loss = 0.73036327\n",
      "Iteration 11, loss = 0.72557160\n",
      "Iteration 12, loss = 0.72092528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79410679\n",
      "Iteration 2, loss = 0.69769070\n",
      "Iteration 3, loss = 0.70328613\n",
      "Iteration 4, loss = 0.69975016\n",
      "Iteration 5, loss = 0.69756939\n",
      "Iteration 6, loss = 0.69619198\n",
      "Iteration 7, loss = 0.69438634\n",
      "Iteration 8, loss = 0.69322676\n",
      "Iteration 9, loss = 0.69285409\n",
      "Iteration 10, loss = 0.69293115\n",
      "Iteration 11, loss = 0.69309116\n",
      "Iteration 12, loss = 0.69313256\n",
      "Iteration 13, loss = 0.69302462\n",
      "Iteration 14, loss = 0.69283358\n",
      "Iteration 15, loss = 0.69264344\n",
      "Iteration 16, loss = 0.69250630\n",
      "Iteration 17, loss = 0.69242977\n",
      "Iteration 18, loss = 0.69239114\n",
      "Iteration 19, loss = 0.69236078\n",
      "Iteration 20, loss = 0.69231941\n",
      "Iteration 21, loss = 0.69226290\n",
      "Iteration 22, loss = 0.69219743\n",
      "Iteration 23, loss = 0.69213166\n",
      "Iteration 24, loss = 0.69207118\n",
      "Iteration 25, loss = 0.69201699\n",
      "Iteration 26, loss = 0.69196688\n",
      "Iteration 27, loss = 0.69191780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79833060\n",
      "Iteration 2, loss = 0.69681312\n",
      "Iteration 3, loss = 0.70314236\n",
      "Iteration 4, loss = 0.69967597\n",
      "Iteration 5, loss = 0.69772608\n",
      "Iteration 6, loss = 0.69625615\n",
      "Iteration 7, loss = 0.69432292\n",
      "Iteration 8, loss = 0.69309096\n",
      "Iteration 9, loss = 0.69270696\n",
      "Iteration 10, loss = 0.69280554\n",
      "Iteration 11, loss = 0.69299345\n",
      "Iteration 12, loss = 0.69305501\n",
      "Iteration 13, loss = 0.69295621\n",
      "Iteration 14, loss = 0.69276658\n",
      "Iteration 15, loss = 0.69257575\n",
      "Iteration 16, loss = 0.69244053\n",
      "Iteration 17, loss = 0.69237046\n",
      "Iteration 18, loss = 0.69234202\n",
      "Iteration 19, loss = 0.69232335\n",
      "Iteration 20, loss = 0.69229314\n",
      "Iteration 21, loss = 0.69224637\n",
      "Iteration 22, loss = 0.69218943\n",
      "Iteration 23, loss = 0.69213169\n",
      "Iteration 24, loss = 0.69207938\n",
      "Iteration 25, loss = 0.69203381\n",
      "Iteration 26, loss = 0.69199270\n",
      "Iteration 27, loss = 0.69195278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79565985\n",
      "Iteration 2, loss = 0.69743614\n",
      "Iteration 3, loss = 0.70338815\n",
      "Iteration 4, loss = 0.69989578\n",
      "Iteration 5, loss = 0.69769164\n",
      "Iteration 6, loss = 0.69632294\n",
      "Iteration 7, loss = 0.69450006\n",
      "Iteration 8, loss = 0.69332435\n",
      "Iteration 9, loss = 0.69294697\n",
      "Iteration 10, loss = 0.69302935\n",
      "Iteration 11, loss = 0.69319974\n",
      "Iteration 12, loss = 0.69325216\n",
      "Iteration 13, loss = 0.69315330\n",
      "Iteration 14, loss = 0.69296877\n",
      "Iteration 15, loss = 0.69278338\n",
      "Iteration 16, loss = 0.69265068\n",
      "Iteration 17, loss = 0.69257943\n",
      "Iteration 18, loss = 0.69254728\n",
      "Iteration 19, loss = 0.69252428\n",
      "Iteration 20, loss = 0.69249054\n",
      "Iteration 21, loss = 0.69244143\n",
      "Iteration 22, loss = 0.69238299\n",
      "Iteration 23, loss = 0.69232398\n",
      "Iteration 24, loss = 0.69227020\n",
      "Iteration 25, loss = 0.69222281\n",
      "Iteration 26, loss = 0.69217968\n",
      "Iteration 27, loss = 0.69213772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79410679\n",
      "Iteration 2, loss = 7.86524816\n",
      "Iteration 3, loss = 3.67038914\n",
      "Iteration 4, loss = 0.73669114\n",
      "Iteration 5, loss = 0.73243238\n",
      "Iteration 6, loss = 0.73283947\n",
      "Iteration 7, loss = 0.73283442\n",
      "Iteration 8, loss = 0.73252783\n",
      "Iteration 9, loss = 0.73199139\n",
      "Iteration 10, loss = 0.73127594\n",
      "Iteration 11, loss = 0.73041928\n",
      "Iteration 12, loss = 0.72945038\n",
      "Iteration 13, loss = 0.72839193\n",
      "Iteration 14, loss = 0.72726197\n",
      "Iteration 15, loss = 0.72607498\n",
      "Iteration 16, loss = 0.72484266\n",
      "Iteration 17, loss = 0.72357454\n",
      "Iteration 18, loss = 0.72227840\n",
      "Iteration 19, loss = 0.72096069\n",
      "Iteration 20, loss = 0.71962761\n",
      "Iteration 21, loss = 0.71828167\n",
      "Iteration 22, loss = 0.71692945\n",
      "Iteration 23, loss = 0.71557422\n",
      "Iteration 24, loss = 0.71421660\n",
      "Iteration 25, loss = 1.11619210\n",
      "Iteration 26, loss = 0.72629916\n",
      "Iteration 27, loss = 0.71016843\n",
      "Iteration 28, loss = 0.70888036\n",
      "Iteration 29, loss = 0.70761936\n",
      "Iteration 30, loss = 0.70638969\n",
      "Iteration 31, loss = 0.70519566\n",
      "Iteration 32, loss = 0.70404156\n",
      "Iteration 33, loss = 0.70293162\n",
      "Iteration 34, loss = 0.70186993\n",
      "Iteration 35, loss = 0.70086040\n",
      "Iteration 36, loss = 0.69990666\n",
      "Iteration 37, loss = 0.69901207\n",
      "Iteration 38, loss = 0.69817955\n",
      "Iteration 39, loss = 0.69741159\n",
      "Iteration 40, loss = 0.69671018\n",
      "Iteration 41, loss = 0.69607666\n",
      "Iteration 42, loss = 0.69551175\n",
      "Iteration 43, loss = 0.69501543\n",
      "Iteration 44, loss = 0.69458686\n",
      "Iteration 45, loss = 0.69422442\n",
      "Iteration 46, loss = 0.69392557\n",
      "Iteration 47, loss = 0.69368696\n",
      "Iteration 48, loss = 0.69350436\n",
      "Iteration 49, loss = 0.69337279\n",
      "Iteration 50, loss = 0.69328659\n",
      "Iteration 51, loss = 0.69323956\n",
      "Iteration 52, loss = 0.69322515\n",
      "Iteration 53, loss = 0.69323663\n",
      "Iteration 54, loss = 0.69326735\n",
      "Iteration 55, loss = 0.69331093\n",
      "Iteration 56, loss = 0.69336148\n",
      "Iteration 57, loss = 0.69341382\n",
      "Iteration 58, loss = 0.69346358\n",
      "Iteration 59, loss = 0.69350730\n",
      "Iteration 60, loss = 0.69354251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79833060\n",
      "Iteration 2, loss = 7.76515300\n",
      "Iteration 3, loss = 3.71309755\n",
      "Iteration 4, loss = 0.77804326\n",
      "Iteration 5, loss = 0.73620991\n",
      "Iteration 6, loss = 0.73666359\n",
      "Iteration 7, loss = 0.73667380\n",
      "Iteration 8, loss = 0.73635942\n",
      "Iteration 9, loss = 0.73579759\n",
      "Iteration 10, loss = 0.73504296\n",
      "Iteration 11, loss = 0.73413617\n",
      "Iteration 12, loss = 0.73310835\n",
      "Iteration 13, loss = 0.73198382\n",
      "Iteration 14, loss = 0.73078188\n",
      "Iteration 15, loss = 0.72951799\n",
      "Iteration 16, loss = 0.72820463\n",
      "Iteration 17, loss = 0.72685189\n",
      "Iteration 18, loss = 0.72546804\n",
      "Iteration 19, loss = 0.72405984\n",
      "Iteration 20, loss = 0.72263300\n",
      "Iteration 21, loss = 0.72119242\n",
      "Iteration 22, loss = 0.71973989\n",
      "Iteration 23, loss = 2.18676709\n",
      "Iteration 24, loss = 0.71800846\n",
      "Iteration 25, loss = 1.12920702\n",
      "Iteration 26, loss = 0.73060694\n",
      "Iteration 27, loss = 0.71654750\n",
      "Iteration 28, loss = 0.71593207\n",
      "Iteration 29, loss = 0.71527256\n",
      "Iteration 30, loss = 0.71457521\n",
      "Iteration 31, loss = 0.71384576\n",
      "Iteration 32, loss = 0.71308961\n",
      "Iteration 33, loss = 0.71231171\n",
      "Iteration 34, loss = 0.71151667\n",
      "Iteration 35, loss = 0.71070872\n",
      "Iteration 36, loss = 0.70989177\n",
      "Iteration 37, loss = 0.70906940\n",
      "Iteration 38, loss = 0.70824490\n",
      "Iteration 39, loss = 0.70742131\n",
      "Iteration 40, loss = 0.70660143\n",
      "Iteration 41, loss = 0.70578790\n",
      "Iteration 42, loss = 0.70498320\n",
      "Iteration 43, loss = 0.70418967\n",
      "Iteration 44, loss = 0.70340959\n",
      "Iteration 45, loss = 0.70264517\n",
      "Iteration 46, loss = 0.70189857\n",
      "Iteration 47, loss = 0.70117192\n",
      "Iteration 48, loss = 0.70046733\n",
      "Iteration 49, loss = 0.69978687\n",
      "Iteration 50, loss = 0.69913259\n",
      "Iteration 51, loss = 0.69850648\n",
      "Iteration 52, loss = 0.69791048\n",
      "Iteration 53, loss = 0.69734641\n",
      "Iteration 54, loss = 0.69681598\n",
      "Iteration 55, loss = 0.69632075\n",
      "Iteration 56, loss = 0.69586206\n",
      "Iteration 57, loss = 0.69544103\n",
      "Iteration 58, loss = 0.69505846\n",
      "Iteration 59, loss = 0.69471486\n",
      "Iteration 60, loss = 0.69441035\n",
      "Iteration 61, loss = 0.69414464\n",
      "Iteration 62, loss = 0.69391699\n",
      "Iteration 63, loss = 0.69372622\n",
      "Iteration 64, loss = 0.69357066\n",
      "Iteration 65, loss = 0.69344815\n",
      "Iteration 66, loss = 0.69335611\n",
      "Iteration 67, loss = 0.69329155\n",
      "Iteration 68, loss = 0.69325113\n",
      "Iteration 69, loss = 0.69323126\n",
      "Iteration 70, loss = 0.69322821\n",
      "Iteration 71, loss = 0.69323820\n",
      "Iteration 72, loss = 0.69325757\n",
      "Iteration 73, loss = 0.69328285\n",
      "Iteration 74, loss = 0.69331091\n",
      "Iteration 75, loss = 0.69333906\n",
      "Iteration 76, loss = 0.69336507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79565985\n",
      "Iteration 2, loss = 7.81732412\n",
      "Iteration 3, loss = 3.69139769\n",
      "Iteration 4, loss = 0.75346999\n",
      "Iteration 5, loss = 0.73432390\n",
      "Iteration 6, loss = 0.73475220\n",
      "Iteration 7, loss = 0.73475265\n",
      "Iteration 8, loss = 0.73444014\n",
      "Iteration 9, loss = 0.73388912\n",
      "Iteration 10, loss = 0.73315240\n",
      "Iteration 11, loss = 0.73226921\n",
      "Iteration 12, loss = 0.73126961\n",
      "Iteration 13, loss = 0.73017712\n",
      "Iteration 14, loss = 0.72901040\n",
      "Iteration 15, loss = 0.72778440\n",
      "Iteration 16, loss = 0.72651120\n",
      "Iteration 17, loss = 0.72520060\n",
      "Iteration 18, loss = 0.72386061\n",
      "Iteration 19, loss = 0.72249781\n",
      "Iteration 20, loss = 0.72111836\n",
      "Iteration 21, loss = 0.71972536\n",
      "Iteration 22, loss = 0.71832494\n",
      "Iteration 23, loss = 0.71692058\n",
      "Iteration 24, loss = 0.71551406\n",
      "Iteration 25, loss = 1.12201804\n",
      "Iteration 26, loss = 0.72778902\n",
      "Iteration 27, loss = 0.71132369\n",
      "Iteration 28, loss = 0.70999411\n",
      "Iteration 29, loss = 0.70868931\n",
      "Iteration 30, loss = 0.70741379\n",
      "Iteration 31, loss = 0.70617204\n",
      "Iteration 32, loss = 0.70496857\n",
      "Iteration 33, loss = 0.70380781\n",
      "Iteration 34, loss = 0.70269406\n",
      "Iteration 35, loss = 0.70163147\n",
      "Iteration 36, loss = 0.70062392\n",
      "Iteration 37, loss = 0.69967501\n",
      "Iteration 38, loss = 0.69878795\n",
      "Iteration 39, loss = 0.69796555\n",
      "Iteration 40, loss = 0.69721009\n",
      "Iteration 41, loss = 0.69652330\n",
      "Iteration 42, loss = 0.69590626\n",
      "Iteration 43, loss = 0.69535934\n",
      "Iteration 44, loss = 0.69488215\n",
      "Iteration 45, loss = 0.69447346\n",
      "Iteration 46, loss = 0.69413120\n",
      "Iteration 47, loss = 0.69385240\n",
      "Iteration 48, loss = 0.69363324\n",
      "Iteration 49, loss = 0.69346907\n",
      "Iteration 50, loss = 0.69335451\n",
      "Iteration 51, loss = 0.69328354\n",
      "Iteration 52, loss = 0.69324968\n",
      "Iteration 53, loss = 0.69324619\n",
      "Iteration 54, loss = 0.69326624\n",
      "Iteration 55, loss = 0.69330316\n",
      "Iteration 56, loss = 0.69335069\n",
      "Iteration 57, loss = 0.69340314\n",
      "Iteration 58, loss = 0.69345556\n",
      "Iteration 59, loss = 0.69350390\n",
      "Iteration 60, loss = 0.69354507\n",
      "Iteration 61, loss = 0.69357692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69446993\n",
      "Iteration 2, loss = 0.75482278\n",
      "Iteration 3, loss = 1.40394658\n",
      "Iteration 4, loss = 0.69842104\n",
      "Iteration 5, loss = 0.71513936\n",
      "Iteration 6, loss = 0.73246854\n",
      "Iteration 7, loss = 0.74075725\n",
      "Iteration 8, loss = 0.73580813\n",
      "Iteration 9, loss = 0.72067804\n",
      "Iteration 10, loss = 0.70386733\n",
      "Iteration 11, loss = 0.69410414\n",
      "Iteration 12, loss = 0.69443836\n",
      "Iteration 13, loss = 0.70040001\n",
      "Iteration 14, loss = 0.70570320\n",
      "Iteration 15, loss = 0.70624160\n",
      "Iteration 16, loss = 0.70222211\n",
      "Iteration 17, loss = 0.69695633\n",
      "Iteration 18, loss = 0.69369210\n",
      "Iteration 19, loss = 0.69325617\n",
      "Iteration 20, loss = 0.69424564\n",
      "Iteration 21, loss = 0.69491914\n",
      "Iteration 22, loss = 0.69464332\n",
      "Iteration 23, loss = 0.69387556\n",
      "Iteration 24, loss = 0.69329537\n",
      "Iteration 25, loss = 0.69315029\n",
      "Iteration 26, loss = 0.69325429\n",
      "Iteration 27, loss = 0.69334090\n",
      "Iteration 28, loss = 0.69331149\n",
      "Iteration 29, loss = 0.69322455\n",
      "Iteration 30, loss = 0.69316290\n",
      "Iteration 31, loss = 0.69315035\n",
      "Iteration 32, loss = 0.69316162\n",
      "Iteration 33, loss = 0.69316846\n",
      "Iteration 34, loss = 0.69316384\n",
      "Iteration 35, loss = 0.69315555\n",
      "Iteration 36, loss = 0.69315095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69468780\n",
      "Iteration 2, loss = 0.70606397\n",
      "Iteration 3, loss = 0.72124999\n",
      "Iteration 4, loss = 0.69815488\n",
      "Iteration 5, loss = 0.69411320\n",
      "Iteration 6, loss = 0.69365599\n",
      "Iteration 7, loss = 0.69353153\n",
      "Iteration 8, loss = 0.69267724\n",
      "Iteration 9, loss = 0.69240448\n",
      "Iteration 10, loss = 0.69202207\n",
      "Iteration 11, loss = 0.69160732\n",
      "Iteration 12, loss = 0.69109525\n",
      "Iteration 13, loss = 0.68934324\n",
      "Iteration 14, loss = 0.69365327\n",
      "Iteration 15, loss = 0.69340890\n",
      "Iteration 16, loss = 0.69324428\n",
      "Iteration 17, loss = 0.69315806\n",
      "Iteration 18, loss = 0.69313708\n",
      "Iteration 19, loss = 0.69316134\n",
      "Iteration 20, loss = 0.69320912\n",
      "Iteration 21, loss = 0.69326098\n",
      "Iteration 22, loss = 0.69330256\n",
      "Iteration 23, loss = 0.69332567\n",
      "Iteration 24, loss = 0.69332806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69475916\n",
      "Iteration 2, loss = 0.75837298\n",
      "Iteration 3, loss = 1.52864104\n",
      "Iteration 4, loss = 0.70302531\n",
      "Iteration 5, loss = 0.72562935\n",
      "Iteration 6, loss = 0.74566765\n",
      "Iteration 7, loss = 0.75102008\n",
      "Iteration 8, loss = 0.73856194\n",
      "Iteration 9, loss = 0.71610754\n",
      "Iteration 10, loss = 0.69775582\n",
      "Iteration 11, loss = 0.69348853\n",
      "Iteration 12, loss = 0.70037473\n",
      "Iteration 13, loss = 0.70859517\n",
      "Iteration 14, loss = 0.71025367\n",
      "Iteration 15, loss = 0.70474504\n",
      "Iteration 16, loss = 0.69740745\n",
      "Iteration 17, loss = 0.69347787\n",
      "Iteration 18, loss = 0.69362165\n",
      "Iteration 19, loss = 0.69501732\n",
      "Iteration 20, loss = 0.69529419\n",
      "Iteration 21, loss = 0.69439767\n",
      "Iteration 22, loss = 0.69347135\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69446993\n",
      "Iteration 2, loss = 17.90417806\n",
      "Iteration 3, loss = 18.13978580\n",
      "Iteration 4, loss = 18.13980713\n",
      "Iteration 5, loss = 14.96931106\n",
      "Iteration 6, loss = 17.90431352\n",
      "Iteration 7, loss = 8.03118650\n",
      "Iteration 8, loss = 3.17645132\n",
      "Iteration 9, loss = 0.69614362\n",
      "Iteration 10, loss = 5.73511837\n",
      "Iteration 11, loss = 1.49504228\n",
      "Iteration 12, loss = 0.69440667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69468780\n",
      "Iteration 2, loss = 18.13975668\n",
      "Iteration 3, loss = 8.97374645\n",
      "Iteration 4, loss = 5.94423752\n",
      "Iteration 5, loss = 0.70832361\n",
      "Iteration 6, loss = 7.03541611\n",
      "Iteration 7, loss = 16.08669361\n",
      "Iteration 8, loss = 1.02892121\n",
      "Iteration 9, loss = 1.48538987\n",
      "Iteration 10, loss = 2.76533525\n",
      "Iteration 11, loss = 0.74036202\n",
      "Iteration 12, loss = 0.73559958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69475916\n",
      "Iteration 2, loss = 18.02196693\n",
      "Iteration 3, loss = 18.02199452\n",
      "Iteration 4, loss = 18.02201542\n",
      "Iteration 5, loss = 5.45950089\n",
      "Iteration 6, loss = 18.02209547\n",
      "Iteration 7, loss = 5.57970679\n",
      "Iteration 8, loss = 17.47458281\n",
      "Iteration 9, loss = 6.73278834\n",
      "Iteration 10, loss = 18.02265430\n",
      "Iteration 11, loss = 11.75910859\n",
      "Iteration 12, loss = 18.02290503\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69941293\n",
      "Iteration 2, loss = 6.02796030\n",
      "Iteration 3, loss = 0.91598483\n",
      "Iteration 4, loss = 0.91916697\n",
      "Iteration 5, loss = 0.86901488\n",
      "Iteration 6, loss = 0.82076126\n",
      "Iteration 7, loss = 0.76503369\n",
      "Iteration 8, loss = 0.71889447\n",
      "Iteration 9, loss = 0.69672542\n",
      "Iteration 10, loss = 0.69318516\n",
      "Iteration 11, loss = 0.69552249\n",
      "Iteration 12, loss = 0.70074846\n",
      "Iteration 13, loss = 0.70612181\n",
      "Iteration 14, loss = 0.70974565\n",
      "Iteration 15, loss = 0.71079618\n",
      "Iteration 16, loss = 0.70928064\n",
      "Iteration 17, loss = 0.70609842\n",
      "Iteration 18, loss = 0.70227059\n",
      "Iteration 19, loss = 0.69867962\n",
      "Iteration 20, loss = 0.69588318\n",
      "Iteration 21, loss = 0.69411452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70286440\n",
      "Iteration 2, loss = 6.76029436\n",
      "Iteration 3, loss = 0.94293329\n",
      "Iteration 4, loss = 0.93968105\n",
      "Iteration 5, loss = 0.88289886\n",
      "Iteration 6, loss = 0.82571259\n",
      "Iteration 7, loss = 0.76114556\n",
      "Iteration 8, loss = 0.71174743\n",
      "Iteration 9, loss = 0.69695360\n",
      "Iteration 10, loss = 0.69319044\n",
      "Iteration 11, loss = 0.69550198\n",
      "Iteration 12, loss = 0.70082267\n",
      "Iteration 13, loss = 0.70631726\n",
      "Iteration 14, loss = 0.71001393\n",
      "Iteration 15, loss = 0.71095719\n",
      "Iteration 16, loss = 0.70933279\n",
      "Iteration 17, loss = 0.70603527\n",
      "Iteration 18, loss = 0.70213671\n",
      "Iteration 19, loss = 0.69850526\n",
      "Iteration 20, loss = 0.69571514\n",
      "Iteration 21, loss = 0.69399831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70123886\n",
      "Iteration 2, loss = 6.33904787\n",
      "Iteration 3, loss = 0.92464675\n",
      "Iteration 4, loss = 0.92403310\n",
      "Iteration 5, loss = 0.87072395\n",
      "Iteration 6, loss = 0.81949739\n",
      "Iteration 7, loss = 0.76141524\n",
      "Iteration 8, loss = 0.71510486\n",
      "Iteration 9, loss = 0.69629940\n",
      "Iteration 10, loss = 0.69320957\n",
      "Iteration 11, loss = 0.69578189\n",
      "Iteration 12, loss = 0.70103436\n",
      "Iteration 13, loss = 0.70629331\n",
      "Iteration 14, loss = 0.70975163\n",
      "Iteration 15, loss = 0.71066167\n",
      "Iteration 16, loss = 0.70906052\n",
      "Iteration 17, loss = 0.70588713\n",
      "Iteration 18, loss = 0.70211269\n",
      "Iteration 19, loss = 0.69859389\n",
      "Iteration 20, loss = 0.69586087\n",
      "Iteration 21, loss = 0.69413067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69941293\n",
      "Iteration 2, loss = 18.13983795\n",
      "Iteration 3, loss = 18.13985835\n",
      "Iteration 4, loss = 0.75019981\n",
      "Iteration 5, loss = 0.75879521\n",
      "Iteration 6, loss = 18.14074071\n",
      "Iteration 7, loss = 2.02994425\n",
      "Iteration 8, loss = 0.74069882\n",
      "Iteration 9, loss = 10.08029323\n",
      "Iteration 10, loss = 2.42494051\n",
      "Iteration 11, loss = 8.32999407\n",
      "Iteration 12, loss = 6.94482700\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70286440\n",
      "Iteration 2, loss = 17.90425852\n",
      "Iteration 3, loss = 17.90427949\n",
      "Iteration 4, loss = 0.75423859\n",
      "Iteration 5, loss = 0.76271654\n",
      "Iteration 6, loss = 17.90515324\n",
      "Iteration 7, loss = 2.01777172\n",
      "Iteration 8, loss = 0.73992906\n",
      "Iteration 9, loss = 10.19410037\n",
      "Iteration 10, loss = 2.40671305\n",
      "Iteration 11, loss = 8.19125567\n",
      "Iteration 12, loss = 6.90629261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70123886\n",
      "Iteration 2, loss = 18.02204680\n",
      "Iteration 3, loss = 18.02206742\n",
      "Iteration 4, loss = 0.75046190\n",
      "Iteration 5, loss = 0.74457917\n",
      "Iteration 6, loss = 18.02293844\n",
      "Iteration 7, loss = 2.02754446\n",
      "Iteration 8, loss = 3.29811131\n",
      "Iteration 9, loss = 2.15434970\n",
      "Iteration 10, loss = 2.00400191\n",
      "Iteration 11, loss = 0.77758428\n",
      "Iteration 12, loss = 2.49843211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79410679\n",
      "Iteration 2, loss = 2.74557386\n",
      "Iteration 3, loss = 0.80458712\n",
      "Iteration 4, loss = 0.80520029\n",
      "Iteration 5, loss = 0.75655373\n",
      "Iteration 6, loss = 0.72633389\n",
      "Iteration 7, loss = 0.71866311\n",
      "Iteration 8, loss = 0.71146389\n",
      "Iteration 9, loss = 0.70521707\n",
      "Iteration 10, loss = 0.70022850\n",
      "Iteration 11, loss = 0.69662752\n",
      "Iteration 12, loss = 0.69438276\n",
      "Iteration 13, loss = 0.69333243\n",
      "Iteration 14, loss = 0.69322479\n",
      "Iteration 15, loss = 0.69376318\n",
      "Iteration 16, loss = 0.69464943\n",
      "Iteration 17, loss = 0.69561999\n",
      "Iteration 18, loss = 0.69647085\n",
      "Iteration 19, loss = 0.69706950\n",
      "Iteration 20, loss = 0.69735471\n",
      "Iteration 21, loss = 0.69732660\n",
      "Iteration 22, loss = 0.69703082\n",
      "Iteration 23, loss = 0.69654032\n",
      "Iteration 24, loss = 0.69593827\n",
      "Iteration 25, loss = 0.69530405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79833060\n",
      "Iteration 2, loss = 2.83382746\n",
      "Iteration 3, loss = 0.81242926\n",
      "Iteration 4, loss = 0.81099585\n",
      "Iteration 5, loss = 0.75634816\n",
      "Iteration 6, loss = 0.72832926\n",
      "Iteration 7, loss = 0.72007663\n",
      "Iteration 8, loss = 0.71236934\n",
      "Iteration 9, loss = 0.70571677\n",
      "Iteration 10, loss = 0.70043836\n",
      "Iteration 11, loss = 0.69666268\n",
      "Iteration 12, loss = 0.69434543\n",
      "Iteration 13, loss = 0.69330301\n",
      "Iteration 14, loss = 0.69325707\n",
      "Iteration 15, loss = 0.69388379\n",
      "Iteration 16, loss = 0.69486093\n",
      "Iteration 17, loss = 0.69590671\n",
      "Iteration 18, loss = 0.69680591\n",
      "Iteration 19, loss = 0.69742188\n",
      "Iteration 20, loss = 0.69769510\n",
      "Iteration 21, loss = 0.69763161\n",
      "Iteration 22, loss = 0.69728511\n",
      "Iteration 23, loss = 0.69673705\n",
      "Iteration 24, loss = 0.69607810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79565985\n",
      "Iteration 2, loss = 2.78753410\n",
      "Iteration 3, loss = 0.80848009\n",
      "Iteration 4, loss = 0.80812183\n",
      "Iteration 5, loss = 0.75657914\n",
      "Iteration 6, loss = 0.72738797\n",
      "Iteration 7, loss = 0.71942401\n",
      "Iteration 8, loss = 0.71196631\n",
      "Iteration 9, loss = 0.70551080\n",
      "Iteration 10, loss = 0.70037096\n",
      "Iteration 11, loss = 0.69667651\n",
      "Iteration 12, loss = 0.69439020\n",
      "Iteration 13, loss = 0.69333972\n",
      "Iteration 14, loss = 0.69326021\n",
      "Iteration 15, loss = 0.69384140\n",
      "Iteration 16, loss = 0.69477289\n",
      "Iteration 17, loss = 0.69578171\n",
      "Iteration 18, loss = 0.69665790\n",
      "Iteration 19, loss = 0.69726653\n",
      "Iteration 20, loss = 0.69754698\n",
      "Iteration 21, loss = 0.69750213\n",
      "Iteration 22, loss = 0.69718157\n",
      "Iteration 23, loss = 0.69666249\n",
      "Iteration 24, loss = 0.69603186\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79410679\n",
      "Iteration 2, loss = 18.13983634\n",
      "Iteration 3, loss = 18.14007792\n",
      "Iteration 4, loss = 0.72568866\n",
      "Iteration 5, loss = 0.72543552\n",
      "Iteration 6, loss = 4.28624038\n",
      "Iteration 7, loss = 18.14152760\n",
      "Iteration 8, loss = 13.55645812\n",
      "Iteration 9, loss = 3.13566219\n",
      "Iteration 10, loss = 4.02403920\n",
      "Iteration 11, loss = 1.60730762\n",
      "Iteration 12, loss = 3.46480871\n",
      "Iteration 13, loss = 3.21025326\n",
      "Iteration 14, loss = 0.80965410\n",
      "Iteration 15, loss = 1.16386997\n",
      "Iteration 16, loss = 5.78170040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79833060\n",
      "Iteration 2, loss = 17.90425691\n",
      "Iteration 3, loss = 17.90449906\n",
      "Iteration 4, loss = 0.98626655\n",
      "Iteration 5, loss = 0.72977056\n",
      "Iteration 6, loss = 4.19236505\n",
      "Iteration 7, loss = 17.90595485\n",
      "Iteration 8, loss = 14.48464109\n",
      "Iteration 9, loss = 4.27582803\n",
      "Iteration 10, loss = 1.08293992\n",
      "Iteration 11, loss = 3.49844394\n",
      "Iteration 12, loss = 3.52187582\n",
      "Iteration 13, loss = 0.99629049\n",
      "Iteration 14, loss = 1.20585138\n",
      "Iteration 15, loss = 5.46509980\n",
      "Iteration 16, loss = 1.99015533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79565985\n",
      "Iteration 2, loss = 18.02204520\n",
      "Iteration 3, loss = 18.02228547\n",
      "Iteration 4, loss = 0.81321655\n",
      "Iteration 5, loss = 0.72774270\n",
      "Iteration 6, loss = 4.27311923\n",
      "Iteration 7, loss = 18.02373028\n",
      "Iteration 8, loss = 14.60645332\n",
      "Iteration 9, loss = 4.48717133\n",
      "Iteration 10, loss = 0.86882003\n",
      "Iteration 11, loss = 2.53183863\n",
      "Iteration 12, loss = 6.54047190\n",
      "Iteration 13, loss = 2.68362881\n",
      "Iteration 14, loss = 9.32671872\n",
      "Iteration 15, loss = 11.47969384\n",
      "Iteration 16, loss = 6.65166126\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70461589\n",
      "Iteration 2, loss = 0.69443652\n",
      "Iteration 3, loss = 0.69225347\n",
      "Iteration 4, loss = 0.69254107\n",
      "Iteration 5, loss = 0.69275799\n",
      "Iteration 6, loss = 0.69278877\n",
      "Iteration 7, loss = 0.69270812\n",
      "Iteration 8, loss = 0.69263433\n",
      "Iteration 9, loss = 0.69257357\n",
      "Iteration 10, loss = 0.69253201\n",
      "Iteration 11, loss = 0.69250590\n",
      "Iteration 12, loss = 0.69247892\n",
      "Iteration 13, loss = 0.69242888\n",
      "Iteration 14, loss = 0.69235548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70296955\n",
      "Iteration 2, loss = 0.69390110\n",
      "Iteration 3, loss = 0.69122049\n",
      "Iteration 4, loss = 0.69117751\n",
      "Iteration 5, loss = 0.69134568\n",
      "Iteration 6, loss = 0.69123278\n",
      "Iteration 7, loss = 0.69119018\n",
      "Iteration 8, loss = 0.69113111\n",
      "Iteration 9, loss = 0.69109620\n",
      "Iteration 10, loss = 0.69106926\n",
      "Iteration 11, loss = 0.69104665\n",
      "Iteration 12, loss = 0.69102921\n",
      "Iteration 13, loss = 0.69099507\n",
      "Iteration 14, loss = 0.69095927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70523959\n",
      "Iteration 2, loss = 0.69574358\n",
      "Iteration 3, loss = 0.69245547\n",
      "Iteration 4, loss = 0.69226064\n",
      "Iteration 5, loss = 0.69234877\n",
      "Iteration 6, loss = 0.69219045\n",
      "Iteration 7, loss = 0.69143701\n",
      "Iteration 8, loss = 0.69136636\n",
      "Iteration 9, loss = 0.69123304\n",
      "Iteration 10, loss = 0.69096244\n",
      "Iteration 11, loss = 0.69089415\n",
      "Iteration 12, loss = 0.69082578\n",
      "Iteration 13, loss = 0.69075725\n",
      "Iteration 14, loss = 0.69069598\n",
      "Iteration 15, loss = 0.69063347\n",
      "Iteration 16, loss = 0.69058208\n",
      "Iteration 17, loss = 0.69052648\n",
      "Iteration 18, loss = 0.69047894\n",
      "Iteration 19, loss = 0.69043330\n",
      "Iteration 20, loss = 0.69039152\n",
      "Iteration 21, loss = 0.69034752\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70461589\n",
      "Iteration 2, loss = 0.72011227\n",
      "Iteration 3, loss = 0.73194095\n",
      "Iteration 4, loss = 0.72514244\n",
      "Iteration 5, loss = 0.69323371\n",
      "Iteration 6, loss = 0.73523082\n",
      "Iteration 7, loss = 0.71134056\n",
      "Iteration 8, loss = 0.69452272\n",
      "Iteration 9, loss = 0.71448610\n",
      "Iteration 10, loss = 0.71913772\n",
      "Iteration 11, loss = 0.70920894\n",
      "Iteration 12, loss = 0.69250764\n",
      "Iteration 13, loss = 0.70143722\n",
      "Iteration 14, loss = 0.70769624\n",
      "Iteration 15, loss = 0.69527740\n",
      "Iteration 16, loss = 0.69307377\n",
      "Iteration 17, loss = 0.70187640\n",
      "Iteration 18, loss = 0.70214719\n",
      "Iteration 19, loss = 0.69568217\n",
      "Iteration 20, loss = 0.69072763\n",
      "Iteration 21, loss = 0.69691314\n",
      "Iteration 22, loss = 0.69700466\n",
      "Iteration 23, loss = 0.69051846\n",
      "Iteration 24, loss = 0.69150182\n",
      "Iteration 25, loss = 0.69561690\n",
      "Iteration 26, loss = 0.69509793\n",
      "Iteration 27, loss = 0.69084179\n",
      "Iteration 28, loss = 0.69064459\n",
      "Iteration 29, loss = 0.69683469\n",
      "Iteration 30, loss = 0.69265218\n",
      "Iteration 31, loss = 0.68780258\n",
      "Iteration 32, loss = 0.69280663\n",
      "Iteration 33, loss = 0.69500294\n",
      "Iteration 34, loss = 0.69435390\n",
      "Iteration 35, loss = 0.69157594\n",
      "Iteration 36, loss = 0.68656221\n",
      "Iteration 37, loss = 0.68860353\n",
      "Iteration 38, loss = 0.68985915\n",
      "Iteration 39, loss = 0.68639725\n",
      "Iteration 40, loss = 0.68888350\n",
      "Iteration 41, loss = 0.68994119\n",
      "Iteration 42, loss = 0.69042043\n",
      "Iteration 43, loss = 0.68963828\n",
      "Iteration 44, loss = 0.68836373\n",
      "Iteration 45, loss = 0.68776312\n",
      "Iteration 46, loss = 0.68541510\n",
      "Iteration 47, loss = 0.69548103\n",
      "Iteration 48, loss = 0.69208530\n",
      "Iteration 49, loss = 0.68696184\n",
      "Iteration 50, loss = 0.68791874\n",
      "Iteration 51, loss = 0.69015323\n",
      "Iteration 52, loss = 0.69077552\n",
      "Iteration 53, loss = 0.68897498\n",
      "Iteration 54, loss = 0.68677818\n",
      "Iteration 55, loss = 0.68581764\n",
      "Iteration 56, loss = 0.68737547\n",
      "Iteration 57, loss = 0.68658799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70296955\n",
      "Iteration 2, loss = 0.72348493\n",
      "Iteration 3, loss = 0.72540563\n",
      "Iteration 4, loss = 0.72219362\n",
      "Iteration 5, loss = 0.69245294\n",
      "Iteration 6, loss = 0.73264008\n",
      "Iteration 7, loss = 0.71254927\n",
      "Iteration 8, loss = 0.69290845\n",
      "Iteration 9, loss = 0.71253481\n",
      "Iteration 10, loss = 0.71875038\n",
      "Iteration 11, loss = 0.70998503\n",
      "Iteration 12, loss = 0.69319016\n",
      "Iteration 13, loss = 0.69827909\n",
      "Iteration 14, loss = 0.70793094\n",
      "Iteration 15, loss = 0.69664162\n",
      "Iteration 16, loss = 0.69154827\n",
      "Iteration 17, loss = 0.70052700\n",
      "Iteration 18, loss = 0.70211467\n",
      "Iteration 19, loss = 0.69829005\n",
      "Iteration 20, loss = 0.69048191\n",
      "Iteration 21, loss = 0.69313835\n",
      "Iteration 22, loss = 0.69692228\n",
      "Iteration 23, loss = 0.69190246\n",
      "Iteration 24, loss = 0.68965370\n",
      "Iteration 25, loss = 0.69401639\n",
      "Iteration 26, loss = 0.69499071\n",
      "Iteration 27, loss = 0.69363932\n",
      "Iteration 28, loss = 0.69116374\n",
      "Iteration 29, loss = 0.68874133\n",
      "Iteration 30, loss = 0.69136155\n",
      "Iteration 31, loss = 0.68988254\n",
      "Iteration 32, loss = 0.68807859\n",
      "Iteration 33, loss = 0.68742965\n",
      "Iteration 34, loss = 0.68749479\n",
      "Iteration 35, loss = 0.68711866\n",
      "Iteration 36, loss = 0.68716537\n",
      "Iteration 37, loss = 0.68782520\n",
      "Iteration 38, loss = 0.68789173\n",
      "Iteration 39, loss = 0.68597736\n",
      "Iteration 40, loss = 0.68872762\n",
      "Iteration 41, loss = 0.68641919\n",
      "Iteration 42, loss = 0.69028185\n",
      "Iteration 43, loss = 0.69371030\n",
      "Iteration 44, loss = 0.68597969\n",
      "Iteration 45, loss = 0.68693266\n",
      "Iteration 46, loss = 0.68458244\n",
      "Iteration 47, loss = 0.68390212\n",
      "Iteration 48, loss = 0.68479144\n",
      "Iteration 49, loss = 0.68393477\n",
      "Iteration 50, loss = 0.68290187\n",
      "Iteration 51, loss = 0.68343149\n",
      "Iteration 52, loss = 0.68306017\n",
      "Iteration 53, loss = 0.68201670\n",
      "Iteration 54, loss = 0.68219166\n",
      "Iteration 55, loss = 0.68204394\n",
      "Iteration 56, loss = 0.68112420\n",
      "Iteration 57, loss = 0.68105131\n",
      "Iteration 58, loss = 0.68095988\n",
      "Iteration 59, loss = 0.68017917\n",
      "Iteration 60, loss = 0.67996435\n",
      "Iteration 61, loss = 0.67984862\n",
      "Iteration 62, loss = 0.67917571\n",
      "Iteration 63, loss = 0.67889255\n",
      "Iteration 64, loss = 0.67872147\n",
      "Iteration 65, loss = 0.67812129\n",
      "Iteration 66, loss = 0.67781112\n",
      "Iteration 67, loss = 0.67757811\n",
      "Iteration 68, loss = 0.67702397\n",
      "Iteration 69, loss = 0.67670607\n",
      "Iteration 70, loss = 0.67641204\n",
      "Iteration 71, loss = 0.67588963\n",
      "Iteration 72, loss = 0.67556897\n",
      "Iteration 73, loss = 0.67521782\n",
      "Iteration 74, loss = 0.67472143\n",
      "Iteration 75, loss = 0.67439374\n",
      "Iteration 76, loss = 0.67399120\n",
      "Iteration 77, loss = 0.67352030\n",
      "Iteration 78, loss = 0.67317518\n",
      "Iteration 79, loss = 0.67273093\n",
      "Iteration 80, loss = 0.67228458\n",
      "Iteration 81, loss = 0.67190895\n",
      "Iteration 82, loss = 0.67143768\n",
      "Iteration 83, loss = 0.67100960\n",
      "Iteration 84, loss = 0.67059325\n",
      "Iteration 85, loss = 0.67011254\n",
      "Iteration 86, loss = 0.66968814\n",
      "Iteration 87, loss = 0.66923001\n",
      "Iteration 88, loss = 0.66875383\n",
      "Iteration 89, loss = 0.66831319\n",
      "Iteration 90, loss = 0.66782456\n",
      "Iteration 91, loss = 0.66735471\n",
      "Iteration 92, loss = 0.66688224\n",
      "Iteration 93, loss = 0.66638121\n",
      "Iteration 94, loss = 0.66590470\n",
      "Iteration 95, loss = 0.66539984\n",
      "Iteration 96, loss = 0.66489736\n",
      "Iteration 97, loss = 0.66439660\n",
      "Iteration 98, loss = 0.66387313\n",
      "Iteration 99, loss = 0.66336277\n",
      "Iteration 100, loss = 0.66283267\n",
      "Iteration 101, loss = 0.66230208\n",
      "Iteration 102, loss = 0.66176876\n",
      "Iteration 103, loss = 0.66122017\n",
      "Iteration 104, loss = 0.66067702\n",
      "Iteration 105, loss = 0.66011743\n",
      "Iteration 106, loss = 0.65955830\n",
      "Iteration 107, loss = 0.65899071\n",
      "Iteration 108, loss = 0.65841480\n",
      "Iteration 109, loss = 0.65783732\n",
      "Iteration 110, loss = 0.65724713\n",
      "Iteration 111, loss = 0.65665651\n",
      "Iteration 112, loss = 0.65605425\n",
      "Iteration 113, loss = 0.65544856\n",
      "Iteration 114, loss = 0.65483468\n",
      "Iteration 115, loss = 0.65421369\n",
      "Iteration 116, loss = 0.65358735\n",
      "Iteration 117, loss = 0.65295160\n",
      "Iteration 118, loss = 0.65231161\n",
      "Iteration 119, loss = 0.65166163\n",
      "Iteration 120, loss = 0.65100707\n",
      "Iteration 121, loss = 0.65034300\n",
      "Iteration 122, loss = 0.64967335\n",
      "Iteration 123, loss = 0.64899499\n",
      "Iteration 124, loss = 0.64830998\n",
      "Iteration 125, loss = 0.64761692\n",
      "Iteration 126, loss = 0.64691639\n",
      "Iteration 127, loss = 0.64620820\n",
      "Iteration 128, loss = 0.64549200\n",
      "Iteration 129, loss = 0.64476826\n",
      "Iteration 130, loss = 0.64403616\n",
      "Iteration 131, loss = 0.64329649\n",
      "Iteration 132, loss = 0.64254827\n",
      "Iteration 133, loss = 0.64179232\n",
      "Iteration 134, loss = 0.64102770\n",
      "Iteration 135, loss = 0.64025512\n",
      "Iteration 136, loss = 0.63947381\n",
      "Iteration 137, loss = 0.63868429\n",
      "Iteration 138, loss = 0.63788598\n",
      "Iteration 139, loss = 0.63707920\n",
      "Iteration 140, loss = 0.63626358\n",
      "Iteration 141, loss = 0.63543921\n",
      "Iteration 142, loss = 0.63460596\n",
      "Iteration 143, loss = 0.63376370\n",
      "Iteration 144, loss = 0.63291248\n",
      "Iteration 145, loss = 0.66644730\n",
      "Iteration 146, loss = 0.64265650\n",
      "Iteration 147, loss = 0.65929682\n",
      "Iteration 148, loss = 0.63548328\n",
      "Iteration 149, loss = 0.66153038\n",
      "Iteration 150, loss = 0.63276769\n",
      "Iteration 151, loss = 0.65015545\n",
      "Iteration 152, loss = 0.62761886\n",
      "Iteration 153, loss = 0.64776801\n",
      "Iteration 154, loss = 0.62716349\n",
      "Iteration 155, loss = 0.64433211\n",
      "Iteration 156, loss = 0.62522169\n",
      "Iteration 157, loss = 0.63831267\n",
      "Iteration 158, loss = 0.62708125\n",
      "Iteration 159, loss = 0.63061026\n",
      "Iteration 160, loss = 0.62933853\n",
      "Iteration 161, loss = 0.62424407\n",
      "Iteration 162, loss = 0.62968398\n",
      "Iteration 163, loss = 0.62077434\n",
      "Iteration 164, loss = 0.62758478\n",
      "Iteration 165, loss = 0.61981258\n",
      "Iteration 166, loss = 0.62393083\n",
      "Iteration 167, loss = 0.61990807\n",
      "Iteration 168, loss = 0.62009576\n",
      "Iteration 169, loss = 0.61977062\n",
      "Iteration 170, loss = 0.61700454\n",
      "Iteration 171, loss = 0.61884675\n",
      "Iteration 172, loss = 0.61486366\n",
      "Iteration 173, loss = 0.61717774\n",
      "Iteration 174, loss = 0.61342167\n",
      "Iteration 175, loss = 0.61508019\n",
      "Iteration 176, loss = 0.61228522\n",
      "Iteration 177, loss = 0.61289001\n",
      "Iteration 178, loss = 0.61115609\n",
      "Iteration 179, loss = 0.61080223\n",
      "Iteration 180, loss = 0.60989864\n",
      "Iteration 181, loss = 0.60888124\n",
      "Iteration 182, loss = 0.60848272\n",
      "Iteration 183, loss = 0.60711049\n",
      "Iteration 184, loss = 0.60694067\n",
      "Iteration 185, loss = 0.60543662\n",
      "Iteration 186, loss = 0.60531572\n",
      "Iteration 187, loss = 0.60381289\n",
      "Iteration 188, loss = 0.60364248\n",
      "Iteration 189, loss = 0.60220233\n",
      "Iteration 190, loss = 0.60194335\n",
      "Iteration 191, loss = 0.60058518\n",
      "Iteration 192, loss = 0.60022731\n",
      "Iteration 193, loss = 0.59895078\n",
      "Iteration 194, loss = 0.59849721\n",
      "Iteration 195, loss = 0.59729600\n",
      "Iteration 196, loss = 0.59675090\n",
      "Iteration 197, loss = 0.64808115\n",
      "Iteration 198, loss = 0.60156162\n",
      "Iteration 199, loss = 0.59413103\n",
      "Iteration 200, loss = 0.59859215\n",
      "Iteration 201, loss = 0.59268454\n",
      "Iteration 202, loss = 0.59575370\n",
      "Iteration 203, loss = 0.59129166\n",
      "Iteration 204, loss = 0.59298799\n",
      "Iteration 205, loss = 0.58991707\n",
      "Iteration 206, loss = 0.59031465\n",
      "Iteration 207, loss = 0.58853789\n",
      "Iteration 208, loss = 0.58774988\n",
      "Iteration 209, loss = 0.58708316\n",
      "Iteration 210, loss = 0.58534271\n",
      "Iteration 211, loss = 0.58547957\n",
      "Iteration 212, loss = 0.58315496\n",
      "Iteration 213, loss = 0.58365828\n",
      "Iteration 214, loss = 0.58121967\n",
      "Iteration 215, loss = 0.58158333\n",
      "Iteration 216, loss = 0.57951833\n",
      "Iteration 217, loss = 0.57931318\n",
      "Iteration 218, loss = 0.57794402\n",
      "Iteration 219, loss = 0.57698700\n",
      "Iteration 220, loss = 0.57631358\n",
      "Iteration 221, loss = 0.57479219\n",
      "Iteration 222, loss = 0.57445220\n",
      "Iteration 223, loss = 0.57284742\n",
      "Iteration 224, loss = 0.57231759\n",
      "Iteration 225, loss = 0.57109142\n",
      "Iteration 226, loss = 0.57006224\n",
      "Iteration 227, loss = 0.56929517\n",
      "Iteration 228, loss = 0.56793184\n",
      "Iteration 229, loss = 0.56725655\n",
      "Iteration 230, loss = 0.56601632\n",
      "Iteration 231, loss = 0.56502830\n",
      "Iteration 232, loss = 0.56412652\n",
      "Iteration 233, loss = 0.56286932\n",
      "Iteration 234, loss = 0.56202810\n",
      "Iteration 235, loss = 0.56088694\n",
      "Iteration 236, loss = 0.55978786\n",
      "Iteration 237, loss = 0.55886566\n",
      "Iteration 238, loss = 0.55765990\n",
      "Iteration 239, loss = 0.55664699\n",
      "Iteration 240, loss = 0.55561493\n",
      "Iteration 241, loss = 0.55442686\n",
      "Iteration 242, loss = 0.55341679\n",
      "Iteration 243, loss = 0.55231946\n",
      "Iteration 244, loss = 0.55115046\n",
      "Iteration 245, loss = 0.55010992\n",
      "Iteration 246, loss = 0.54898387\n",
      "Iteration 247, loss = 0.54781519\n",
      "Iteration 248, loss = 0.54673797\n",
      "Iteration 249, loss = 0.54560115\n",
      "Iteration 250, loss = 0.54442030\n",
      "Iteration 251, loss = 0.54330480\n",
      "Iteration 252, loss = 0.54216373\n",
      "Iteration 253, loss = 0.54097004\n",
      "Iteration 254, loss = 0.53981249\n",
      "Iteration 255, loss = 0.53866354\n",
      "Iteration 256, loss = 0.53746666\n",
      "Iteration 257, loss = 0.53626802\n",
      "Iteration 258, loss = 0.53509476\n",
      "Iteration 259, loss = 0.53390237\n",
      "Iteration 260, loss = 0.53268071\n",
      "Iteration 261, loss = 0.53146613\n",
      "Iteration 262, loss = 0.53026217\n",
      "Iteration 263, loss = 0.52904093\n",
      "Iteration 264, loss = 0.52779919\n",
      "Iteration 265, loss = 0.52655750\n",
      "Iteration 266, loss = 0.52532181\n",
      "Iteration 267, loss = 0.52407838\n",
      "Iteration 268, loss = 0.52281849\n",
      "Iteration 269, loss = 0.52154880\n",
      "Iteration 270, loss = 0.52027850\n",
      "Iteration 271, loss = 0.51900800\n",
      "Iteration 272, loss = 0.51773126\n",
      "Iteration 273, loss = 0.51644381\n",
      "Iteration 274, loss = 0.51514637\n",
      "Iteration 275, loss = 0.51384220\n",
      "Iteration 276, loss = 0.51253395\n",
      "Iteration 277, loss = 0.51122207\n",
      "Iteration 278, loss = 0.50990570\n",
      "Iteration 279, loss = 0.50858382\n",
      "Iteration 280, loss = 0.50725583\n",
      "Iteration 281, loss = 0.50592198\n",
      "Iteration 282, loss = 0.50458291\n",
      "Iteration 283, loss = 0.50324021\n",
      "Iteration 284, loss = 0.50189658\n",
      "Iteration 285, loss = 0.50055818\n",
      "Iteration 286, loss = 0.49923857\n",
      "Iteration 287, loss = 0.49797177\n",
      "Iteration 288, loss = 0.49684364\n",
      "Iteration 289, loss = 0.49608636\n",
      "Iteration 290, loss = 0.49632097\n",
      "Iteration 291, loss = 0.49955481\n",
      "Iteration 292, loss = 0.51448805\n",
      "Iteration 293, loss = 0.54781875\n",
      "Iteration 294, loss = 0.62567911\n",
      "Iteration 295, loss = 0.51595609\n",
      "Iteration 296, loss = 0.50520419\n",
      "Iteration 297, loss = 0.56416362\n",
      "Iteration 298, loss = 0.48830764\n",
      "Iteration 299, loss = 0.53131776\n",
      "Iteration 300, loss = 0.53007262\n",
      "Iteration 1, loss = 0.70523959\n",
      "Iteration 2, loss = 0.72285407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.73106915\n",
      "Iteration 4, loss = 0.72379551\n",
      "Iteration 5, loss = 0.69324333\n",
      "Iteration 6, loss = 0.73374795\n",
      "Iteration 7, loss = 0.71012274\n",
      "Iteration 8, loss = 0.69476427\n",
      "Iteration 9, loss = 0.71369997\n",
      "Iteration 10, loss = 0.71767675\n",
      "Iteration 11, loss = 0.70775021\n",
      "Iteration 12, loss = 0.69224488\n",
      "Iteration 13, loss = 0.70334368\n",
      "Iteration 14, loss = 0.70723740\n",
      "Iteration 15, loss = 0.69422053\n",
      "Iteration 16, loss = 0.69429728\n",
      "Iteration 17, loss = 0.70227985\n",
      "Iteration 18, loss = 0.70192658\n",
      "Iteration 19, loss = 0.69489431\n",
      "Iteration 20, loss = 0.69128577\n",
      "Iteration 21, loss = 0.69754293\n",
      "Iteration 22, loss = 0.69652550\n",
      "Iteration 23, loss = 0.69049948\n",
      "Iteration 24, loss = 0.69239094\n",
      "Iteration 25, loss = 0.69585443\n",
      "Iteration 26, loss = 0.69481391\n",
      "Iteration 27, loss = 0.69013282\n",
      "Iteration 28, loss = 0.68983416\n",
      "Iteration 29, loss = 0.69229836\n",
      "Iteration 30, loss = 0.69042758\n",
      "Iteration 31, loss = 0.68835606\n",
      "Iteration 32, loss = 0.69153802\n",
      "Iteration 33, loss = 0.69192791\n",
      "Iteration 34, loss = 0.69115576\n",
      "Iteration 35, loss = 0.68816983\n",
      "Iteration 36, loss = 0.69207300\n",
      "Iteration 37, loss = 0.69539087\n",
      "Iteration 38, loss = 0.68826800\n",
      "Iteration 39, loss = 0.69004449\n",
      "Iteration 40, loss = 0.69297062\n",
      "Iteration 41, loss = 0.69427209\n",
      "Iteration 42, loss = 0.69244526\n",
      "Iteration 43, loss = 0.68955123\n",
      "Iteration 44, loss = 0.68533658\n",
      "Iteration 45, loss = 0.68810562\n",
      "Iteration 46, loss = 0.68716039\n",
      "Iteration 47, loss = 0.68703670\n",
      "Iteration 48, loss = 0.68489954\n",
      "Iteration 49, loss = 0.68838971\n",
      "Iteration 50, loss = 0.68713834\n",
      "Iteration 51, loss = 0.68392522\n",
      "Iteration 52, loss = 0.68316880\n",
      "Iteration 53, loss = 0.68299489\n",
      "Iteration 54, loss = 0.68266222\n",
      "Iteration 55, loss = 0.68228053\n",
      "Iteration 56, loss = 0.68207026\n",
      "Iteration 57, loss = 0.68175569\n",
      "Iteration 58, loss = 0.68136374\n",
      "Iteration 59, loss = 0.68110351\n",
      "Iteration 60, loss = 0.68078655\n",
      "Iteration 61, loss = 0.68039233\n",
      "Iteration 62, loss = 0.68009502\n",
      "Iteration 63, loss = 0.67976445\n",
      "Iteration 64, loss = 0.67936881\n",
      "Iteration 65, loss = 0.67904462\n",
      "Iteration 66, loss = 0.67869552\n",
      "Iteration 67, loss = 0.67829659\n",
      "Iteration 68, loss = 0.67795134\n",
      "Iteration 69, loss = 0.67758200\n",
      "Iteration 70, loss = 0.67717825\n",
      "Iteration 71, loss = 0.67681487\n",
      "Iteration 72, loss = 0.67642526\n",
      "Iteration 73, loss = 0.67601548\n",
      "Iteration 74, loss = 0.67563445\n",
      "Iteration 75, loss = 0.67522546\n",
      "Iteration 76, loss = 0.67480890\n",
      "Iteration 77, loss = 0.67440941\n",
      "Iteration 78, loss = 0.67398270\n",
      "Iteration 79, loss = 0.67355817\n",
      "Iteration 80, loss = 0.67313878\n",
      "Iteration 81, loss = 0.67269671\n",
      "Iteration 82, loss = 0.67226214\n",
      "Iteration 83, loss = 0.67182190\n",
      "Iteration 84, loss = 0.67136688\n",
      "Iteration 85, loss = 0.67091899\n",
      "Iteration 86, loss = 0.67045835\n",
      "Iteration 87, loss = 0.66999182\n",
      "Iteration 88, loss = 0.66952686\n",
      "Iteration 89, loss = 0.66904799\n",
      "Iteration 90, loss = 0.66856917\n",
      "Iteration 91, loss = 0.66808446\n",
      "Iteration 92, loss = 0.66759017\n",
      "Iteration 93, loss = 0.66709600\n",
      "Iteration 94, loss = 0.66659146\n",
      "Iteration 95, loss = 0.66608297\n",
      "Iteration 96, loss = 0.66556991\n",
      "Iteration 97, loss = 0.66504769\n",
      "Iteration 98, loss = 0.66452319\n",
      "Iteration 99, loss = 0.66398998\n",
      "Iteration 100, loss = 0.66345163\n",
      "Iteration 101, loss = 0.66290790\n",
      "Iteration 102, loss = 0.66235601\n",
      "Iteration 103, loss = 0.66180010\n",
      "Iteration 104, loss = 0.66123591\n",
      "Iteration 105, loss = 0.66066638\n",
      "Iteration 106, loss = 0.66009018\n",
      "Iteration 107, loss = 0.65950676\n",
      "Iteration 108, loss = 0.65891783\n",
      "Iteration 109, loss = 0.65832087\n",
      "Iteration 110, loss = 0.65771827\n",
      "Iteration 111, loss = 0.65710796\n",
      "Iteration 112, loss = 0.65649112\n",
      "Iteration 113, loss = 0.65586723\n",
      "Iteration 114, loss = 0.65523594\n",
      "Iteration 115, loss = 0.65459799\n",
      "Iteration 116, loss = 0.65395217\n",
      "Iteration 117, loss = 0.65329963\n",
      "Iteration 118, loss = 0.65263913\n",
      "Iteration 119, loss = 0.65197159\n",
      "Iteration 120, loss = 0.65129616\n",
      "Iteration 121, loss = 0.65061328\n",
      "Iteration 122, loss = 0.64992258\n",
      "Iteration 123, loss = 0.64922408\n",
      "Iteration 124, loss = 0.64851776\n",
      "Iteration 125, loss = 0.64780335\n",
      "Iteration 126, loss = 0.64708103\n",
      "Iteration 127, loss = 0.64635042\n",
      "Iteration 128, loss = 0.64561176\n",
      "Iteration 129, loss = 0.64486463\n",
      "Iteration 130, loss = 0.64410928\n",
      "Iteration 131, loss = 0.64334531\n",
      "Iteration 132, loss = 0.64257517\n",
      "Iteration 133, loss = 0.64180490\n",
      "Iteration 134, loss = 0.64101312\n",
      "Iteration 135, loss = 0.64022733\n",
      "Iteration 136, loss = 0.63960643\n",
      "Iteration 137, loss = 0.63866712\n",
      "Iteration 138, loss = 0.63785670\n",
      "Iteration 139, loss = 0.63698497\n",
      "Iteration 140, loss = 0.63631433\n",
      "Iteration 141, loss = 0.63532128\n",
      "Iteration 142, loss = 0.63452631\n",
      "Iteration 143, loss = 0.63356076\n",
      "Iteration 144, loss = 0.63276611\n",
      "Iteration 145, loss = 0.63243639\n",
      "Iteration 146, loss = 0.63093135\n",
      "Iteration 147, loss = 0.66565405\n",
      "Iteration 148, loss = 0.66538628\n",
      "Iteration 149, loss = 0.64831007\n",
      "Iteration 150, loss = 0.64016352\n",
      "Iteration 151, loss = 0.65327701\n",
      "Iteration 152, loss = 0.63076534\n",
      "Iteration 153, loss = 0.64086901\n",
      "Iteration 154, loss = 0.62608156\n",
      "Iteration 155, loss = 0.64022372\n",
      "Iteration 156, loss = 0.63092332\n",
      "Iteration 157, loss = 0.64015130\n",
      "Iteration 158, loss = 0.62372590\n",
      "Iteration 159, loss = 0.63847003\n",
      "Iteration 160, loss = 0.62133784\n",
      "Iteration 161, loss = 0.63433417\n",
      "Iteration 162, loss = 0.62159497\n",
      "Iteration 163, loss = 0.62802153\n",
      "Iteration 164, loss = 0.62321189\n",
      "Iteration 165, loss = 0.62188697\n",
      "Iteration 166, loss = 0.62393898\n",
      "Iteration 167, loss = 0.61762569\n",
      "Iteration 168, loss = 0.62286177\n",
      "Iteration 169, loss = 0.61547300\n",
      "Iteration 170, loss = 0.62027835\n",
      "Iteration 171, loss = 0.61465211\n",
      "Iteration 172, loss = 0.61702483\n",
      "Iteration 173, loss = 0.61419061\n",
      "Iteration 174, loss = 0.61389589\n",
      "Iteration 175, loss = 0.61344712\n",
      "Iteration 176, loss = 0.61128846\n",
      "Iteration 177, loss = 0.61221596\n",
      "Iteration 178, loss = 0.60922787\n",
      "Iteration 179, loss = 0.61057263\n",
      "Iteration 180, loss = 0.60754840\n",
      "Iteration 181, loss = 0.60868864\n",
      "Iteration 182, loss = 0.60604986\n",
      "Iteration 183, loss = 0.60672462\n",
      "Iteration 184, loss = 0.60458656\n",
      "Iteration 185, loss = 0.60477386\n",
      "Iteration 186, loss = 0.60308221\n",
      "Iteration 187, loss = 0.60287633\n",
      "Iteration 188, loss = 0.60150916\n",
      "Iteration 189, loss = 0.60103359\n",
      "Iteration 190, loss = 0.59986892\n",
      "Iteration 191, loss = 0.59923177\n",
      "Iteration 192, loss = 0.59817140\n",
      "Iteration 193, loss = 0.59745325\n",
      "Iteration 194, loss = 0.59642901\n",
      "Iteration 195, loss = 0.59568206\n",
      "Iteration 196, loss = 0.59465089\n",
      "Iteration 197, loss = 0.59390593\n",
      "Iteration 198, loss = 0.61174893\n",
      "Iteration 199, loss = 0.60948186\n",
      "Iteration 200, loss = 0.59110303\n",
      "Iteration 201, loss = 0.60478423\n",
      "Iteration 202, loss = 0.58944194\n",
      "Iteration 203, loss = 0.60058980\n",
      "Iteration 204, loss = 0.58780872\n",
      "Iteration 205, loss = 0.59677213\n",
      "Iteration 206, loss = 0.58618924\n",
      "Iteration 207, loss = 0.59329821\n",
      "Iteration 208, loss = 0.58459050\n",
      "Iteration 209, loss = 0.59005943\n",
      "Iteration 210, loss = 0.58301398\n",
      "Iteration 211, loss = 0.58701636\n",
      "Iteration 212, loss = 0.58147227\n",
      "Iteration 213, loss = 0.58410173\n",
      "Iteration 214, loss = 0.57995804\n",
      "Iteration 215, loss = 0.58129530\n",
      "Iteration 216, loss = 0.57846326\n",
      "Iteration 217, loss = 0.57858308\n",
      "Iteration 218, loss = 0.57695016\n",
      "Iteration 219, loss = 0.57598434\n",
      "Iteration 220, loss = 0.57536831\n",
      "Iteration 221, loss = 0.57353935\n",
      "Iteration 222, loss = 0.57364818\n",
      "Iteration 223, loss = 0.57129501\n",
      "Iteration 224, loss = 0.57172575\n",
      "Iteration 225, loss = 0.56928528\n",
      "Iteration 226, loss = 0.56957595\n",
      "Iteration 227, loss = 0.56749071\n",
      "Iteration 228, loss = 0.56724203\n",
      "Iteration 229, loss = 0.56581499\n",
      "Iteration 230, loss = 0.56485267\n",
      "Iteration 231, loss = 0.56409449\n",
      "Iteration 232, loss = 0.56257820\n",
      "Iteration 233, loss = 0.56216535\n",
      "Iteration 234, loss = 0.56053299\n",
      "Iteration 235, loss = 0.55997607\n",
      "Iteration 236, loss = 0.55867470\n",
      "Iteration 237, loss = 0.55765519\n",
      "Iteration 238, loss = 0.55680104\n",
      "Iteration 239, loss = 0.55542993\n",
      "Iteration 240, loss = 0.55471199\n",
      "Iteration 241, loss = 0.55340776\n",
      "Iteration 242, loss = 0.55242502\n",
      "Iteration 243, loss = 0.55143981\n",
      "Iteration 244, loss = 0.55016994\n",
      "Iteration 245, loss = 0.54929481\n",
      "Iteration 246, loss = 0.54808359\n",
      "Iteration 247, loss = 0.54698815\n",
      "Iteration 248, loss = 0.54599985\n",
      "Iteration 249, loss = 0.54475513\n",
      "Iteration 250, loss = 0.54373328\n",
      "Iteration 251, loss = 0.54262778\n",
      "Iteration 252, loss = 0.54142067\n",
      "Iteration 253, loss = 0.54038549\n",
      "Iteration 254, loss = 0.53922003\n",
      "Iteration 255, loss = 0.53803845\n",
      "Iteration 256, loss = 0.53696448\n",
      "Iteration 257, loss = 0.53577627\n",
      "Iteration 258, loss = 0.53459420\n",
      "Iteration 259, loss = 0.53348364\n",
      "Iteration 260, loss = 0.53228762\n",
      "Iteration 261, loss = 0.53108855\n",
      "Iteration 262, loss = 0.52994586\n",
      "Iteration 263, loss = 0.52874778\n",
      "Iteration 264, loss = 0.52752649\n",
      "Iteration 265, loss = 0.52635003\n",
      "Iteration 266, loss = 0.52515130\n",
      "Iteration 267, loss = 0.52391340\n",
      "Iteration 268, loss = 0.52269791\n",
      "Iteration 269, loss = 0.52149102\n",
      "Iteration 270, loss = 0.52024964\n",
      "Iteration 271, loss = 0.51899843\n",
      "Iteration 272, loss = 0.51776450\n",
      "Iteration 273, loss = 0.51652354\n",
      "Iteration 274, loss = 0.51525788\n",
      "Iteration 275, loss = 0.51398717\n",
      "Iteration 276, loss = 0.51272507\n",
      "Iteration 277, loss = 0.51145772\n",
      "Iteration 278, loss = 0.51017289\n",
      "Iteration 279, loss = 0.50887838\n",
      "Iteration 280, loss = 0.50758534\n",
      "Iteration 281, loss = 0.50629220\n",
      "Iteration 282, loss = 0.50499031\n",
      "Iteration 283, loss = 0.50367636\n",
      "Iteration 284, loss = 0.50235460\n",
      "Iteration 285, loss = 0.50102999\n",
      "Iteration 286, loss = 0.49970335\n",
      "Iteration 287, loss = 0.49837214\n",
      "Iteration 288, loss = 0.49703366\n",
      "Iteration 289, loss = 0.49568712\n",
      "Iteration 290, loss = 0.49433323\n",
      "Iteration 291, loss = 0.49297327\n",
      "Iteration 292, loss = 0.49160806\n",
      "Iteration 293, loss = 0.49023797\n",
      "Iteration 294, loss = 0.48886294\n",
      "Iteration 295, loss = 0.48748285\n",
      "Iteration 296, loss = 0.48609767\n",
      "Iteration 297, loss = 0.48470753\n",
      "Iteration 298, loss = 0.48331298\n",
      "Iteration 299, loss = 0.48191520\n",
      "Iteration 300, loss = 0.48051732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69498415\n",
      "Iteration 2, loss = 0.69454238\n",
      "Iteration 3, loss = 0.69414384\n",
      "Iteration 4, loss = 0.69391641\n",
      "Iteration 5, loss = 0.69347028\n",
      "Iteration 6, loss = 0.69298865\n",
      "Iteration 7, loss = 0.69164679\n",
      "Iteration 8, loss = 0.69147198\n",
      "Iteration 9, loss = 0.69170055\n",
      "Iteration 10, loss = 0.69322751\n",
      "Iteration 11, loss = 0.69336406\n",
      "Iteration 12, loss = 0.69245234\n",
      "Iteration 13, loss = 0.69225711\n",
      "Iteration 14, loss = 0.68911224\n",
      "Iteration 15, loss = 0.68889943\n",
      "Iteration 16, loss = 0.68989727\n",
      "Iteration 17, loss = 0.68840347\n",
      "Iteration 18, loss = 0.68861056\n",
      "Iteration 19, loss = 0.69112181\n",
      "Iteration 20, loss = 0.69118661\n",
      "Iteration 21, loss = 0.68746355\n",
      "Iteration 22, loss = 0.68907165\n",
      "Iteration 23, loss = 0.68961124\n",
      "Iteration 24, loss = 0.68470041\n",
      "Iteration 25, loss = 0.68406252\n",
      "Iteration 26, loss = 0.68354314\n",
      "Iteration 27, loss = 0.68304303\n",
      "Iteration 28, loss = 0.68289929\n",
      "Iteration 29, loss = 0.68475204\n",
      "Iteration 30, loss = 0.68806318\n",
      "Iteration 31, loss = 0.68960498\n",
      "Iteration 32, loss = 0.68595228\n",
      "Iteration 33, loss = 0.68600384\n",
      "Iteration 34, loss = 0.69148366\n",
      "Iteration 35, loss = 0.69274761\n",
      "Iteration 36, loss = 0.68831218\n",
      "Iteration 37, loss = 0.68651476\n",
      "Iteration 38, loss = 0.68114421\n",
      "Iteration 39, loss = 0.67875867\n",
      "Iteration 40, loss = 0.67895647\n",
      "Iteration 41, loss = 0.67833987\n",
      "Iteration 42, loss = 0.67757309\n",
      "Iteration 43, loss = 0.67690351\n",
      "Iteration 44, loss = 0.67624182\n",
      "Iteration 45, loss = 0.67551087\n",
      "Iteration 46, loss = 0.67484126\n",
      "Iteration 47, loss = 0.67436877\n",
      "Iteration 48, loss = 0.67387260\n",
      "Iteration 49, loss = 0.67335653\n",
      "Iteration 50, loss = 0.67285936\n",
      "Iteration 51, loss = 0.67234673\n",
      "Iteration 52, loss = 0.67181121\n",
      "Iteration 53, loss = 0.67133962\n",
      "Iteration 54, loss = 0.67146536\n",
      "Iteration 55, loss = 0.67227324\n",
      "Iteration 56, loss = 0.67452860\n",
      "Iteration 57, loss = 0.67340915\n",
      "Iteration 58, loss = 0.68162017\n",
      "Iteration 59, loss = 0.67842637\n",
      "Iteration 60, loss = 0.67157741\n",
      "Iteration 61, loss = 0.67078978\n",
      "Iteration 62, loss = 0.66942169\n",
      "Iteration 63, loss = 0.66796090\n",
      "Iteration 64, loss = 0.66679216\n",
      "Iteration 65, loss = 0.66590267\n",
      "Iteration 66, loss = 0.66516163\n",
      "Iteration 67, loss = 0.66442206\n",
      "Iteration 68, loss = 0.66369658\n",
      "Iteration 69, loss = 0.66330288\n",
      "Iteration 70, loss = 0.66544221\n",
      "Iteration 71, loss = 0.66607426\n",
      "Iteration 72, loss = 0.67534782\n",
      "Iteration 73, loss = 0.67665597\n",
      "Iteration 74, loss = 0.69013394\n",
      "Iteration 75, loss = 0.68827824\n",
      "Iteration 76, loss = 0.70015134\n",
      "Iteration 77, loss = 0.69017141\n",
      "Iteration 78, loss = 0.67940510\n",
      "Iteration 79, loss = 0.67291861\n",
      "Iteration 80, loss = 0.66838098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353788\n",
      "Iteration 2, loss = 0.69304172\n",
      "Iteration 3, loss = 0.69260824\n",
      "Iteration 4, loss = 0.69226768\n",
      "Iteration 5, loss = 0.69138668\n",
      "Iteration 6, loss = 0.68953933\n",
      "Iteration 7, loss = 0.68929097\n",
      "Iteration 8, loss = 0.69090958\n",
      "Iteration 9, loss = 0.69045426\n",
      "Iteration 10, loss = 0.68918811\n",
      "Iteration 11, loss = 0.68913490\n",
      "Iteration 12, loss = 0.68895052\n",
      "Iteration 13, loss = 0.68859771\n",
      "Iteration 14, loss = 0.68979029\n",
      "Iteration 15, loss = 0.68976794\n",
      "Iteration 16, loss = 0.68953545\n",
      "Iteration 17, loss = 0.68947846\n",
      "Iteration 18, loss = 0.68938730\n",
      "Iteration 19, loss = 0.68940185\n",
      "Iteration 20, loss = 0.68888129\n",
      "Iteration 21, loss = 0.68970531\n",
      "Iteration 22, loss = 0.69631729\n",
      "Iteration 23, loss = 0.69866275\n",
      "Iteration 24, loss = 0.69001735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69160887\n",
      "Iteration 2, loss = 0.69142039\n",
      "Iteration 3, loss = 0.69119983\n",
      "Iteration 4, loss = 0.69105608\n",
      "Iteration 5, loss = 0.69095262\n",
      "Iteration 6, loss = 0.69071946\n",
      "Iteration 7, loss = 0.69052838\n",
      "Iteration 8, loss = 0.69032309\n",
      "Iteration 9, loss = 0.69010054\n",
      "Iteration 10, loss = 0.68986343\n",
      "Iteration 11, loss = 0.68960783\n",
      "Iteration 12, loss = 0.68932876\n",
      "Iteration 13, loss = 0.68904952\n",
      "Iteration 14, loss = 0.68876963\n",
      "Iteration 15, loss = 0.68848403\n",
      "Iteration 16, loss = 0.68819351\n",
      "Iteration 17, loss = 0.68788737\n",
      "Iteration 18, loss = 0.68758894\n",
      "Iteration 19, loss = 0.68728518\n",
      "Iteration 20, loss = 0.68697614\n",
      "Iteration 21, loss = 0.68666633\n",
      "Iteration 22, loss = 0.68634668\n",
      "Iteration 23, loss = 0.68602631\n",
      "Iteration 24, loss = 0.68570262\n",
      "Iteration 25, loss = 0.68537605\n",
      "Iteration 26, loss = 0.68504731\n",
      "Iteration 27, loss = 0.68471612\n",
      "Iteration 28, loss = 0.68438266\n",
      "Iteration 29, loss = 0.68405060\n",
      "Iteration 30, loss = 0.68371540\n",
      "Iteration 31, loss = 0.68338119\n",
      "Iteration 32, loss = 0.68304689\n",
      "Iteration 33, loss = 0.68270365\n",
      "Iteration 34, loss = 0.68236374\n",
      "Iteration 35, loss = 0.68202126\n",
      "Iteration 36, loss = 0.68167868\n",
      "Iteration 37, loss = 0.68133831\n",
      "Iteration 38, loss = 0.68099452\n",
      "Iteration 39, loss = 0.68065050\n",
      "Iteration 40, loss = 0.68030812\n",
      "Iteration 41, loss = 0.67996174\n",
      "Iteration 42, loss = 0.67961705\n",
      "Iteration 43, loss = 0.67927032\n",
      "Iteration 44, loss = 0.67892592\n",
      "Iteration 45, loss = 0.67857737\n",
      "Iteration 46, loss = 0.67823065\n",
      "Iteration 47, loss = 0.67788296\n",
      "Iteration 48, loss = 0.67753482\n",
      "Iteration 49, loss = 0.67718734\n",
      "Iteration 50, loss = 0.67683912\n",
      "Iteration 51, loss = 0.67649130\n",
      "Iteration 52, loss = 0.67614189\n",
      "Iteration 53, loss = 0.67579447\n",
      "Iteration 54, loss = 0.67544508\n",
      "Iteration 55, loss = 0.67509551\n",
      "Iteration 56, loss = 0.67474736\n",
      "Iteration 57, loss = 0.67439683\n",
      "Iteration 58, loss = 0.67404691\n",
      "Iteration 59, loss = 0.67369627\n",
      "Iteration 60, loss = 0.67334584\n",
      "Iteration 61, loss = 0.67299725\n",
      "Iteration 62, loss = 0.67264770\n",
      "Iteration 63, loss = 0.67229230\n",
      "Iteration 64, loss = 0.67193981\n",
      "Iteration 65, loss = 0.67158781\n",
      "Iteration 66, loss = 0.67123565\n",
      "Iteration 67, loss = 0.67088337\n",
      "Iteration 68, loss = 0.67053121\n",
      "Iteration 69, loss = 0.67017973\n",
      "Iteration 70, loss = 0.66982812\n",
      "Iteration 71, loss = 0.66947858\n",
      "Iteration 72, loss = 0.66912724\n",
      "Iteration 73, loss = 0.66877385\n",
      "Iteration 74, loss = 0.66842180\n",
      "Iteration 75, loss = 0.66807070\n",
      "Iteration 76, loss = 0.66771905\n",
      "Iteration 77, loss = 0.66736575\n",
      "Iteration 78, loss = 0.66701108\n",
      "Iteration 79, loss = 0.66666057\n",
      "Iteration 80, loss = 0.66630819\n",
      "Iteration 81, loss = 0.66595195\n",
      "Iteration 82, loss = 0.66559648\n",
      "Iteration 83, loss = 0.66524352\n",
      "Iteration 84, loss = 0.66489185\n",
      "Iteration 85, loss = 0.66453407\n",
      "Iteration 86, loss = 0.66417873\n",
      "Iteration 87, loss = 0.66382392\n",
      "Iteration 88, loss = 0.66346769\n",
      "Iteration 89, loss = 0.66311350\n",
      "Iteration 90, loss = 0.66276021\n",
      "Iteration 91, loss = 0.66240025\n",
      "Iteration 92, loss = 0.66204522\n",
      "Iteration 93, loss = 0.66169173\n",
      "Iteration 94, loss = 0.66132948\n",
      "Iteration 95, loss = 0.66097423\n",
      "Iteration 96, loss = 0.66061441\n",
      "Iteration 97, loss = 0.66025702\n",
      "Iteration 98, loss = 0.65990190\n",
      "Iteration 99, loss = 0.65954025\n",
      "Iteration 100, loss = 0.65917942\n",
      "Iteration 101, loss = 0.65882015\n",
      "Iteration 102, loss = 0.65846551\n",
      "Iteration 103, loss = 0.65810137\n",
      "Iteration 104, loss = 0.65773439\n",
      "Iteration 105, loss = 0.65736461\n",
      "Iteration 106, loss = 0.65699844\n",
      "Iteration 107, loss = 0.65663345\n",
      "Iteration 108, loss = 0.65627322\n",
      "Iteration 109, loss = 0.65589724\n",
      "Iteration 110, loss = 0.65552753\n",
      "Iteration 111, loss = 0.65515444\n",
      "Iteration 112, loss = 0.65475472\n",
      "Iteration 113, loss = 0.65420890\n",
      "Iteration 114, loss = 0.65319076\n",
      "Iteration 115, loss = 0.64997587\n",
      "Iteration 116, loss = 0.64890445\n",
      "Iteration 117, loss = 0.65206135\n",
      "Iteration 118, loss = 0.65619392\n",
      "Iteration 119, loss = 0.66749000\n",
      "Iteration 120, loss = 0.67874763\n",
      "Iteration 121, loss = 0.69510435\n",
      "Iteration 122, loss = 0.69402811\n",
      "Iteration 123, loss = 0.67297824\n",
      "Iteration 124, loss = 0.66609164\n",
      "Iteration 125, loss = 0.65271884\n",
      "Iteration 126, loss = 0.65123119\n",
      "Iteration 127, loss = 0.65098047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69498415\n",
      "Iteration 2, loss = 4.59513601\n",
      "Iteration 3, loss = 1.71834768\n",
      "Iteration 4, loss = 0.73357774\n",
      "Iteration 5, loss = 0.69271618\n",
      "Iteration 6, loss = 0.69885230\n",
      "Iteration 7, loss = 0.69891847\n",
      "Iteration 8, loss = 0.69896554\n",
      "Iteration 9, loss = 0.69899738\n",
      "Iteration 10, loss = 0.69902111\n",
      "Iteration 11, loss = 0.69903282\n",
      "Iteration 12, loss = 0.69902868\n",
      "Iteration 13, loss = 0.69901536\n",
      "Iteration 14, loss = 0.69899944\n",
      "Iteration 15, loss = 0.69897728\n",
      "Iteration 16, loss = 0.69894960\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353788\n",
      "Iteration 2, loss = 4.22837371\n",
      "Iteration 3, loss = 1.15318803\n",
      "Iteration 4, loss = 0.88677181\n",
      "Iteration 5, loss = 0.92124231\n",
      "Iteration 6, loss = 0.88876541\n",
      "Iteration 7, loss = 0.82127965\n",
      "Iteration 8, loss = 0.75148431\n",
      "Iteration 9, loss = 0.70367636\n",
      "Iteration 10, loss = 0.69066505\n",
      "Iteration 11, loss = 0.69650632\n",
      "Iteration 12, loss = 0.69852899\n",
      "Iteration 13, loss = 0.69860782\n",
      "Iteration 14, loss = 0.69866974\n",
      "Iteration 15, loss = 0.69871694\n",
      "Iteration 16, loss = 0.69875114\n",
      "Iteration 17, loss = 0.69877371\n",
      "Iteration 18, loss = 0.69878586\n",
      "Iteration 19, loss = 0.69878863\n",
      "Iteration 20, loss = 0.69878296\n",
      "Iteration 21, loss = 0.69876966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69160887\n",
      "Iteration 2, loss = 3.87544636\n",
      "Iteration 3, loss = 1.13637089\n",
      "Iteration 4, loss = 0.77571748\n",
      "Iteration 5, loss = 0.72519365\n",
      "Iteration 6, loss = 0.69610164\n",
      "Iteration 7, loss = 0.69518784\n",
      "Iteration 8, loss = 0.69818321\n",
      "Iteration 9, loss = 0.69821273\n",
      "Iteration 10, loss = 0.69822205\n",
      "Iteration 11, loss = 0.69821512\n",
      "Iteration 12, loss = 0.69819475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80310699\n",
      "Iteration 2, loss = 0.75015825\n",
      "Iteration 3, loss = 0.73431741\n",
      "Iteration 4, loss = 0.72302896\n",
      "Iteration 5, loss = 0.71317080\n",
      "Iteration 6, loss = 0.70795456\n",
      "Iteration 7, loss = 0.70398310\n",
      "Iteration 8, loss = 0.70073831\n",
      "Iteration 9, loss = 0.70084974\n",
      "Iteration 10, loss = 0.70020883\n",
      "Iteration 11, loss = 0.69925503\n",
      "Iteration 12, loss = 0.69803023\n",
      "Iteration 13, loss = 0.69675543\n",
      "Iteration 14, loss = 0.69571622\n",
      "Iteration 15, loss = 0.69483654\n",
      "Iteration 16, loss = 0.69409642\n",
      "Iteration 17, loss = 0.69333925\n",
      "Iteration 18, loss = 0.69237693\n",
      "Iteration 19, loss = 0.69234618\n",
      "Iteration 20, loss = 0.69233754\n",
      "Iteration 21, loss = 0.69232812\n",
      "Iteration 22, loss = 0.69235645\n",
      "Iteration 23, loss = 0.69240059\n",
      "Iteration 24, loss = 0.69244308\n",
      "Iteration 25, loss = 0.69248604\n",
      "Iteration 26, loss = 0.69252159\n",
      "Iteration 27, loss = 0.69254730\n",
      "Iteration 28, loss = 0.69255216\n",
      "Iteration 29, loss = 0.69252625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80875424\n",
      "Iteration 2, loss = 0.75267549\n",
      "Iteration 3, loss = 0.73648647\n",
      "Iteration 4, loss = 0.72453866\n",
      "Iteration 5, loss = 0.71416218\n",
      "Iteration 6, loss = 0.70920845\n",
      "Iteration 7, loss = 0.70485379\n",
      "Iteration 8, loss = 0.70317555\n",
      "Iteration 9, loss = 0.70229532\n",
      "Iteration 10, loss = 0.70128889\n",
      "Iteration 11, loss = 0.70013564\n",
      "Iteration 12, loss = 0.69861865\n",
      "Iteration 13, loss = 0.69725860\n",
      "Iteration 14, loss = 0.69608328\n",
      "Iteration 15, loss = 0.69506158\n",
      "Iteration 16, loss = 0.69422139\n",
      "Iteration 17, loss = 0.69357007\n",
      "Iteration 18, loss = 0.69311049\n",
      "Iteration 19, loss = 0.69283301\n",
      "Iteration 20, loss = 0.69270700\n",
      "Iteration 21, loss = 0.69269726\n",
      "Iteration 22, loss = 0.69276822\n",
      "Iteration 23, loss = 0.69288572\n",
      "Iteration 24, loss = 0.69301969\n",
      "Iteration 25, loss = 0.69314560\n",
      "Iteration 26, loss = 0.69325103\n",
      "Iteration 27, loss = 0.69332637\n",
      "Iteration 28, loss = 0.69336326\n",
      "Iteration 29, loss = 0.69335661\n",
      "Iteration 30, loss = 0.69331455\n",
      "Iteration 31, loss = 0.69324461\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80781766\n",
      "Iteration 2, loss = 0.75091611\n",
      "Iteration 3, loss = 0.73505664\n",
      "Iteration 4, loss = 0.72339807\n",
      "Iteration 5, loss = 0.71342914\n",
      "Iteration 6, loss = 0.70862901\n",
      "Iteration 7, loss = 0.70450829\n",
      "Iteration 8, loss = 0.70154827\n",
      "Iteration 9, loss = 0.70077877\n",
      "Iteration 10, loss = 0.69994109\n",
      "Iteration 11, loss = 0.69902190\n",
      "Iteration 12, loss = 0.69796581\n",
      "Iteration 13, loss = 0.69685415\n",
      "Iteration 14, loss = 0.69578086\n",
      "Iteration 15, loss = 0.69483133\n",
      "Iteration 16, loss = 0.69407147\n",
      "Iteration 17, loss = 0.69349024\n",
      "Iteration 18, loss = 0.69309610\n",
      "Iteration 19, loss = 0.69286464\n",
      "Iteration 20, loss = 0.69276559\n",
      "Iteration 21, loss = 0.69270696\n",
      "Iteration 22, loss = 0.69282873\n",
      "Iteration 23, loss = 0.69299494\n",
      "Iteration 24, loss = 0.69313233\n",
      "Iteration 25, loss = 0.69322481\n",
      "Iteration 26, loss = 0.69328110\n",
      "Iteration 27, loss = 0.69330235\n",
      "Iteration 28, loss = 0.69329571\n",
      "Iteration 29, loss = 0.69324845\n",
      "Iteration 30, loss = 0.69315162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80310699\n",
      "Iteration 2, loss = 0.73863356\n",
      "Iteration 3, loss = 0.69293169\n",
      "Iteration 4, loss = 0.69311965\n",
      "Iteration 5, loss = 0.69459223\n",
      "Iteration 6, loss = 0.69448358\n",
      "Iteration 7, loss = 0.69304908\n",
      "Iteration 8, loss = 0.69258039\n",
      "Iteration 9, loss = 0.69333846\n",
      "Iteration 10, loss = 0.69360198\n",
      "Iteration 11, loss = 0.69278756\n",
      "Iteration 12, loss = 0.69211311\n",
      "Iteration 13, loss = 0.69236695\n",
      "Iteration 14, loss = 0.69270635\n",
      "Iteration 15, loss = 0.69228050\n",
      "Iteration 16, loss = 0.69163925\n",
      "Iteration 17, loss = 0.69158899\n",
      "Iteration 18, loss = 0.69181756\n",
      "Iteration 19, loss = 0.69159027\n",
      "Iteration 20, loss = 0.69106229\n",
      "Iteration 21, loss = 0.69087386\n",
      "Iteration 22, loss = 0.69097353\n",
      "Iteration 23, loss = 0.69080237\n",
      "Iteration 24, loss = 0.69036852\n",
      "Iteration 25, loss = 0.69014658\n",
      "Iteration 26, loss = 0.69014507\n",
      "Iteration 27, loss = 0.68995960\n",
      "Iteration 28, loss = 0.68958679\n",
      "Iteration 29, loss = 0.68937026\n",
      "Iteration 30, loss = 0.68929673\n",
      "Iteration 31, loss = 0.68907189\n",
      "Iteration 32, loss = 0.68873726\n",
      "Iteration 33, loss = 0.68853794\n",
      "Iteration 34, loss = 0.68840321\n",
      "Iteration 35, loss = 0.68813633\n",
      "Iteration 36, loss = 0.68783415\n",
      "Iteration 37, loss = 0.68764360\n",
      "Iteration 38, loss = 0.68744772\n",
      "Iteration 39, loss = 0.68715138\n",
      "Iteration 40, loss = 0.68688097\n",
      "Iteration 41, loss = 0.68667825\n",
      "Iteration 42, loss = 0.68642215\n",
      "Iteration 43, loss = 0.68611962\n",
      "Iteration 44, loss = 0.68587049\n",
      "Iteration 45, loss = 0.68562889\n",
      "Iteration 46, loss = 0.68532924\n",
      "Iteration 47, loss = 0.68503970\n",
      "Iteration 48, loss = 0.68478330\n",
      "Iteration 49, loss = 0.68449065\n",
      "Iteration 50, loss = 0.68417803\n",
      "Iteration 51, loss = 0.68389498\n",
      "Iteration 52, loss = 0.68359923\n",
      "Iteration 53, loss = 0.68327367\n",
      "Iteration 54, loss = 0.68296434\n",
      "Iteration 55, loss = 0.68265560\n",
      "Iteration 56, loss = 0.68231861\n",
      "Iteration 57, loss = 0.68198619\n",
      "Iteration 58, loss = 0.68165861\n",
      "Iteration 59, loss = 0.68130753\n",
      "Iteration 60, loss = 0.68095423\n",
      "Iteration 61, loss = 0.68060509\n",
      "Iteration 62, loss = 0.68023633\n",
      "Iteration 63, loss = 0.67986226\n",
      "Iteration 64, loss = 0.67948977\n",
      "Iteration 65, loss = 0.67910002\n",
      "Iteration 66, loss = 0.67870423\n",
      "Iteration 67, loss = 0.67830684\n",
      "Iteration 68, loss = 0.67789329\n",
      "Iteration 69, loss = 0.67747390\n",
      "Iteration 70, loss = 0.67704941\n",
      "Iteration 71, loss = 0.67660968\n",
      "Iteration 72, loss = 0.67616452\n",
      "Iteration 73, loss = 0.67571052\n",
      "Iteration 74, loss = 0.67524248\n",
      "Iteration 75, loss = 0.67476852\n",
      "Iteration 76, loss = 0.67428227\n",
      "Iteration 77, loss = 0.67378398\n",
      "Iteration 78, loss = 0.67327770\n",
      "Iteration 79, loss = 0.67275689\n",
      "Iteration 80, loss = 0.67222573\n",
      "Iteration 81, loss = 0.67168293\n",
      "Iteration 82, loss = 0.67112578\n",
      "Iteration 83, loss = 0.67055803\n",
      "Iteration 84, loss = 0.66997527\n",
      "Iteration 85, loss = 0.66937955\n",
      "Iteration 86, loss = 0.66877020\n",
      "Iteration 87, loss = 0.66814530\n",
      "Iteration 88, loss = 0.66753077\n",
      "Iteration 89, loss = 0.66710204\n",
      "Iteration 90, loss = 0.66625676\n",
      "Iteration 91, loss = 0.66691935\n",
      "Iteration 92, loss = 0.67250908\n",
      "Iteration 93, loss = 0.66709315\n",
      "Iteration 94, loss = 0.67894372\n",
      "Iteration 95, loss = 0.70677100\n",
      "Iteration 96, loss = 0.70698695\n",
      "Iteration 97, loss = 0.66247462\n",
      "Iteration 98, loss = 0.69913877\n",
      "Iteration 99, loss = 0.70375647\n",
      "Iteration 100, loss = 0.70748014\n",
      "Iteration 101, loss = 0.70995971\n",
      "Iteration 102, loss = 0.71107201\n",
      "Iteration 103, loss = 0.71084884\n",
      "Iteration 104, loss = 0.70943148\n",
      "Iteration 105, loss = 0.70704007\n",
      "Iteration 106, loss = 0.70395139\n",
      "Iteration 107, loss = 0.70048114\n",
      "Iteration 108, loss = 0.69696704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80875424\n",
      "Iteration 2, loss = 0.74235980\n",
      "Iteration 3, loss = 0.69296119\n",
      "Iteration 4, loss = 0.69316572\n",
      "Iteration 5, loss = 0.69525989\n",
      "Iteration 6, loss = 0.69552983\n",
      "Iteration 7, loss = 0.69363727\n",
      "Iteration 8, loss = 0.69264840\n",
      "Iteration 9, loss = 0.69357024\n",
      "Iteration 10, loss = 0.69432153\n",
      "Iteration 11, loss = 0.69352643\n",
      "Iteration 12, loss = 0.69240723\n",
      "Iteration 13, loss = 0.69245893\n",
      "Iteration 14, loss = 0.69314321\n",
      "Iteration 15, loss = 0.69301860\n",
      "Iteration 16, loss = 0.69216360\n",
      "Iteration 17, loss = 0.69178652\n",
      "Iteration 18, loss = 0.69214130\n",
      "Iteration 19, loss = 0.69226999\n",
      "Iteration 20, loss = 0.69173761\n",
      "Iteration 21, loss = 0.69125141\n",
      "Iteration 22, loss = 0.69133172\n",
      "Iteration 23, loss = 0.69147119\n",
      "Iteration 24, loss = 0.69113833\n",
      "Iteration 25, loss = 0.69069198\n",
      "Iteration 26, loss = 0.69062659\n",
      "Iteration 27, loss = 0.69069165\n",
      "Iteration 28, loss = 0.69044354\n",
      "Iteration 29, loss = 0.69006373\n",
      "Iteration 30, loss = 0.68993894\n",
      "Iteration 31, loss = 0.68992710\n",
      "Iteration 32, loss = 0.68969525\n",
      "Iteration 33, loss = 0.68937053\n",
      "Iteration 34, loss = 0.68923083\n",
      "Iteration 35, loss = 0.68915250\n",
      "Iteration 36, loss = 0.68890886\n",
      "Iteration 37, loss = 0.68862659\n",
      "Iteration 38, loss = 0.68848339\n",
      "Iteration 39, loss = 0.68834657\n",
      "Iteration 40, loss = 0.68808629\n",
      "Iteration 41, loss = 0.68783941\n",
      "Iteration 42, loss = 0.68768741\n",
      "Iteration 43, loss = 0.68749568\n",
      "Iteration 44, loss = 0.68722856\n",
      "Iteration 45, loss = 0.68700947\n",
      "Iteration 46, loss = 0.68683252\n",
      "Iteration 47, loss = 0.68659470\n",
      "Iteration 48, loss = 0.68633574\n",
      "Iteration 49, loss = 0.68612827\n",
      "Iteration 50, loss = 0.68591078\n",
      "Iteration 51, loss = 0.68564652\n",
      "Iteration 52, loss = 0.68540287\n",
      "Iteration 53, loss = 0.68518053\n",
      "Iteration 54, loss = 0.68492200\n",
      "Iteration 55, loss = 0.68465494\n",
      "Iteration 56, loss = 0.68441335\n",
      "Iteration 57, loss = 0.68415526\n",
      "Iteration 58, loss = 0.68387533\n",
      "Iteration 59, loss = 0.68361180\n",
      "Iteration 60, loss = 0.68334563\n",
      "Iteration 61, loss = 0.68305635\n",
      "Iteration 62, loss = 0.68277321\n",
      "Iteration 63, loss = 0.68249320\n",
      "Iteration 64, loss = 0.68219344\n",
      "Iteration 65, loss = 0.68189276\n",
      "Iteration 66, loss = 0.68159591\n",
      "Iteration 67, loss = 0.68128301\n",
      "Iteration 68, loss = 0.68096567\n",
      "Iteration 69, loss = 0.68065059\n",
      "Iteration 70, loss = 0.68032185\n",
      "Iteration 71, loss = 0.67998730\n",
      "Iteration 72, loss = 0.67965280\n",
      "Iteration 73, loss = 0.67930588\n",
      "Iteration 74, loss = 0.67895309\n",
      "Iteration 75, loss = 0.67859796\n",
      "Iteration 76, loss = 0.67823086\n",
      "Iteration 77, loss = 0.67785819\n",
      "Iteration 78, loss = 0.67748070\n",
      "Iteration 79, loss = 0.67709168\n",
      "Iteration 80, loss = 0.67669739\n",
      "Iteration 81, loss = 0.67629568\n",
      "Iteration 82, loss = 0.67588310\n",
      "Iteration 83, loss = 0.67546486\n",
      "Iteration 84, loss = 0.67503684\n",
      "Iteration 85, loss = 0.67459917\n",
      "Iteration 86, loss = 0.67415436\n",
      "Iteration 87, loss = 0.67369823\n",
      "Iteration 88, loss = 0.67323338\n",
      "Iteration 89, loss = 0.67275897\n",
      "Iteration 90, loss = 0.67227320\n",
      "Iteration 91, loss = 0.67177844\n",
      "Iteration 92, loss = 0.67127184\n",
      "Iteration 93, loss = 0.67075459\n",
      "Iteration 94, loss = 0.67023253\n",
      "Iteration 95, loss = 0.66971401\n",
      "Iteration 96, loss = 0.66918056\n",
      "Iteration 97, loss = 0.66864120\n",
      "Iteration 98, loss = 0.66808398\n",
      "Iteration 99, loss = 0.66752285\n",
      "Iteration 100, loss = 0.66693964\n",
      "Iteration 101, loss = 0.66635174\n",
      "Iteration 102, loss = 0.66574425\n",
      "Iteration 103, loss = 0.66517762\n",
      "Iteration 104, loss = 0.66450210\n",
      "Iteration 105, loss = 0.66399010\n",
      "Iteration 106, loss = 0.66337544\n",
      "Iteration 107, loss = 0.66258592\n",
      "Iteration 108, loss = 0.66213406\n",
      "Iteration 109, loss = 0.66137167\n",
      "Iteration 110, loss = 0.66044941\n",
      "Iteration 111, loss = 0.65999885\n",
      "Iteration 112, loss = 0.65912361\n",
      "Iteration 113, loss = 0.65822874\n",
      "Iteration 114, loss = 0.65754265\n",
      "Iteration 115, loss = 0.65662602\n",
      "Iteration 116, loss = 0.65596981\n",
      "Iteration 117, loss = 0.65505412\n",
      "Iteration 118, loss = 0.65422339\n",
      "Iteration 119, loss = 0.65340839\n",
      "Iteration 120, loss = 0.65240531\n",
      "Iteration 121, loss = 0.65171493\n",
      "Iteration 122, loss = 0.65062001\n",
      "Iteration 123, loss = 0.64969916\n",
      "Iteration 124, loss = 0.64883779\n",
      "Iteration 125, loss = 0.64768750\n",
      "Iteration 126, loss = 0.64688198\n",
      "Iteration 127, loss = 0.64584868\n",
      "Iteration 128, loss = 0.64477240\n",
      "Iteration 129, loss = 0.64352803\n",
      "Iteration 130, loss = 0.64251006\n",
      "Iteration 131, loss = 0.64162954\n",
      "Iteration 132, loss = 0.64029754\n",
      "Iteration 133, loss = 0.63912044\n",
      "Iteration 134, loss = 0.63783419\n",
      "Iteration 135, loss = 0.63658404\n",
      "Iteration 136, loss = 0.63543757\n",
      "Iteration 137, loss = 0.63433300\n",
      "Iteration 138, loss = 0.63400400\n",
      "Iteration 139, loss = 0.63264629\n",
      "Iteration 140, loss = 0.63347029\n",
      "Iteration 141, loss = 0.63078351\n",
      "Iteration 142, loss = 0.62958630\n",
      "Iteration 143, loss = 0.62644447\n",
      "Iteration 144, loss = 0.62512066\n",
      "Iteration 145, loss = 0.62517297\n",
      "Iteration 146, loss = 0.62340641\n",
      "Iteration 147, loss = 0.62213368\n",
      "Iteration 148, loss = 0.61946596\n",
      "Iteration 149, loss = 0.61771884\n",
      "Iteration 150, loss = 0.61666629\n",
      "Iteration 151, loss = 0.61540446\n",
      "Iteration 152, loss = 0.61394993\n",
      "Iteration 153, loss = 0.61192137\n",
      "Iteration 154, loss = 0.60995421\n",
      "Iteration 155, loss = 0.60840792\n",
      "Iteration 156, loss = 0.60711969\n",
      "Iteration 157, loss = 0.60582997\n",
      "Iteration 158, loss = 0.60461875\n",
      "Iteration 159, loss = 0.60499053\n",
      "Iteration 160, loss = 0.60862692\n",
      "Iteration 161, loss = 0.64710916\n",
      "Iteration 162, loss = 0.68034390\n",
      "Iteration 163, loss = 0.68938668\n",
      "Iteration 164, loss = 0.60170391\n",
      "Iteration 165, loss = 0.72345641\n",
      "Iteration 166, loss = 0.72140327\n",
      "Iteration 167, loss = 0.69190694\n",
      "Iteration 168, loss = 0.67267285\n",
      "Iteration 169, loss = 0.61084776\n",
      "Iteration 170, loss = 0.67071417\n",
      "Iteration 171, loss = 0.67404609\n",
      "Iteration 172, loss = 0.67107388\n",
      "Iteration 173, loss = 0.67323846\n",
      "Iteration 174, loss = 0.67780320\n",
      "Iteration 175, loss = 0.68133774\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80781766\n",
      "Iteration 2, loss = 0.74051747\n",
      "Iteration 3, loss = 0.69278771\n",
      "Iteration 4, loss = 0.69301977\n",
      "Iteration 5, loss = 0.69473839\n",
      "Iteration 6, loss = 0.69479411\n",
      "Iteration 7, loss = 0.69318262\n",
      "Iteration 8, loss = 0.69250903\n",
      "Iteration 9, loss = 0.69334474\n",
      "Iteration 10, loss = 0.69381249\n",
      "Iteration 11, loss = 0.69300197\n",
      "Iteration 12, loss = 0.69215034\n",
      "Iteration 13, loss = 0.69234538\n",
      "Iteration 14, loss = 0.69284459\n",
      "Iteration 15, loss = 0.69253359\n",
      "Iteration 16, loss = 0.69179611\n",
      "Iteration 17, loss = 0.69163530\n",
      "Iteration 18, loss = 0.69194953\n",
      "Iteration 19, loss = 0.69186828\n",
      "Iteration 20, loss = 0.69131986\n",
      "Iteration 21, loss = 0.69102308\n",
      "Iteration 22, loss = 0.69115703\n",
      "Iteration 23, loss = 0.69112522\n",
      "Iteration 24, loss = 0.69071111\n",
      "Iteration 25, loss = 0.69040330\n",
      "Iteration 26, loss = 0.69041990\n",
      "Iteration 27, loss = 0.69035623\n",
      "Iteration 28, loss = 0.69001408\n",
      "Iteration 29, loss = 0.68973443\n",
      "Iteration 30, loss = 0.68968462\n",
      "Iteration 31, loss = 0.68956699\n",
      "Iteration 32, loss = 0.68925531\n",
      "Iteration 33, loss = 0.68901318\n",
      "Iteration 34, loss = 0.68892130\n",
      "Iteration 35, loss = 0.68874578\n",
      "Iteration 36, loss = 0.68845121\n",
      "Iteration 37, loss = 0.68824182\n",
      "Iteration 38, loss = 0.68811001\n",
      "Iteration 39, loss = 0.68788300\n",
      "Iteration 40, loss = 0.68760780\n",
      "Iteration 41, loss = 0.68741873\n",
      "Iteration 42, loss = 0.68723842\n",
      "Iteration 43, loss = 0.68697612\n",
      "Iteration 44, loss = 0.68672641\n",
      "Iteration 45, loss = 0.68653350\n",
      "Iteration 46, loss = 0.68630066\n",
      "Iteration 47, loss = 0.68602826\n",
      "Iteration 48, loss = 0.68579786\n",
      "Iteration 49, loss = 0.68557313\n",
      "Iteration 50, loss = 0.68530166\n",
      "Iteration 51, loss = 0.68503930\n",
      "Iteration 52, loss = 0.68480178\n",
      "Iteration 53, loss = 0.68453327\n",
      "Iteration 54, loss = 0.68425130\n",
      "Iteration 55, loss = 0.68399261\n",
      "Iteration 56, loss = 0.68371989\n",
      "Iteration 57, loss = 0.68342525\n",
      "Iteration 58, loss = 0.68314496\n",
      "Iteration 59, loss = 0.68286087\n",
      "Iteration 60, loss = 0.68255479\n",
      "Iteration 61, loss = 0.68225491\n",
      "Iteration 62, loss = 0.68195502\n",
      "Iteration 63, loss = 0.68163575\n",
      "Iteration 64, loss = 0.68131740\n",
      "Iteration 65, loss = 0.68099911\n",
      "Iteration 66, loss = 0.68066414\n",
      "Iteration 67, loss = 0.68032749\n",
      "Iteration 68, loss = 0.67998919\n",
      "Iteration 69, loss = 0.67963605\n",
      "Iteration 70, loss = 0.67928010\n",
      "Iteration 71, loss = 0.67892010\n",
      "Iteration 72, loss = 0.67854669\n",
      "Iteration 73, loss = 0.67817006\n",
      "Iteration 74, loss = 0.67778662\n",
      "Iteration 75, loss = 0.67739105\n",
      "Iteration 76, loss = 0.67699161\n",
      "Iteration 77, loss = 0.67658268\n",
      "Iteration 78, loss = 0.67616330\n",
      "Iteration 79, loss = 0.67573861\n",
      "Iteration 80, loss = 0.67530232\n",
      "Iteration 81, loss = 0.67485717\n",
      "Iteration 82, loss = 0.67440411\n",
      "Iteration 83, loss = 0.67393885\n",
      "Iteration 84, loss = 0.67346549\n",
      "Iteration 85, loss = 0.67298111\n",
      "Iteration 86, loss = 0.67248535\n",
      "Iteration 87, loss = 0.67198017\n",
      "Iteration 88, loss = 0.67146214\n",
      "Iteration 89, loss = 0.67093369\n",
      "Iteration 90, loss = 0.67039295\n",
      "Iteration 91, loss = 0.66983959\n",
      "Iteration 92, loss = 0.66927451\n",
      "Iteration 93, loss = 0.66869550\n",
      "Iteration 94, loss = 0.66812244\n",
      "Iteration 95, loss = 0.66752777\n",
      "Iteration 96, loss = 0.66693214\n",
      "Iteration 97, loss = 0.66633075\n",
      "Iteration 98, loss = 0.66570549\n",
      "Iteration 99, loss = 0.66507390\n",
      "Iteration 100, loss = 0.66442497\n",
      "Iteration 101, loss = 0.66375610\n",
      "Iteration 102, loss = 0.66306840\n",
      "Iteration 103, loss = 0.66236243\n",
      "Iteration 104, loss = 0.66164371\n",
      "Iteration 105, loss = 0.66094144\n",
      "Iteration 106, loss = 0.66077365\n",
      "Iteration 107, loss = 0.65939840\n",
      "Iteration 108, loss = 0.65905030\n",
      "Iteration 109, loss = 0.65931109\n",
      "Iteration 110, loss = 0.65717742\n",
      "Iteration 111, loss = 0.65920957\n",
      "Iteration 112, loss = 0.66815045\n",
      "Iteration 113, loss = 0.65712938\n",
      "Iteration 114, loss = 0.67917699\n",
      "Iteration 115, loss = 0.69903614\n",
      "Iteration 116, loss = 0.69080060\n",
      "Iteration 117, loss = 0.65749303\n",
      "Iteration 118, loss = 0.71742053\n",
      "Iteration 119, loss = 0.66860011\n",
      "Iteration 120, loss = 0.66329473\n",
      "Iteration 121, loss = 0.68629801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70461589\n",
      "Iteration 2, loss = 0.69424886\n",
      "Iteration 3, loss = 0.69369561\n",
      "Iteration 4, loss = 0.69375915\n",
      "Iteration 5, loss = 0.69244152\n",
      "Iteration 6, loss = 0.69214853\n",
      "Iteration 7, loss = 0.69180436\n",
      "Iteration 8, loss = 0.69173712\n",
      "Iteration 9, loss = 0.69170879\n",
      "Iteration 10, loss = 0.69180654\n",
      "Iteration 11, loss = 0.69072317\n",
      "Iteration 12, loss = 0.69139889\n",
      "Iteration 13, loss = 0.69114324\n",
      "Iteration 14, loss = 0.69088906\n",
      "Iteration 15, loss = 0.69062155\n",
      "Iteration 16, loss = 0.69034316\n",
      "Iteration 17, loss = 0.69005384\n",
      "Iteration 18, loss = 0.68975436\n",
      "Iteration 19, loss = 0.68944538\n",
      "Iteration 20, loss = 0.68912745\n",
      "Iteration 21, loss = 0.68880102\n",
      "Iteration 22, loss = 0.68846646\n",
      "Iteration 23, loss = 0.68812407\n",
      "Iteration 24, loss = 0.68777407\n",
      "Iteration 25, loss = 0.68741659\n",
      "Iteration 26, loss = 0.68705173\n",
      "Iteration 27, loss = 0.68667953\n",
      "Iteration 28, loss = 0.68629997\n",
      "Iteration 29, loss = 0.68591297\n",
      "Iteration 30, loss = 0.68551843\n",
      "Iteration 31, loss = 0.68511621\n",
      "Iteration 32, loss = 0.68470611\n",
      "Iteration 33, loss = 0.68428791\n",
      "Iteration 34, loss = 0.68386136\n",
      "Iteration 35, loss = 0.68342617\n",
      "Iteration 36, loss = 0.68298202\n",
      "Iteration 37, loss = 0.68252859\n",
      "Iteration 38, loss = 0.68206549\n",
      "Iteration 39, loss = 0.68159233\n",
      "Iteration 40, loss = 0.68110870\n",
      "Iteration 41, loss = 0.68061416\n",
      "Iteration 42, loss = 0.68010824\n",
      "Iteration 43, loss = 0.67959044\n",
      "Iteration 44, loss = 0.67906027\n",
      "Iteration 45, loss = 0.67851719\n",
      "Iteration 46, loss = 0.67796064\n",
      "Iteration 47, loss = 0.67739004\n",
      "Iteration 48, loss = 0.67680478\n",
      "Iteration 49, loss = 0.67620425\n",
      "Iteration 50, loss = 0.67558780\n",
      "Iteration 51, loss = 0.67495474\n",
      "Iteration 52, loss = 0.67430437\n",
      "Iteration 53, loss = 0.67363597\n",
      "Iteration 54, loss = 0.67294879\n",
      "Iteration 55, loss = 0.67224203\n",
      "Iteration 56, loss = 0.67151490\n",
      "Iteration 57, loss = 0.67076655\n",
      "Iteration 58, loss = 0.66999610\n",
      "Iteration 59, loss = 0.66920266\n",
      "Iteration 60, loss = 0.66838529\n",
      "Iteration 61, loss = 0.66754301\n",
      "Iteration 62, loss = 0.66667481\n",
      "Iteration 63, loss = 0.66577965\n",
      "Iteration 64, loss = 0.66485644\n",
      "Iteration 65, loss = 0.66390405\n",
      "Iteration 66, loss = 0.66292133\n",
      "Iteration 67, loss = 0.66190705\n",
      "Iteration 68, loss = 0.66085995\n",
      "Iteration 69, loss = 0.65977874\n",
      "Iteration 70, loss = 0.65866205\n",
      "Iteration 71, loss = 0.65750848\n",
      "Iteration 72, loss = 0.65631657\n",
      "Iteration 73, loss = 0.65508481\n",
      "Iteration 74, loss = 0.65381163\n",
      "Iteration 75, loss = 0.65249539\n",
      "Iteration 76, loss = 0.65113441\n",
      "Iteration 77, loss = 0.64972693\n",
      "Iteration 78, loss = 0.64827114\n",
      "Iteration 79, loss = 0.64676516\n",
      "Iteration 80, loss = 0.64520703\n",
      "Iteration 81, loss = 0.64359524\n",
      "Iteration 82, loss = 0.64201304\n",
      "Iteration 83, loss = 0.64325104\n",
      "Iteration 84, loss = 0.83514278\n",
      "Iteration 85, loss = 0.92040118\n",
      "Iteration 86, loss = 0.72670865\n",
      "Iteration 87, loss = 0.70480294\n",
      "Iteration 88, loss = 0.69591699\n",
      "Iteration 89, loss = 0.68914261\n",
      "Iteration 90, loss = 0.69088539\n",
      "Iteration 91, loss = 0.69080077\n",
      "Iteration 92, loss = 0.68970806\n",
      "Iteration 93, loss = 0.68803436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70296955\n",
      "Iteration 2, loss = 0.69177452\n",
      "Iteration 3, loss = 0.69147193\n",
      "Iteration 4, loss = 0.69184494\n",
      "Iteration 5, loss = 0.69098625\n",
      "Iteration 6, loss = 0.69169771\n",
      "Iteration 7, loss = 0.69185612\n",
      "Iteration 8, loss = 0.69178690\n",
      "Iteration 9, loss = 0.69154368\n",
      "Iteration 10, loss = 0.69125168\n",
      "Iteration 11, loss = 0.69144428\n",
      "Iteration 12, loss = 0.69083903\n",
      "Iteration 13, loss = 0.69103027\n",
      "Iteration 14, loss = 0.69052917\n",
      "Iteration 15, loss = 0.69028860\n",
      "Iteration 16, loss = 0.69003023\n",
      "Iteration 17, loss = 0.68971180\n",
      "Iteration 18, loss = 0.68953753\n",
      "Iteration 19, loss = 0.68936383\n",
      "Iteration 20, loss = 0.69020306\n",
      "Iteration 21, loss = 0.68881237\n",
      "Iteration 22, loss = 0.68845312\n",
      "Iteration 23, loss = 0.68817982\n",
      "Iteration 24, loss = 0.68790210\n",
      "Iteration 25, loss = 0.68759331\n",
      "Iteration 26, loss = 0.68729763\n",
      "Iteration 27, loss = 0.68701406\n",
      "Iteration 28, loss = 0.68667669\n",
      "Iteration 29, loss = 0.68635759\n",
      "Iteration 30, loss = 0.68605608\n",
      "Iteration 31, loss = 0.68570452\n",
      "Iteration 32, loss = 0.68535331\n",
      "Iteration 33, loss = 0.68495279\n",
      "Iteration 34, loss = 0.68490798\n",
      "Iteration 35, loss = 0.68456942\n",
      "Iteration 36, loss = 0.68422610\n",
      "Iteration 37, loss = 0.68447654\n",
      "Iteration 38, loss = 0.68455729\n",
      "Iteration 39, loss = 0.68424758\n",
      "Iteration 40, loss = 0.68389872\n",
      "Iteration 41, loss = 0.68354889\n",
      "Iteration 42, loss = 0.68319172\n",
      "Iteration 43, loss = 0.68282665\n",
      "Iteration 44, loss = 0.68245357\n",
      "Iteration 45, loss = 0.68207245\n",
      "Iteration 46, loss = 0.68168325\n",
      "Iteration 47, loss = 0.68128586\n",
      "Iteration 48, loss = 0.68088017\n",
      "Iteration 49, loss = 0.68046603\n",
      "Iteration 50, loss = 0.68004328\n",
      "Iteration 51, loss = 0.67961171\n",
      "Iteration 52, loss = 0.67917112\n",
      "Iteration 53, loss = 0.67872127\n",
      "Iteration 54, loss = 0.67826190\n",
      "Iteration 55, loss = 0.67779272\n",
      "Iteration 56, loss = 0.67731345\n",
      "Iteration 57, loss = 0.67682375\n",
      "Iteration 58, loss = 0.67632330\n",
      "Iteration 59, loss = 0.67581173\n",
      "Iteration 60, loss = 0.67528866\n",
      "Iteration 61, loss = 0.67475371\n",
      "Iteration 62, loss = 0.67420645\n",
      "Iteration 63, loss = 0.67364645\n",
      "Iteration 64, loss = 0.67307326\n",
      "Iteration 65, loss = 0.67248640\n",
      "Iteration 66, loss = 0.67188538\n",
      "Iteration 67, loss = 0.67126968\n",
      "Iteration 68, loss = 0.67063876\n",
      "Iteration 69, loss = 0.66999208\n",
      "Iteration 70, loss = 0.66932906\n",
      "Iteration 71, loss = 0.66864908\n",
      "Iteration 72, loss = 0.66795152\n",
      "Iteration 73, loss = 0.66723575\n",
      "Iteration 74, loss = 0.66650108\n",
      "Iteration 75, loss = 0.66574682\n",
      "Iteration 76, loss = 0.66497223\n",
      "Iteration 77, loss = 0.66417658\n",
      "Iteration 78, loss = 0.66335907\n",
      "Iteration 79, loss = 0.66251891\n",
      "Iteration 80, loss = 0.66165524\n",
      "Iteration 81, loss = 0.66076721\n",
      "Iteration 82, loss = 0.65985390\n",
      "Iteration 83, loss = 0.65891439\n",
      "Iteration 84, loss = 0.65794770\n",
      "Iteration 85, loss = 0.65695284\n",
      "Iteration 86, loss = 0.65592875\n",
      "Iteration 87, loss = 0.65487436\n",
      "Iteration 88, loss = 0.65378854\n",
      "Iteration 89, loss = 0.65267016\n",
      "Iteration 90, loss = 0.65151799\n",
      "Iteration 91, loss = 0.65033080\n",
      "Iteration 92, loss = 0.64910731\n",
      "Iteration 93, loss = 0.64784618\n",
      "Iteration 94, loss = 0.64654603\n",
      "Iteration 95, loss = 0.64520544\n",
      "Iteration 96, loss = 0.64382294\n",
      "Iteration 97, loss = 0.64239700\n",
      "Iteration 98, loss = 0.64092604\n",
      "Iteration 99, loss = 0.63940845\n",
      "Iteration 100, loss = 0.63784254\n",
      "Iteration 101, loss = 0.63622659\n",
      "Iteration 102, loss = 0.63455880\n",
      "Iteration 103, loss = 0.63283734\n",
      "Iteration 104, loss = 0.63106031\n",
      "Iteration 105, loss = 0.62922576\n",
      "Iteration 106, loss = 0.62733170\n",
      "Iteration 107, loss = 0.62537607\n",
      "Iteration 108, loss = 0.62335702\n",
      "Iteration 109, loss = 0.62127941\n",
      "Iteration 110, loss = 0.61934704\n",
      "Iteration 111, loss = 0.62410744\n",
      "Iteration 112, loss = 0.74878447\n",
      "Iteration 113, loss = 0.69619614\n",
      "Iteration 114, loss = 0.61655390\n",
      "Iteration 115, loss = 0.73380079\n",
      "Iteration 116, loss = 0.92912058\n",
      "Iteration 117, loss = 1.40438725\n",
      "Iteration 118, loss = 0.68996345\n",
      "Iteration 119, loss = 0.78751861\n",
      "Iteration 120, loss = 0.70099908\n",
      "Iteration 121, loss = 0.69020421\n",
      "Iteration 122, loss = 0.68981359\n",
      "Iteration 123, loss = 0.68951282\n",
      "Iteration 124, loss = 0.69664524\n",
      "Iteration 125, loss = 0.69500213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70523959\n",
      "Iteration 2, loss = 0.69329182\n",
      "Iteration 3, loss = 0.69397733\n",
      "Iteration 4, loss = 0.69381742\n",
      "Iteration 5, loss = 0.69240225\n",
      "Iteration 6, loss = 0.69204445\n",
      "Iteration 7, loss = 0.69216414\n",
      "Iteration 8, loss = 0.69174048\n",
      "Iteration 9, loss = 0.69149782\n",
      "Iteration 10, loss = 0.69127098\n",
      "Iteration 11, loss = 0.69106239\n",
      "Iteration 12, loss = 0.69083471\n",
      "Iteration 13, loss = 0.69058935\n",
      "Iteration 14, loss = 0.69033264\n",
      "Iteration 15, loss = 0.69006621\n",
      "Iteration 16, loss = 0.68978958\n",
      "Iteration 17, loss = 0.68950911\n",
      "Iteration 18, loss = 0.68922416\n",
      "Iteration 19, loss = 0.68892626\n",
      "Iteration 20, loss = 0.68862850\n",
      "Iteration 21, loss = 0.68831386\n",
      "Iteration 22, loss = 0.68799833\n",
      "Iteration 23, loss = 0.68768133\n",
      "Iteration 24, loss = 0.68735574\n",
      "Iteration 25, loss = 0.68702728\n",
      "Iteration 26, loss = 0.68668590\n",
      "Iteration 27, loss = 0.68634980\n",
      "Iteration 28, loss = 0.68599458\n",
      "Iteration 29, loss = 0.68564252\n",
      "Iteration 30, loss = 0.68529254\n",
      "Iteration 31, loss = 0.68491938\n",
      "Iteration 32, loss = 0.68456255\n",
      "Iteration 33, loss = 0.68418096\n",
      "Iteration 34, loss = 0.68380139\n",
      "Iteration 35, loss = 0.68341004\n",
      "Iteration 36, loss = 0.68303431\n",
      "Iteration 37, loss = 0.68262251\n",
      "Iteration 38, loss = 0.68222525\n",
      "Iteration 39, loss = 0.68180331\n",
      "Iteration 40, loss = 0.68139582\n",
      "Iteration 41, loss = 0.68096931\n",
      "Iteration 42, loss = 0.68052662\n",
      "Iteration 43, loss = 0.68008958\n",
      "Iteration 44, loss = 0.67963478\n",
      "Iteration 45, loss = 0.67919559\n",
      "Iteration 46, loss = 0.67872039\n",
      "Iteration 47, loss = 0.67824746\n",
      "Iteration 48, loss = 0.67776035\n",
      "Iteration 49, loss = 0.67725407\n",
      "Iteration 50, loss = 0.67674614\n",
      "Iteration 51, loss = 0.67624061\n",
      "Iteration 52, loss = 0.67572130\n",
      "Iteration 53, loss = 0.67517827\n",
      "Iteration 54, loss = 0.67462026\n",
      "Iteration 55, loss = 0.67405815\n",
      "Iteration 56, loss = 0.67350600\n",
      "Iteration 57, loss = 0.67294788\n",
      "Iteration 58, loss = 0.67234084\n",
      "Iteration 59, loss = 0.67169063\n",
      "Iteration 60, loss = 0.67104569\n",
      "Iteration 61, loss = 0.67039554\n",
      "Iteration 62, loss = 0.66974156\n",
      "Iteration 63, loss = 0.66907821\n",
      "Iteration 64, loss = 0.66838362\n",
      "Iteration 65, loss = 0.66767794\n",
      "Iteration 66, loss = 0.66696297\n",
      "Iteration 67, loss = 0.66628803\n",
      "Iteration 68, loss = 0.66565369\n",
      "Iteration 69, loss = 0.66525938\n",
      "Iteration 70, loss = 0.66567914\n",
      "Iteration 71, loss = 0.66742740\n",
      "Iteration 72, loss = 0.70307797\n",
      "Iteration 73, loss = 0.67804589\n",
      "Iteration 74, loss = 0.68138247\n",
      "Iteration 75, loss = 0.67647714\n",
      "Iteration 76, loss = 0.67921665\n",
      "Iteration 77, loss = 0.68548504\n",
      "Iteration 78, loss = 0.75783254\n",
      "Iteration 79, loss = 0.72996338\n",
      "Iteration 80, loss = 0.73665848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70461589\n",
      "Iteration 2, loss = 5.31979866\n",
      "Iteration 3, loss = 0.69324660\n",
      "Iteration 4, loss = 0.69328745\n",
      "Iteration 5, loss = 0.69332560\n",
      "Iteration 6, loss = 0.69336061\n",
      "Iteration 7, loss = 0.69339234\n",
      "Iteration 8, loss = 0.69342076\n",
      "Iteration 9, loss = 0.69344593\n",
      "Iteration 10, loss = 0.69346794\n",
      "Iteration 11, loss = 0.69348693\n",
      "Iteration 12, loss = 0.69350303\n",
      "Iteration 13, loss = 0.69351639\n",
      "Iteration 14, loss = 0.69352719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70296955\n",
      "Iteration 2, loss = 5.38726344\n",
      "Iteration 3, loss = 0.69315143\n",
      "Iteration 4, loss = 0.69316197\n",
      "Iteration 5, loss = 0.69317636\n",
      "Iteration 6, loss = 0.69319248\n",
      "Iteration 7, loss = 0.69320915\n",
      "Iteration 8, loss = 0.69322568\n",
      "Iteration 9, loss = 0.69324165\n",
      "Iteration 10, loss = 0.69325678\n",
      "Iteration 11, loss = 0.69327089\n",
      "Iteration 12, loss = 0.69328389\n",
      "Iteration 13, loss = 0.69329572\n",
      "Iteration 14, loss = 0.69330636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70523959\n",
      "Iteration 2, loss = 5.35257442\n",
      "Iteration 3, loss = 0.69319905\n",
      "Iteration 4, loss = 0.69322501\n",
      "Iteration 5, loss = 0.69325163\n",
      "Iteration 6, loss = 0.69327761\n",
      "Iteration 7, loss = 0.69330226\n",
      "Iteration 8, loss = 0.69332524\n",
      "Iteration 9, loss = 0.69334634\n",
      "Iteration 10, loss = 0.69336547\n",
      "Iteration 11, loss = 0.69338260\n",
      "Iteration 12, loss = 0.69339775\n",
      "Iteration 13, loss = 0.69341096\n",
      "Iteration 14, loss = 0.69342229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69498415\n",
      "Iteration 2, loss = 0.73265129\n",
      "Iteration 3, loss = 1.08896037\n",
      "Iteration 4, loss = 0.75439742\n",
      "Iteration 5, loss = 0.75429102\n",
      "Iteration 6, loss = 0.74981507\n",
      "Iteration 7, loss = 0.74260265\n",
      "Iteration 8, loss = 0.72235828\n",
      "Iteration 9, loss = 0.70655330\n",
      "Iteration 10, loss = 0.69965626\n",
      "Iteration 11, loss = 0.69164171\n",
      "Iteration 12, loss = 0.69173384\n",
      "Iteration 13, loss = 0.69141369\n",
      "Iteration 14, loss = 0.69118111\n",
      "Iteration 15, loss = 0.69104334\n",
      "Iteration 16, loss = 0.69089853\n",
      "Iteration 17, loss = 0.69074757\n",
      "Iteration 18, loss = 0.69058995\n",
      "Iteration 19, loss = 0.69042661\n",
      "Iteration 20, loss = 0.69025804\n",
      "Iteration 21, loss = 0.69008455\n",
      "Iteration 22, loss = 0.68990644\n",
      "Iteration 23, loss = 0.68972396\n",
      "Iteration 24, loss = 0.68953732\n",
      "Iteration 25, loss = 0.68934667\n",
      "Iteration 26, loss = 0.68915215\n",
      "Iteration 27, loss = 0.68895386\n",
      "Iteration 28, loss = 0.68875186\n",
      "Iteration 29, loss = 0.68854619\n",
      "Iteration 30, loss = 0.68833688\n",
      "Iteration 31, loss = 0.68812393\n",
      "Iteration 32, loss = 0.68790731\n",
      "Iteration 33, loss = 0.68768699\n",
      "Iteration 34, loss = 0.68746293\n",
      "Iteration 35, loss = 0.68723504\n",
      "Iteration 36, loss = 0.68700326\n",
      "Iteration 37, loss = 0.68676749\n",
      "Iteration 38, loss = 0.68652762\n",
      "Iteration 39, loss = 0.68628355\n",
      "Iteration 40, loss = 0.68603514\n",
      "Iteration 41, loss = 0.68578227\n",
      "Iteration 42, loss = 0.68552479\n",
      "Iteration 43, loss = 0.68526253\n",
      "Iteration 44, loss = 0.68499536\n",
      "Iteration 45, loss = 0.68472308\n",
      "Iteration 46, loss = 0.68444552\n",
      "Iteration 47, loss = 0.68416249\n",
      "Iteration 48, loss = 0.68387380\n",
      "Iteration 49, loss = 0.68357924\n",
      "Iteration 50, loss = 0.68327860\n",
      "Iteration 51, loss = 0.68297164\n",
      "Iteration 52, loss = 0.68265815\n",
      "Iteration 53, loss = 0.68233788\n",
      "Iteration 54, loss = 0.68201059\n",
      "Iteration 55, loss = 0.68167601\n",
      "Iteration 56, loss = 0.68133387\n",
      "Iteration 57, loss = 0.68098390\n",
      "Iteration 58, loss = 0.68062581\n",
      "Iteration 59, loss = 0.68025931\n",
      "Iteration 60, loss = 0.67988407\n",
      "Iteration 61, loss = 0.67949979\n",
      "Iteration 62, loss = 0.67910613\n",
      "Iteration 63, loss = 0.67870275\n",
      "Iteration 64, loss = 0.67828928\n",
      "Iteration 65, loss = 0.67786537\n",
      "Iteration 66, loss = 0.67743063\n",
      "Iteration 67, loss = 0.67698465\n",
      "Iteration 68, loss = 0.67652704\n",
      "Iteration 69, loss = 0.67605736\n",
      "Iteration 70, loss = 0.67557518\n",
      "Iteration 71, loss = 0.67508002\n",
      "Iteration 72, loss = 0.67457142\n",
      "Iteration 73, loss = 0.67404889\n",
      "Iteration 74, loss = 0.67351190\n",
      "Iteration 75, loss = 0.67295993\n",
      "Iteration 76, loss = 0.67239242\n",
      "Iteration 77, loss = 0.67180880\n",
      "Iteration 78, loss = 0.67120847\n",
      "Iteration 79, loss = 0.67059082\n",
      "Iteration 80, loss = 0.66995520\n",
      "Iteration 81, loss = 0.66930095\n",
      "Iteration 82, loss = 0.66862736\n",
      "Iteration 83, loss = 0.66793371\n",
      "Iteration 84, loss = 0.66721926\n",
      "Iteration 85, loss = 0.66648323\n",
      "Iteration 86, loss = 0.66572480\n",
      "Iteration 87, loss = 0.66494312\n",
      "Iteration 88, loss = 0.66413733\n",
      "Iteration 89, loss = 0.66330652\n",
      "Iteration 90, loss = 0.66244972\n",
      "Iteration 91, loss = 0.66156595\n",
      "Iteration 92, loss = 0.66065420\n",
      "Iteration 93, loss = 0.65971339\n",
      "Iteration 94, loss = 0.65874240\n",
      "Iteration 95, loss = 0.65774010\n",
      "Iteration 96, loss = 0.65670526\n",
      "Iteration 97, loss = 0.65563666\n",
      "Iteration 98, loss = 0.65453298\n",
      "Iteration 99, loss = 0.65339287\n",
      "Iteration 100, loss = 0.65221494\n",
      "Iteration 101, loss = 0.65099772\n",
      "Iteration 102, loss = 0.64973969\n",
      "Iteration 103, loss = 0.64843929\n",
      "Iteration 104, loss = 0.64709500\n",
      "Iteration 105, loss = 0.64570672\n",
      "Iteration 106, loss = 0.64429938\n",
      "Iteration 107, loss = 0.64331361\n",
      "Iteration 108, loss = 0.65082323\n",
      "Iteration 109, loss = 0.87503964\n",
      "Iteration 110, loss = 0.72405435\n",
      "Iteration 111, loss = 0.72718855\n",
      "Iteration 112, loss = 0.73507711\n",
      "Iteration 113, loss = 0.73272483\n",
      "Iteration 114, loss = 0.72986734\n",
      "Iteration 115, loss = 0.72687840\n",
      "Iteration 116, loss = 0.72402356\n",
      "Iteration 117, loss = 0.72144102\n",
      "Iteration 118, loss = 0.72013592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353788\n",
      "Iteration 2, loss = 0.69385580\n",
      "Iteration 3, loss = 0.76728230\n",
      "Iteration 4, loss = 1.77095509\n",
      "Iteration 5, loss = 0.69879832\n",
      "Iteration 6, loss = 0.69936809\n",
      "Iteration 7, loss = 0.69985051\n",
      "Iteration 8, loss = 0.70020907\n",
      "Iteration 9, loss = 0.70044678\n",
      "Iteration 10, loss = 0.70057032\n",
      "Iteration 11, loss = 0.70058896\n",
      "Iteration 12, loss = 0.70051364\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69160887\n",
      "Iteration 2, loss = 0.69720629\n",
      "Iteration 3, loss = 0.87924544\n",
      "Iteration 4, loss = 2.17049681\n",
      "Iteration 5, loss = 0.69963222\n",
      "Iteration 6, loss = 0.70028169\n",
      "Iteration 7, loss = 0.70078269\n",
      "Iteration 8, loss = 0.70117282\n",
      "Iteration 9, loss = 0.70144738\n",
      "Iteration 10, loss = 0.70159388\n",
      "Iteration 11, loss = 0.70162262\n",
      "Iteration 12, loss = 0.70160846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69498415\n",
      "Iteration 2, loss = 17.90405757\n",
      "Iteration 3, loss = 15.15427181\n",
      "Iteration 4, loss = 4.31385118\n",
      "Iteration 5, loss = 1.22475882\n",
      "Iteration 6, loss = 0.70170241\n",
      "Iteration 7, loss = 0.70279705\n",
      "Iteration 8, loss = 0.70379232\n",
      "Iteration 9, loss = 0.70469578\n",
      "Iteration 10, loss = 0.70538082\n",
      "Iteration 11, loss = 0.70586498\n",
      "Iteration 12, loss = 0.70616709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353788\n",
      "Iteration 2, loss = 18.13963724\n",
      "Iteration 3, loss = 7.69202247\n",
      "Iteration 4, loss = 5.54925845\n",
      "Iteration 5, loss = 4.44456332\n",
      "Iteration 6, loss = 2.76024109\n",
      "Iteration 7, loss = 1.30733241\n",
      "Iteration 8, loss = 0.77774332\n",
      "Iteration 9, loss = 0.69973929\n",
      "Iteration 10, loss = 0.77773776\n",
      "Iteration 11, loss = 0.82527853\n",
      "Iteration 12, loss = 0.80662979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69160887\n",
      "Iteration 2, loss = 18.02184742\n",
      "Iteration 3, loss = 8.61359750\n",
      "Iteration 4, loss = 4.54489312\n",
      "Iteration 5, loss = 2.01110234\n",
      "Iteration 6, loss = 0.69917689\n",
      "Iteration 7, loss = 0.70005656\n",
      "Iteration 8, loss = 0.70061072\n",
      "Iteration 9, loss = 0.70092760\n",
      "Iteration 10, loss = 0.70104044\n",
      "Iteration 11, loss = 0.70097938\n",
      "Iteration 12, loss = 0.70077255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80310699\n",
      "Iteration 2, loss = 0.71334035\n",
      "Iteration 3, loss = 0.70315283\n",
      "Iteration 4, loss = 0.69447651\n",
      "Iteration 5, loss = 0.69302037\n",
      "Iteration 6, loss = 0.69531398\n",
      "Iteration 7, loss = 0.69601784\n",
      "Iteration 8, loss = 0.69439252\n",
      "Iteration 9, loss = 0.69284766\n",
      "Iteration 10, loss = 0.69248415\n",
      "Iteration 11, loss = 0.69262960\n",
      "Iteration 12, loss = 0.69261118\n",
      "Iteration 13, loss = 0.69238446\n",
      "Iteration 14, loss = 0.69217657\n",
      "Iteration 15, loss = 0.69209073\n",
      "Iteration 16, loss = 0.69201055\n",
      "Iteration 17, loss = 0.69196663\n",
      "Iteration 18, loss = 0.69186806\n",
      "Iteration 19, loss = 0.69177440\n",
      "Iteration 20, loss = 0.69167962\n",
      "Iteration 21, loss = 0.69158239\n",
      "Iteration 22, loss = 0.69148185\n",
      "Iteration 23, loss = 0.69137968\n",
      "Iteration 24, loss = 0.69127627\n",
      "Iteration 25, loss = 0.69117120\n",
      "Iteration 26, loss = 0.69106437\n",
      "Iteration 27, loss = 0.69095588\n",
      "Iteration 28, loss = 0.69084580\n",
      "Iteration 29, loss = 0.69073411\n",
      "Iteration 30, loss = 0.69062079\n",
      "Iteration 31, loss = 0.69050584\n",
      "Iteration 32, loss = 0.69038924\n",
      "Iteration 33, loss = 0.69027095\n",
      "Iteration 34, loss = 0.69015095\n",
      "Iteration 35, loss = 0.69002918\n",
      "Iteration 36, loss = 0.68990559\n",
      "Iteration 37, loss = 0.68978012\n",
      "Iteration 38, loss = 0.68965272\n",
      "Iteration 39, loss = 0.68952331\n",
      "Iteration 40, loss = 0.68939181\n",
      "Iteration 41, loss = 0.68925816\n",
      "Iteration 42, loss = 0.68912225\n",
      "Iteration 43, loss = 0.68898401\n",
      "Iteration 44, loss = 0.68884335\n",
      "Iteration 45, loss = 0.68870015\n",
      "Iteration 46, loss = 0.68855433\n",
      "Iteration 47, loss = 0.68840578\n",
      "Iteration 48, loss = 0.68825438\n",
      "Iteration 49, loss = 0.68810003\n",
      "Iteration 50, loss = 0.68794260\n",
      "Iteration 51, loss = 0.68778198\n",
      "Iteration 52, loss = 0.68761804\n",
      "Iteration 53, loss = 0.68745064\n",
      "Iteration 54, loss = 0.68727966\n",
      "Iteration 55, loss = 0.68710496\n",
      "Iteration 56, loss = 0.68692638\n",
      "Iteration 57, loss = 0.68674380\n",
      "Iteration 58, loss = 0.68655705\n",
      "Iteration 59, loss = 0.68636597\n",
      "Iteration 60, loss = 0.68617042\n",
      "Iteration 61, loss = 0.68597021\n",
      "Iteration 62, loss = 0.68576519\n",
      "Iteration 63, loss = 0.68555517\n",
      "Iteration 64, loss = 0.68533997\n",
      "Iteration 65, loss = 0.68511940\n",
      "Iteration 66, loss = 0.68489327\n",
      "Iteration 67, loss = 0.68466138\n",
      "Iteration 68, loss = 0.68442353\n",
      "Iteration 69, loss = 0.68417950\n",
      "Iteration 70, loss = 0.68392906\n",
      "Iteration 71, loss = 0.68367200\n",
      "Iteration 72, loss = 0.68340808\n",
      "Iteration 73, loss = 0.68313706\n",
      "Iteration 74, loss = 0.68285869\n",
      "Iteration 75, loss = 0.68257270\n",
      "Iteration 76, loss = 0.68227883\n",
      "Iteration 77, loss = 0.68197681\n",
      "Iteration 78, loss = 0.68166635\n",
      "Iteration 79, loss = 0.68134716\n",
      "Iteration 80, loss = 0.68101892\n",
      "Iteration 81, loss = 0.68068132\n",
      "Iteration 82, loss = 0.68033404\n",
      "Iteration 83, loss = 0.67997673\n",
      "Iteration 84, loss = 0.67960905\n",
      "Iteration 85, loss = 0.67923062\n",
      "Iteration 86, loss = 0.67884108\n",
      "Iteration 87, loss = 0.67844002\n",
      "Iteration 88, loss = 0.67802706\n",
      "Iteration 89, loss = 0.67760176\n",
      "Iteration 90, loss = 0.67716369\n",
      "Iteration 91, loss = 0.67671240\n",
      "Iteration 92, loss = 0.67624742\n",
      "Iteration 93, loss = 0.67576827\n",
      "Iteration 94, loss = 0.67527443\n",
      "Iteration 95, loss = 0.67476540\n",
      "Iteration 96, loss = 0.67424062\n",
      "Iteration 97, loss = 0.67369953\n",
      "Iteration 98, loss = 0.67314155\n",
      "Iteration 99, loss = 0.67256607\n",
      "Iteration 100, loss = 0.67197245\n",
      "Iteration 101, loss = 0.67136004\n",
      "Iteration 102, loss = 0.67072817\n",
      "Iteration 103, loss = 0.67007611\n",
      "Iteration 104, loss = 0.66940315\n",
      "Iteration 105, loss = 0.66870850\n",
      "Iteration 106, loss = 0.66799139\n",
      "Iteration 107, loss = 0.66725097\n",
      "Iteration 108, loss = 0.66651473\n",
      "Iteration 109, loss = 0.66669741\n",
      "Iteration 110, loss = 0.68216438\n",
      "Iteration 111, loss = 0.91589458\n",
      "Iteration 112, loss = 0.72658129\n",
      "Iteration 113, loss = 0.72652700\n",
      "Iteration 114, loss = 0.72192867\n",
      "Iteration 115, loss = 0.71899816\n",
      "Iteration 116, loss = 0.69626781\n",
      "Iteration 117, loss = 0.69543738\n",
      "Iteration 118, loss = 0.69527537\n",
      "Iteration 119, loss = 0.69510611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80875424\n",
      "Iteration 2, loss = 0.71668719\n",
      "Iteration 3, loss = 0.70444559\n",
      "Iteration 4, loss = 0.69469090\n",
      "Iteration 5, loss = 0.69313765\n",
      "Iteration 6, loss = 0.69589866\n",
      "Iteration 7, loss = 0.69653583\n",
      "Iteration 8, loss = 0.69457881\n",
      "Iteration 9, loss = 0.69285511\n",
      "Iteration 10, loss = 0.69253196\n",
      "Iteration 11, loss = 0.69280205\n",
      "Iteration 12, loss = 0.69273766\n",
      "Iteration 13, loss = 0.69250307\n",
      "Iteration 14, loss = 0.69232748\n",
      "Iteration 15, loss = 0.69225513\n",
      "Iteration 16, loss = 0.69219368\n",
      "Iteration 17, loss = 0.69210430\n",
      "Iteration 18, loss = 0.69200411\n",
      "Iteration 19, loss = 0.69184580\n",
      "Iteration 20, loss = 0.69176719\n",
      "Iteration 21, loss = 0.69185904\n",
      "Iteration 22, loss = 0.69163833\n",
      "Iteration 23, loss = 0.69158928\n",
      "Iteration 24, loss = 0.69150702\n",
      "Iteration 25, loss = 0.69142042\n",
      "Iteration 26, loss = 0.69132911\n",
      "Iteration 27, loss = 0.69123287\n",
      "Iteration 28, loss = 0.69113212\n",
      "Iteration 29, loss = 0.69102563\n",
      "Iteration 30, loss = 0.69084033\n",
      "Iteration 31, loss = 0.69076020\n",
      "Iteration 32, loss = 0.69074192\n",
      "Iteration 33, loss = 0.69064603\n",
      "Iteration 34, loss = 0.69054487\n",
      "Iteration 35, loss = 0.69043865\n",
      "Iteration 36, loss = 0.69032762\n",
      "Iteration 37, loss = 0.69021208\n",
      "Iteration 38, loss = 0.69008276\n",
      "Iteration 39, loss = 0.68996776\n",
      "Iteration 40, loss = 0.68985728\n",
      "Iteration 41, loss = 0.68966906\n",
      "Iteration 42, loss = 0.68954521\n",
      "Iteration 43, loss = 0.68949253\n",
      "Iteration 44, loss = 0.68945876\n",
      "Iteration 45, loss = 0.68923540\n",
      "Iteration 46, loss = 0.68918432\n",
      "Iteration 47, loss = 0.68905497\n",
      "Iteration 48, loss = 0.68892664\n",
      "Iteration 49, loss = 0.68879563\n",
      "Iteration 50, loss = 0.68866200\n",
      "Iteration 51, loss = 0.68852605\n",
      "Iteration 52, loss = 0.68838756\n",
      "Iteration 53, loss = 0.68824636\n",
      "Iteration 54, loss = 0.68810237\n",
      "Iteration 55, loss = 0.68795551\n",
      "Iteration 56, loss = 0.68780565\n",
      "Iteration 57, loss = 0.68765269\n",
      "Iteration 58, loss = 0.68749653\n",
      "Iteration 59, loss = 0.68733705\n",
      "Iteration 60, loss = 0.68717415\n",
      "Iteration 61, loss = 0.68700769\n",
      "Iteration 62, loss = 0.68683756\n",
      "Iteration 63, loss = 0.68666363\n",
      "Iteration 64, loss = 0.68648578\n",
      "Iteration 65, loss = 0.68630387\n",
      "Iteration 66, loss = 0.68611776\n",
      "Iteration 67, loss = 0.68592731\n",
      "Iteration 68, loss = 0.68573239\n",
      "Iteration 69, loss = 0.68553283\n",
      "Iteration 70, loss = 0.68532849\n",
      "Iteration 71, loss = 0.68511920\n",
      "Iteration 72, loss = 0.68490482\n",
      "Iteration 73, loss = 0.68468515\n",
      "Iteration 74, loss = 0.68446004\n",
      "Iteration 75, loss = 0.68422931\n",
      "Iteration 76, loss = 0.68399277\n",
      "Iteration 77, loss = 0.68375023\n",
      "Iteration 78, loss = 0.68350149\n",
      "Iteration 79, loss = 0.68324636\n",
      "Iteration 80, loss = 0.68298463\n",
      "Iteration 81, loss = 0.68271607\n",
      "Iteration 82, loss = 0.68244048\n",
      "Iteration 83, loss = 0.68215762\n",
      "Iteration 84, loss = 0.68186725\n",
      "Iteration 85, loss = 0.68156913\n",
      "Iteration 86, loss = 0.68126302\n",
      "Iteration 87, loss = 0.68094864\n",
      "Iteration 88, loss = 0.68062573\n",
      "Iteration 89, loss = 0.68029401\n",
      "Iteration 90, loss = 0.67995320\n",
      "Iteration 91, loss = 0.67960299\n",
      "Iteration 92, loss = 0.67924309\n",
      "Iteration 93, loss = 0.67887316\n",
      "Iteration 94, loss = 0.67849289\n",
      "Iteration 95, loss = 0.67810194\n",
      "Iteration 96, loss = 0.67769994\n",
      "Iteration 97, loss = 0.67728654\n",
      "Iteration 98, loss = 0.67686137\n",
      "Iteration 99, loss = 0.67642402\n",
      "Iteration 100, loss = 0.67597410\n",
      "Iteration 101, loss = 0.67551119\n",
      "Iteration 102, loss = 0.67503486\n",
      "Iteration 103, loss = 0.67454724\n",
      "Iteration 104, loss = 0.67411566\n",
      "Iteration 105, loss = 0.67375345\n",
      "Iteration 106, loss = 0.67376450\n",
      "Iteration 107, loss = 0.67705297\n",
      "Iteration 108, loss = 0.87714444\n",
      "Iteration 109, loss = 0.72950075\n",
      "Iteration 110, loss = 0.72947951\n",
      "Iteration 111, loss = 0.72929358\n",
      "Iteration 112, loss = 0.73017812\n",
      "Iteration 113, loss = 0.70713593\n",
      "Iteration 114, loss = 0.69904233\n",
      "Iteration 115, loss = 0.69625856\n",
      "Iteration 116, loss = 0.69613657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80781766\n",
      "Iteration 2, loss = 0.71449284\n",
      "Iteration 3, loss = 0.70342013\n",
      "Iteration 4, loss = 0.69468974\n",
      "Iteration 5, loss = 0.69298432\n",
      "Iteration 6, loss = 0.69554468\n",
      "Iteration 7, loss = 0.69616727\n",
      "Iteration 8, loss = 0.69435048\n",
      "Iteration 9, loss = 0.69277615\n",
      "Iteration 10, loss = 0.69246410\n",
      "Iteration 11, loss = 0.69264201\n",
      "Iteration 12, loss = 0.69260295\n",
      "Iteration 13, loss = 0.69236700\n",
      "Iteration 14, loss = 0.69218681\n",
      "Iteration 15, loss = 0.69211015\n",
      "Iteration 16, loss = 0.69204856\n",
      "Iteration 17, loss = 0.69196218\n",
      "Iteration 18, loss = 0.69186920\n",
      "Iteration 19, loss = 0.69178276\n",
      "Iteration 20, loss = 0.69169825\n",
      "Iteration 21, loss = 0.69161072\n",
      "Iteration 22, loss = 0.69152063\n",
      "Iteration 23, loss = 0.69142927\n",
      "Iteration 24, loss = 0.69133662\n",
      "Iteration 25, loss = 0.69124236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80310699\n",
      "Iteration 2, loss = 0.70785509\n",
      "Iteration 3, loss = 2.32581716\n",
      "Iteration 4, loss = 2.89887637\n",
      "Iteration 5, loss = 0.71836441\n",
      "Iteration 6, loss = 0.72587731\n",
      "Iteration 7, loss = 0.72566773\n",
      "Iteration 8, loss = 0.72538509\n",
      "Iteration 9, loss = 0.72504179\n",
      "Iteration 10, loss = 0.72464752\n",
      "Iteration 11, loss = 0.72421016\n",
      "Iteration 12, loss = 0.72373621\n",
      "Iteration 13, loss = 0.72323122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80875424\n",
      "Iteration 2, loss = 0.70774279\n",
      "Iteration 3, loss = 2.21171167\n",
      "Iteration 4, loss = 3.16716473\n",
      "Iteration 5, loss = 0.69463444\n",
      "Iteration 6, loss = 0.69298648\n",
      "Iteration 7, loss = 0.70316744\n",
      "Iteration 8, loss = 0.71640953\n",
      "Iteration 9, loss = 0.72179918\n",
      "Iteration 10, loss = 0.72414984\n",
      "Iteration 11, loss = 0.72445322\n",
      "Iteration 12, loss = 0.72355789\n",
      "Iteration 13, loss = 0.72199358\n",
      "Iteration 14, loss = 0.71994727\n",
      "Iteration 15, loss = 0.71730076\n",
      "Iteration 16, loss = 0.71372649\n",
      "Iteration 17, loss = 0.70889403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80781766\n",
      "Iteration 2, loss = 0.70695420\n",
      "Iteration 3, loss = 2.27145842\n",
      "Iteration 4, loss = 3.20524363\n",
      "Iteration 5, loss = 0.69296029\n",
      "Iteration 6, loss = 0.69381423\n",
      "Iteration 7, loss = 0.70357774\n",
      "Iteration 8, loss = 0.71373434\n",
      "Iteration 9, loss = 0.71794201\n",
      "Iteration 10, loss = 0.72002295\n",
      "Iteration 11, loss = 0.72064255\n",
      "Iteration 12, loss = 0.72024819\n",
      "Iteration 13, loss = 0.71903034\n",
      "Iteration 14, loss = 0.71695720\n",
      "Iteration 15, loss = 0.71384067\n",
      "Iteration 16, loss = 0.70946999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70461589\n",
      "Iteration 2, loss = 0.70599232\n",
      "Iteration 3, loss = 1.04379434\n",
      "Iteration 4, loss = 0.85154989\n",
      "Iteration 5, loss = 17.90405701\n",
      "Iteration 6, loss = 0.70119852\n",
      "Iteration 7, loss = 0.70173235\n",
      "Iteration 8, loss = 0.70162731\n",
      "Iteration 9, loss = 0.70099286\n",
      "Iteration 10, loss = 0.69998042\n",
      "Iteration 11, loss = 0.69875601\n",
      "Iteration 12, loss = 0.69747855\n",
      "Iteration 13, loss = 0.69628330\n",
      "Iteration 14, loss = 0.69526646\n",
      "Iteration 15, loss = 0.69448120\n",
      "Iteration 16, loss = 0.69396386\n",
      "Iteration 17, loss = 0.69370631\n",
      "Iteration 18, loss = 0.69366599\n",
      "Iteration 19, loss = 0.69377970\n",
      "Iteration 20, loss = 0.69397766\n",
      "Iteration 21, loss = 0.69419561\n",
      "Iteration 22, loss = 0.69438346\n",
      "Iteration 23, loss = 0.69450971\n",
      "Iteration 24, loss = 0.69456190\n",
      "Iteration 25, loss = 0.69454384\n",
      "Iteration 26, loss = 0.69447085\n",
      "Iteration 27, loss = 0.69436426\n",
      "Iteration 28, loss = 0.69424627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70296955\n",
      "Iteration 2, loss = 0.69516102\n",
      "Iteration 3, loss = 0.82429653\n",
      "Iteration 4, loss = 0.71642902\n",
      "Iteration 5, loss = 9.70100039\n",
      "Iteration 6, loss = 0.70046971\n",
      "Iteration 7, loss = 0.70094585\n",
      "Iteration 8, loss = 0.70064554\n",
      "Iteration 9, loss = 0.69974867\n",
      "Iteration 10, loss = 0.69849291\n",
      "Iteration 11, loss = 0.69711613\n",
      "Iteration 12, loss = 0.69581747\n",
      "Iteration 13, loss = 0.69473634\n",
      "Iteration 14, loss = 0.69394643\n",
      "Iteration 15, loss = 0.69346115\n",
      "Iteration 16, loss = 0.69324673\n",
      "Iteration 17, loss = 0.69322960\n",
      "Iteration 18, loss = 0.69328623\n",
      "Iteration 19, loss = 0.69337258\n",
      "Iteration 20, loss = 0.69346276\n",
      "Iteration 21, loss = 0.69353789\n",
      "Iteration 22, loss = 0.69358673\n",
      "Iteration 23, loss = 0.69360522\n",
      "Iteration 24, loss = 0.69359511\n",
      "Iteration 25, loss = 0.69356219\n",
      "Iteration 26, loss = 0.69351438\n",
      "Iteration 27, loss = 0.69346002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70523959\n",
      "Iteration 2, loss = 0.70456797\n",
      "Iteration 3, loss = 0.98311939\n",
      "Iteration 4, loss = 0.84539117\n",
      "Iteration 5, loss = 18.02184633\n",
      "Iteration 6, loss = 0.70369326\n",
      "Iteration 7, loss = 0.70576534\n",
      "Iteration 8, loss = 0.70663961\n",
      "Iteration 9, loss = 0.70631429\n",
      "Iteration 10, loss = 0.70499157\n",
      "Iteration 11, loss = 0.70299833\n",
      "Iteration 12, loss = 0.70070528\n",
      "Iteration 13, loss = 0.69848775\n",
      "Iteration 14, loss = 0.69654067\n",
      "Iteration 15, loss = 0.69505308\n",
      "Iteration 16, loss = 0.69411164\n",
      "Iteration 17, loss = 0.69369348\n",
      "Iteration 18, loss = 0.69369598\n",
      "Iteration 19, loss = 0.69397376\n",
      "Iteration 20, loss = 0.69437432\n",
      "Iteration 21, loss = 0.69476634\n",
      "Iteration 22, loss = 0.69505734\n",
      "Iteration 23, loss = 0.69520005\n",
      "Iteration 24, loss = 0.69518903\n",
      "Iteration 25, loss = 0.69505069\n",
      "Iteration 26, loss = 0.69483002\n",
      "Iteration 27, loss = 0.69457735\n",
      "Iteration 28, loss = 0.69433751\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70461589\n",
      "Iteration 2, loss = 17.90437004\n",
      "Iteration 3, loss = 17.90478296\n",
      "Iteration 4, loss = 18.14086971\n",
      "Iteration 5, loss = 18.14125847\n",
      "Iteration 6, loss = 5.08834968\n",
      "Iteration 7, loss = 0.73903632\n",
      "Iteration 8, loss = 0.69770209\n",
      "Iteration 9, loss = 0.69873242\n",
      "Iteration 10, loss = 0.69973770\n",
      "Iteration 11, loss = 0.70068427\n",
      "Iteration 12, loss = 0.70155086\n",
      "Iteration 13, loss = 0.70232541\n",
      "Iteration 14, loss = 0.70300277\n",
      "Iteration 15, loss = 0.70358301\n",
      "Iteration 16, loss = 0.70407012\n",
      "Iteration 17, loss = 0.70447094\n",
      "Iteration 18, loss = 0.70479430\n",
      "Iteration 19, loss = 0.70505024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70296955\n",
      "Iteration 2, loss = 18.13994947\n",
      "Iteration 3, loss = 18.14036040\n",
      "Iteration 4, loss = 17.90528543\n",
      "Iteration 5, loss = 17.90567208\n",
      "Iteration 6, loss = 5.15202039\n",
      "Iteration 7, loss = 0.73251151\n",
      "Iteration 8, loss = 0.69720697\n",
      "Iteration 9, loss = 0.69818690\n",
      "Iteration 10, loss = 0.69916405\n",
      "Iteration 11, loss = 0.70010193\n",
      "Iteration 12, loss = 0.70097639\n",
      "Iteration 13, loss = 0.70177243\n",
      "Iteration 14, loss = 0.70248203\n",
      "Iteration 15, loss = 0.70310250\n",
      "Iteration 16, loss = 0.70363523\n",
      "Iteration 17, loss = 0.70408470\n",
      "Iteration 18, loss = 0.70445760\n",
      "Iteration 19, loss = 0.70476215\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70523959\n",
      "Iteration 2, loss = 18.02215760\n",
      "Iteration 3, loss = 18.02256673\n",
      "Iteration 4, loss = 18.02307091\n",
      "Iteration 5, loss = 18.02345657\n",
      "Iteration 6, loss = 5.56535789\n",
      "Iteration 7, loss = 0.71266183\n",
      "Iteration 8, loss = 0.69740037\n",
      "Iteration 9, loss = 0.69834664\n",
      "Iteration 10, loss = 0.69927677\n",
      "Iteration 11, loss = 0.70016015\n",
      "Iteration 12, loss = 0.70097727\n",
      "Iteration 13, loss = 0.70171676\n",
      "Iteration 14, loss = 0.70237337\n",
      "Iteration 15, loss = 0.70294645\n",
      "Iteration 16, loss = 0.70343880\n",
      "Iteration 17, loss = 0.70385573\n",
      "Iteration 18, loss = 0.70420431\n",
      "Iteration 19, loss = 0.70449273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69498415\n",
      "Iteration 2, loss = 2.22821041\n",
      "Iteration 3, loss = 1.15307153\n",
      "Iteration 4, loss = 0.74443467\n",
      "Iteration 5, loss = 0.74039250\n",
      "Iteration 6, loss = 0.73481751\n",
      "Iteration 7, loss = 0.72832687\n",
      "Iteration 8, loss = 0.72151140\n",
      "Iteration 9, loss = 0.71488808\n",
      "Iteration 10, loss = 0.70886710\n",
      "Iteration 11, loss = 0.70373422\n",
      "Iteration 12, loss = 0.69964769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353788\n",
      "Iteration 2, loss = 1.16553600\n",
      "Iteration 3, loss = 11.26150104\n",
      "Iteration 4, loss = 18.13965730\n",
      "Iteration 5, loss = 13.83824719\n",
      "Iteration 6, loss = 0.73727155\n",
      "Iteration 7, loss = 0.73189678\n",
      "Iteration 8, loss = 0.72586046\n",
      "Iteration 9, loss = 0.71969168\n",
      "Iteration 10, loss = 0.71384267\n",
      "Iteration 11, loss = 0.70866103\n",
      "Iteration 12, loss = 0.70437653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69160887\n",
      "Iteration 2, loss = 1.10449783\n",
      "Iteration 3, loss = 2.20006627\n",
      "Iteration 4, loss = 0.73542069\n",
      "Iteration 5, loss = 0.72999623\n",
      "Iteration 6, loss = 0.72388216\n",
      "Iteration 7, loss = 0.71760852\n",
      "Iteration 8, loss = 0.71162709\n",
      "Iteration 9, loss = 0.70628510\n",
      "Iteration 10, loss = 0.70181293\n",
      "Iteration 11, loss = 0.69832499\n",
      "Iteration 12, loss = 0.69583159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69498415\n",
      "Iteration 2, loss = 17.90434353\n",
      "Iteration 3, loss = 17.90444123\n",
      "Iteration 4, loss = 0.71173250\n",
      "Iteration 5, loss = 0.73410962\n",
      "Iteration 6, loss = 0.74936641\n",
      "Iteration 7, loss = 17.90635552\n",
      "Iteration 8, loss = 0.94273855\n",
      "Iteration 9, loss = 0.80132319\n",
      "Iteration 10, loss = 0.80843044\n",
      "Iteration 11, loss = 0.80757625\n",
      "Iteration 12, loss = 0.80072182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69353788\n",
      "Iteration 2, loss = 18.13994775\n",
      "Iteration 3, loss = 18.14009712\n",
      "Iteration 4, loss = 17.90489342\n",
      "Iteration 5, loss = 17.90535111\n",
      "Iteration 6, loss = 1.47912772\n",
      "Iteration 7, loss = 17.90640480\n",
      "Iteration 8, loss = 18.14249482\n",
      "Iteration 9, loss = 18.14303793\n",
      "Iteration 10, loss = 5.45371743\n",
      "Iteration 11, loss = 17.90853951\n",
      "Iteration 12, loss = 17.90904935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69160887\n",
      "Iteration 2, loss = 18.02215576\n",
      "Iteration 3, loss = 18.02227566\n",
      "Iteration 4, loss = 0.70875920\n",
      "Iteration 5, loss = 0.72758234\n",
      "Iteration 6, loss = 0.74052380\n",
      "Iteration 7, loss = 18.02403469\n",
      "Iteration 8, loss = 0.82875548\n",
      "Iteration 9, loss = 0.79295005\n",
      "Iteration 10, loss = 0.80302860\n",
      "Iteration 11, loss = 0.80564775\n",
      "Iteration 12, loss = 0.80235115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80310699\n",
      "Iteration 2, loss = 0.80214470\n",
      "Iteration 3, loss = 0.69670213\n",
      "Iteration 4, loss = 1.12705169\n",
      "Iteration 5, loss = 0.71084624\n",
      "Iteration 6, loss = 0.71813160\n",
      "Iteration 7, loss = 0.71672449\n",
      "Iteration 8, loss = 0.71441547\n",
      "Iteration 9, loss = 0.71150480\n",
      "Iteration 10, loss = 0.70829348\n",
      "Iteration 11, loss = 0.70505502\n",
      "Iteration 12, loss = 0.70201558\n",
      "Iteration 13, loss = 0.69934241\n",
      "Iteration 14, loss = 0.69714013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80875424\n",
      "Iteration 2, loss = 0.80137560\n",
      "Iteration 3, loss = 0.69569480\n",
      "Iteration 4, loss = 0.71624743\n",
      "Iteration 5, loss = 0.71140812\n",
      "Iteration 6, loss = 0.70682957\n",
      "Iteration 7, loss = 0.70277142\n",
      "Iteration 8, loss = 0.69940096\n",
      "Iteration 9, loss = 0.69679574\n",
      "Iteration 10, loss = 0.69495429\n",
      "Iteration 11, loss = 0.69381249\n",
      "Iteration 12, loss = 0.69326253\n",
      "Iteration 13, loss = 0.69317224\n",
      "Iteration 14, loss = 0.69340244\n",
      "Iteration 15, loss = 0.69382106\n",
      "Iteration 16, loss = 0.69431342\n",
      "Iteration 17, loss = 0.69478842\n",
      "Iteration 18, loss = 0.69518107\n",
      "Iteration 19, loss = 0.69545212\n",
      "Iteration 20, loss = 0.69558531\n",
      "Iteration 21, loss = 0.69558327\n",
      "Iteration 22, loss = 0.69546259\n",
      "Iteration 23, loss = 0.69524886\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80781766\n",
      "Iteration 2, loss = 0.79941627\n",
      "Iteration 3, loss = 0.70541104\n",
      "Iteration 4, loss = 0.71587048\n",
      "Iteration 5, loss = 0.71144180\n",
      "Iteration 6, loss = 0.70714454\n",
      "Iteration 7, loss = 0.70324519\n",
      "Iteration 8, loss = 0.69992721\n",
      "Iteration 9, loss = 0.69729039\n",
      "Iteration 10, loss = 0.69535823\n",
      "Iteration 11, loss = 0.69409129\n",
      "Iteration 12, loss = 0.69340386\n",
      "Iteration 13, loss = 0.69318171\n",
      "Iteration 14, loss = 0.69329861\n",
      "Iteration 15, loss = 0.69363046\n",
      "Iteration 16, loss = 0.69406593\n",
      "Iteration 17, loss = 0.69451348\n",
      "Iteration 18, loss = 0.69490491\n",
      "Iteration 19, loss = 0.69519589\n",
      "Iteration 20, loss = 0.69536421\n",
      "Iteration 21, loss = 0.69540644\n",
      "Iteration 22, loss = 0.69533357\n",
      "Iteration 23, loss = 0.69516650\n",
      "Iteration 24, loss = 0.69493159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80310699\n",
      "Iteration 2, loss = 18.14002932\n",
      "Iteration 3, loss = 18.14058211\n",
      "Iteration 4, loss = 0.73106069\n",
      "Iteration 5, loss = 0.73760205\n",
      "Iteration 6, loss = 0.74197614\n",
      "Iteration 7, loss = 0.74553191\n",
      "Iteration 8, loss = 0.74509676\n",
      "Iteration 9, loss = 0.73464567\n",
      "Iteration 10, loss = 17.66647222\n",
      "Iteration 11, loss = 17.90958104\n",
      "Iteration 12, loss = 8.98433447\n",
      "Iteration 13, loss = 0.74029173\n",
      "Iteration 14, loss = 1.23740776\n",
      "Iteration 15, loss = 0.73536726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80875424\n",
      "Iteration 2, loss = 17.90444974\n",
      "Iteration 3, loss = 1.08648698\n",
      "Iteration 4, loss = 0.73372178\n",
      "Iteration 5, loss = 2.70006451\n",
      "Iteration 6, loss = 0.74892171\n",
      "Iteration 7, loss = 0.77255819\n",
      "Iteration 8, loss = 0.76026797\n",
      "Iteration 9, loss = 0.69784584\n",
      "Iteration 10, loss = 17.90913644\n",
      "Iteration 11, loss = 0.87506152\n",
      "Iteration 12, loss = 0.77786748\n",
      "Iteration 13, loss = 0.78321167\n",
      "Iteration 14, loss = 0.78636087\n",
      "Iteration 15, loss = 0.78747331\n",
      "Iteration 16, loss = 0.78676413\n",
      "Iteration 17, loss = 0.78448037\n",
      "Iteration 18, loss = 3.79498640\n",
      "Iteration 19, loss = 0.78174815\n",
      "Iteration 20, loss = 0.78108806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80781766\n",
      "Iteration 2, loss = 18.02223691\n",
      "Iteration 3, loss = 18.02278522\n",
      "Iteration 4, loss = 0.73271400\n",
      "Iteration 5, loss = 0.73928485\n",
      "Iteration 6, loss = 0.74360557\n",
      "Iteration 7, loss = 0.74590257\n",
      "Iteration 8, loss = 2.47554018\n",
      "Iteration 9, loss = 0.72578757\n",
      "Iteration 10, loss = 14.33261388\n",
      "Iteration 11, loss = 18.02734718\n",
      "Iteration 12, loss = 8.28426248\n",
      "Iteration 13, loss = 0.73349479\n",
      "Iteration 14, loss = 1.07588834\n",
      "Iteration 15, loss = 0.72777019\n",
      "Iteration 16, loss = 0.72589845\n",
      "Iteration 17, loss = 0.72391367\n",
      "Iteration 18, loss = 0.72187904\n",
      "Iteration 19, loss = 0.71985122\n",
      "Iteration 20, loss = 0.71787978\n",
      "Iteration 21, loss = 0.71600666\n",
      "Iteration 22, loss = 0.71426606\n",
      "Iteration 23, loss = 0.71268433\n",
      "Iteration 24, loss = 0.71128020\n",
      "Iteration 25, loss = 0.71006508\n",
      "Iteration 26, loss = 0.70904355\n",
      "Iteration 27, loss = 0.70821402\n",
      "Iteration 28, loss = 0.70756945\n",
      "Iteration 29, loss = 0.70709820\n",
      "Iteration 30, loss = 0.70678492\n",
      "Iteration 31, loss = 0.70661150\n",
      "Iteration 32, loss = 0.70655795\n",
      "Iteration 33, loss = 0.70660332\n",
      "Iteration 34, loss = 0.70672652\n",
      "Iteration 35, loss = 0.70690703\n",
      "Iteration 36, loss = 0.70712557\n",
      "Iteration 37, loss = 0.70736460\n",
      "Iteration 38, loss = 0.70760873\n",
      "Iteration 39, loss = 0.70784495\n",
      "Iteration 40, loss = 0.70806279\n",
      "Iteration 41, loss = 0.70825435\n",
      "Iteration 42, loss = 0.70841422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73268849\n",
      "Iteration 2, loss = 0.69401743\n",
      "Iteration 3, loss = 0.69475722\n",
      "Iteration 4, loss = 0.69651030\n",
      "Iteration 5, loss = 0.69539710\n",
      "Iteration 6, loss = 0.69348165\n",
      "Iteration 7, loss = 0.69230265\n",
      "Iteration 8, loss = 0.69150611\n",
      "Iteration 9, loss = 0.69124333\n",
      "Iteration 10, loss = 0.69113237\n",
      "Iteration 11, loss = 0.69102131\n",
      "Iteration 12, loss = 0.69086452\n",
      "Iteration 13, loss = 0.69063215\n",
      "Iteration 14, loss = 0.69040297\n",
      "Iteration 15, loss = 0.69018711\n",
      "Iteration 16, loss = 0.68997857\n",
      "Iteration 17, loss = 0.68983862\n",
      "Iteration 18, loss = 0.68969397\n",
      "Iteration 19, loss = 0.68953072\n",
      "Iteration 20, loss = 0.68931848\n",
      "Iteration 21, loss = 0.68913346\n",
      "Iteration 22, loss = 0.68883917\n",
      "Iteration 23, loss = 0.68855545\n",
      "Iteration 24, loss = 0.68841408\n",
      "Iteration 25, loss = 0.68831890\n",
      "Iteration 26, loss = 0.68815718\n",
      "Iteration 27, loss = 0.68800671\n",
      "Iteration 28, loss = 0.68787848\n",
      "Iteration 29, loss = 0.68776264\n",
      "Iteration 30, loss = 0.68755353\n",
      "Iteration 31, loss = 0.68741013\n",
      "Iteration 32, loss = 0.68728475\n",
      "Iteration 33, loss = 0.68718919\n",
      "Iteration 34, loss = 0.68703036\n",
      "Iteration 35, loss = 0.68689656\n",
      "Iteration 36, loss = 0.68676064\n",
      "Iteration 37, loss = 0.68664925\n",
      "Iteration 38, loss = 0.68646436\n",
      "Iteration 39, loss = 0.68631417\n",
      "Iteration 40, loss = 0.68617755\n",
      "Iteration 41, loss = 0.68604351\n",
      "Iteration 42, loss = 0.68590615\n",
      "Iteration 43, loss = 0.68572758\n",
      "Iteration 44, loss = 0.68557286\n",
      "Iteration 45, loss = 0.68541648\n",
      "Iteration 46, loss = 0.68531485\n",
      "Iteration 47, loss = 0.68518163\n",
      "Iteration 48, loss = 0.68499761\n",
      "Iteration 49, loss = 0.68487022\n",
      "Iteration 50, loss = 0.68477985\n",
      "Iteration 51, loss = 0.68461913\n",
      "Iteration 52, loss = 0.68443786\n",
      "Iteration 53, loss = 0.68428799\n",
      "Iteration 54, loss = 0.68413890\n",
      "Iteration 55, loss = 0.68394672\n",
      "Iteration 56, loss = 0.68360671\n",
      "Iteration 57, loss = 0.68333804\n",
      "Iteration 58, loss = 0.68315709\n",
      "Iteration 59, loss = 0.68305189\n",
      "Iteration 60, loss = 0.68291813\n",
      "Iteration 61, loss = 0.68274268\n",
      "Iteration 62, loss = 0.68258896\n",
      "Iteration 63, loss = 0.68241837\n",
      "Iteration 64, loss = 0.68225433\n",
      "Iteration 65, loss = 0.68209234\n",
      "Iteration 66, loss = 0.68198366\n",
      "Iteration 67, loss = 0.68188703\n",
      "Iteration 68, loss = 0.68161664\n",
      "Iteration 69, loss = 0.68144510\n",
      "Iteration 70, loss = 0.68128618\n",
      "Iteration 71, loss = 0.68113793\n",
      "Iteration 72, loss = 0.68096611\n",
      "Iteration 73, loss = 0.68078938\n",
      "Iteration 74, loss = 0.68066853\n",
      "Iteration 75, loss = 0.68055249\n",
      "Iteration 76, loss = 0.68034617\n",
      "Iteration 77, loss = 0.68019066\n",
      "Iteration 78, loss = 0.68001676\n",
      "Iteration 79, loss = 0.67985592\n",
      "Iteration 80, loss = 0.67968794\n",
      "Iteration 81, loss = 0.67956004\n",
      "Iteration 82, loss = 0.67936993\n",
      "Iteration 83, loss = 0.67920633\n",
      "Iteration 84, loss = 0.67903376\n",
      "Iteration 85, loss = 0.67887387\n",
      "Iteration 86, loss = 0.67875376\n",
      "Iteration 87, loss = 0.67857686\n",
      "Iteration 88, loss = 0.67830065\n",
      "Iteration 89, loss = 0.67808559\n",
      "Iteration 90, loss = 0.67788510\n",
      "Iteration 91, loss = 0.67770608\n",
      "Iteration 92, loss = 0.67751585\n",
      "Iteration 93, loss = 0.67731252\n",
      "Iteration 94, loss = 0.67713729\n",
      "Iteration 95, loss = 0.67693435\n",
      "Iteration 96, loss = 0.67676223\n",
      "Iteration 97, loss = 0.67660140\n",
      "Iteration 98, loss = 0.67643629\n",
      "Iteration 99, loss = 0.67621958\n",
      "Iteration 100, loss = 0.67605790\n",
      "Iteration 101, loss = 0.67590389\n",
      "Iteration 102, loss = 0.67570085\n",
      "Iteration 103, loss = 0.67554898\n",
      "Iteration 104, loss = 0.67538034\n",
      "Iteration 105, loss = 0.67515202\n",
      "Iteration 106, loss = 0.67496689\n",
      "Iteration 107, loss = 0.67494113\n",
      "Iteration 108, loss = 0.67485080\n",
      "Iteration 109, loss = 0.67448731\n",
      "Iteration 110, loss = 0.67432445\n",
      "Iteration 111, loss = 0.67411730\n",
      "Iteration 112, loss = 0.67389142\n",
      "Iteration 113, loss = 0.67371599\n",
      "Iteration 114, loss = 0.67350160\n",
      "Iteration 115, loss = 0.67328170\n",
      "Iteration 116, loss = 0.67308345\n",
      "Iteration 117, loss = 0.67289225\n",
      "Iteration 118, loss = 0.67274512\n",
      "Iteration 119, loss = 0.67255754\n",
      "Iteration 120, loss = 0.67232704\n",
      "Iteration 121, loss = 0.67212426\n",
      "Iteration 122, loss = 0.67191034\n",
      "Iteration 123, loss = 0.67171692\n",
      "Iteration 124, loss = 0.67155715\n",
      "Iteration 125, loss = 0.67133288\n",
      "Iteration 126, loss = 0.67113064\n",
      "Iteration 127, loss = 0.67091977\n",
      "Iteration 128, loss = 0.67071515\n",
      "Iteration 129, loss = 0.67051603\n",
      "Iteration 130, loss = 0.67032464\n",
      "Iteration 131, loss = 0.67014018\n",
      "Iteration 132, loss = 0.66994870\n",
      "Iteration 133, loss = 0.66973087\n",
      "Iteration 134, loss = 0.66951967\n",
      "Iteration 135, loss = 0.66930478\n",
      "Iteration 136, loss = 0.66910806\n",
      "Iteration 137, loss = 0.66893633\n",
      "Iteration 138, loss = 0.66874740\n",
      "Iteration 139, loss = 0.66848973\n",
      "Iteration 140, loss = 0.66828028\n",
      "Iteration 141, loss = 0.66806640\n",
      "Iteration 142, loss = 0.66787386\n",
      "Iteration 143, loss = 0.66770974\n",
      "Iteration 144, loss = 0.66742640\n",
      "Iteration 145, loss = 0.66721321\n",
      "Iteration 146, loss = 0.66699596\n",
      "Iteration 147, loss = 0.66679189\n",
      "Iteration 148, loss = 0.66663665\n",
      "Iteration 149, loss = 0.66636086\n",
      "Iteration 150, loss = 0.66614210\n",
      "Iteration 151, loss = 0.66593881\n",
      "Iteration 152, loss = 0.66571784\n",
      "Iteration 153, loss = 0.66549073\n",
      "Iteration 154, loss = 0.66527534\n",
      "Iteration 155, loss = 0.66507059\n",
      "Iteration 156, loss = 0.66491582\n",
      "Iteration 157, loss = 0.66465239\n",
      "Iteration 158, loss = 0.66441800\n",
      "Iteration 159, loss = 0.66420199\n",
      "Iteration 160, loss = 0.66397345\n",
      "Iteration 161, loss = 0.66373474\n",
      "Iteration 162, loss = 0.66348092\n",
      "Iteration 163, loss = 0.66325663\n",
      "Iteration 164, loss = 0.66302861\n",
      "Iteration 165, loss = 0.66279497\n",
      "Iteration 166, loss = 0.66258706\n",
      "Iteration 167, loss = 0.66238879\n",
      "Iteration 168, loss = 0.66210224\n",
      "Iteration 169, loss = 0.66187053\n",
      "Iteration 170, loss = 0.66163403\n",
      "Iteration 171, loss = 0.66139809\n",
      "Iteration 172, loss = 0.66116391\n",
      "Iteration 173, loss = 0.66092128\n",
      "Iteration 174, loss = 0.66068334\n",
      "Iteration 175, loss = 0.66044453\n",
      "Iteration 176, loss = 0.66020901\n",
      "Iteration 177, loss = 0.65995386\n",
      "Iteration 178, loss = 0.65971529\n",
      "Iteration 179, loss = 0.65947045\n",
      "Iteration 180, loss = 0.65922553\n",
      "Iteration 181, loss = 0.65897389\n",
      "Iteration 182, loss = 0.65873174\n",
      "Iteration 183, loss = 0.65848625\n",
      "Iteration 184, loss = 0.65823387\n",
      "Iteration 185, loss = 0.65797177\n",
      "Iteration 186, loss = 0.65772235\n",
      "Iteration 187, loss = 0.65747343\n",
      "Iteration 188, loss = 0.65721748\n",
      "Iteration 189, loss = 0.65695137\n",
      "Iteration 190, loss = 0.65668785\n",
      "Iteration 191, loss = 0.65642441\n",
      "Iteration 192, loss = 0.65616584\n",
      "Iteration 193, loss = 0.65589734\n",
      "Iteration 194, loss = 0.65563221\n",
      "Iteration 195, loss = 0.65536547\n",
      "Iteration 196, loss = 0.65510727\n",
      "Iteration 197, loss = 0.65484320\n",
      "Iteration 198, loss = 0.65457062\n",
      "Iteration 199, loss = 0.65429565\n",
      "Iteration 200, loss = 0.65402106\n",
      "Iteration 201, loss = 0.65374830\n",
      "Iteration 202, loss = 0.65347544\n",
      "Iteration 203, loss = 0.65320170\n",
      "Iteration 204, loss = 0.65292754\n",
      "Iteration 205, loss = 0.65265567\n",
      "Iteration 206, loss = 0.65236976\n",
      "Iteration 207, loss = 0.65209181\n",
      "Iteration 208, loss = 0.65181291\n",
      "Iteration 209, loss = 0.65152564\n",
      "Iteration 210, loss = 0.65124212\n",
      "Iteration 211, loss = 0.65095094\n",
      "Iteration 212, loss = 0.65066301\n",
      "Iteration 213, loss = 0.65037383\n",
      "Iteration 214, loss = 0.65008693\n",
      "Iteration 215, loss = 0.64979158\n",
      "Iteration 216, loss = 0.64950052\n",
      "Iteration 217, loss = 0.64921136\n",
      "Iteration 218, loss = 0.64891126\n",
      "Iteration 219, loss = 0.64861831\n",
      "Iteration 220, loss = 0.64832211\n",
      "Iteration 221, loss = 0.64801672\n",
      "Iteration 222, loss = 0.64771326\n",
      "Iteration 223, loss = 0.64741070\n",
      "Iteration 224, loss = 0.64710550\n",
      "Iteration 225, loss = 0.64680151\n",
      "Iteration 226, loss = 0.64649694\n",
      "Iteration 227, loss = 0.64618774\n",
      "Iteration 228, loss = 0.64587378\n",
      "Iteration 229, loss = 0.64556520\n",
      "Iteration 230, loss = 0.64525420\n",
      "Iteration 231, loss = 0.64492961\n",
      "Iteration 232, loss = 0.64461750\n",
      "Iteration 233, loss = 0.64429770\n",
      "Iteration 234, loss = 0.64397788\n",
      "Iteration 235, loss = 0.64366118\n",
      "Iteration 236, loss = 0.64333194\n",
      "Iteration 237, loss = 0.64301803\n",
      "Iteration 238, loss = 0.64268409\n",
      "Iteration 239, loss = 0.64235521\n",
      "Iteration 240, loss = 0.64202370\n",
      "Iteration 241, loss = 0.64169427\n",
      "Iteration 242, loss = 0.64135624\n",
      "Iteration 243, loss = 0.64102385\n",
      "Iteration 244, loss = 0.64069023\n",
      "Iteration 245, loss = 0.64035436\n",
      "Iteration 246, loss = 0.64000600\n",
      "Iteration 247, loss = 0.63967451\n",
      "Iteration 248, loss = 0.63932979\n",
      "Iteration 249, loss = 0.63899782\n",
      "Iteration 250, loss = 0.63864387\n",
      "Iteration 251, loss = 0.63828055\n",
      "Iteration 252, loss = 0.63793324\n",
      "Iteration 253, loss = 0.63757548\n",
      "Iteration 254, loss = 0.63723194\n",
      "Iteration 255, loss = 0.63688128\n",
      "Iteration 256, loss = 0.63651183\n",
      "Iteration 257, loss = 0.63613148\n",
      "Iteration 258, loss = 0.63576201\n",
      "Iteration 259, loss = 0.63539497\n",
      "Iteration 260, loss = 0.63502751\n",
      "Iteration 261, loss = 0.63465332\n",
      "Iteration 262, loss = 0.63427733\n",
      "Iteration 263, loss = 0.63390658\n",
      "Iteration 264, loss = 0.63354208\n",
      "Iteration 265, loss = 0.63314908\n",
      "Iteration 266, loss = 0.63276393\n",
      "Iteration 267, loss = 0.63238076\n",
      "Iteration 268, loss = 0.63200269\n",
      "Iteration 269, loss = 0.63161644\n",
      "Iteration 270, loss = 0.63123257\n",
      "Iteration 271, loss = 0.63084964\n",
      "Iteration 272, loss = 0.63045435\n",
      "Iteration 273, loss = 0.63006904\n",
      "Iteration 274, loss = 0.62965958\n",
      "Iteration 275, loss = 0.62925196\n",
      "Iteration 276, loss = 0.62885151\n",
      "Iteration 277, loss = 0.62844364\n",
      "Iteration 278, loss = 0.62803792\n",
      "Iteration 279, loss = 0.62763095\n",
      "Iteration 280, loss = 0.62722018\n",
      "Iteration 281, loss = 0.62680484\n",
      "Iteration 282, loss = 0.62638796\n",
      "Iteration 283, loss = 0.62597299\n",
      "Iteration 284, loss = 0.62555100\n",
      "Iteration 285, loss = 0.62512771\n",
      "Iteration 286, loss = 0.62470369\n",
      "Iteration 287, loss = 0.62427408\n",
      "Iteration 288, loss = 0.62384157\n",
      "Iteration 289, loss = 0.62340548\n",
      "Iteration 290, loss = 0.62297280\n",
      "Iteration 291, loss = 0.62253886\n",
      "Iteration 292, loss = 0.62209620\n",
      "Iteration 293, loss = 0.62166920\n",
      "Iteration 294, loss = 0.62120484\n",
      "Iteration 295, loss = 0.62075078\n",
      "Iteration 296, loss = 0.62029663\n",
      "Iteration 297, loss = 0.61984855\n",
      "Iteration 298, loss = 0.61939251\n",
      "Iteration 299, loss = 0.61892552\n",
      "Iteration 300, loss = 0.61846624\n",
      "Iteration 1, loss = 0.72984577\n",
      "Iteration 2, loss = 0.69452953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.69524148\n",
      "Iteration 4, loss = 0.69685236\n",
      "Iteration 5, loss = 0.69453170\n",
      "Iteration 6, loss = 0.69270724\n",
      "Iteration 7, loss = 0.69195227\n",
      "Iteration 8, loss = 0.69158290\n",
      "Iteration 9, loss = 0.69125368\n",
      "Iteration 10, loss = 0.69100837\n",
      "Iteration 11, loss = 0.69080168\n",
      "Iteration 12, loss = 0.69044602\n",
      "Iteration 13, loss = 0.69013479\n",
      "Iteration 14, loss = 0.68988014\n",
      "Iteration 15, loss = 0.68964946\n",
      "Iteration 16, loss = 0.68943441\n",
      "Iteration 17, loss = 0.68926151\n",
      "Iteration 18, loss = 0.68910068\n",
      "Iteration 19, loss = 0.68896643\n",
      "Iteration 20, loss = 0.68887128\n",
      "Iteration 21, loss = 0.68862594\n",
      "Iteration 22, loss = 0.68838892\n",
      "Iteration 23, loss = 0.68817405\n",
      "Iteration 24, loss = 0.68808072\n",
      "Iteration 25, loss = 0.68801784\n",
      "Iteration 26, loss = 0.68786029\n",
      "Iteration 27, loss = 0.68763927\n",
      "Iteration 28, loss = 0.68749527\n",
      "Iteration 29, loss = 0.68736840\n",
      "Iteration 30, loss = 0.68721515\n",
      "Iteration 31, loss = 0.68706109\n",
      "Iteration 32, loss = 0.68692488\n",
      "Iteration 33, loss = 0.68681750\n",
      "Iteration 34, loss = 0.68665356\n",
      "Iteration 35, loss = 0.68653574\n",
      "Iteration 36, loss = 0.68640636\n",
      "Iteration 37, loss = 0.68626083\n",
      "Iteration 38, loss = 0.68607496\n",
      "Iteration 39, loss = 0.68594073\n",
      "Iteration 40, loss = 0.68577083\n",
      "Iteration 41, loss = 0.68562735\n",
      "Iteration 42, loss = 0.68547359\n",
      "Iteration 43, loss = 0.68532817\n",
      "Iteration 44, loss = 0.68520178\n",
      "Iteration 45, loss = 0.68507252\n",
      "Iteration 46, loss = 0.68491332\n",
      "Iteration 47, loss = 0.68486365\n",
      "Iteration 48, loss = 0.68485749\n",
      "Iteration 49, loss = 0.68449305\n",
      "Iteration 50, loss = 0.68431267\n",
      "Iteration 51, loss = 0.68417512\n",
      "Iteration 52, loss = 0.68414273\n",
      "Iteration 53, loss = 0.68392788\n",
      "Iteration 54, loss = 0.68391519\n",
      "Iteration 55, loss = 0.68362050\n",
      "Iteration 56, loss = 0.68349382\n",
      "Iteration 57, loss = 0.68333912\n",
      "Iteration 58, loss = 0.68331040\n",
      "Iteration 59, loss = 0.68315303\n",
      "Iteration 60, loss = 0.68306579\n",
      "Iteration 61, loss = 0.68279977\n",
      "Iteration 62, loss = 0.68259993\n",
      "Iteration 63, loss = 0.68248014\n",
      "Iteration 64, loss = 0.68248454\n",
      "Iteration 65, loss = 0.68260196\n",
      "Iteration 66, loss = 0.68234075\n",
      "Iteration 67, loss = 0.68186756\n",
      "Iteration 68, loss = 0.68182045\n",
      "Iteration 69, loss = 0.68167511\n",
      "Iteration 70, loss = 0.68162024\n",
      "Iteration 71, loss = 0.68163292\n",
      "Iteration 72, loss = 0.68118705\n",
      "Iteration 73, loss = 0.68113548\n",
      "Iteration 74, loss = 0.68089578\n",
      "Iteration 75, loss = 0.68075562\n",
      "Iteration 76, loss = 0.68054921\n",
      "Iteration 77, loss = 0.68043746\n",
      "Iteration 78, loss = 0.68045084\n",
      "Iteration 79, loss = 0.68032065\n",
      "Iteration 80, loss = 0.68020962\n",
      "Iteration 81, loss = 0.67989650\n",
      "Iteration 82, loss = 0.67967592\n",
      "Iteration 83, loss = 0.67974952\n",
      "Iteration 84, loss = 0.67988140\n",
      "Iteration 85, loss = 0.67943293\n",
      "Iteration 86, loss = 0.67912233\n",
      "Iteration 87, loss = 0.67900233\n",
      "Iteration 88, loss = 0.67877128\n",
      "Iteration 89, loss = 0.67866650\n",
      "Iteration 90, loss = 0.67847305\n",
      "Iteration 91, loss = 0.67841734\n",
      "Iteration 92, loss = 0.67853251\n",
      "Iteration 93, loss = 0.67832491\n",
      "Iteration 94, loss = 0.67800814\n",
      "Iteration 95, loss = 0.67773460\n",
      "Iteration 96, loss = 0.67757766\n",
      "Iteration 97, loss = 0.67769549\n",
      "Iteration 98, loss = 0.67770554\n",
      "Iteration 99, loss = 0.67727126\n",
      "Iteration 100, loss = 0.67696940\n",
      "Iteration 101, loss = 0.67688112\n",
      "Iteration 102, loss = 0.67697195\n",
      "Iteration 103, loss = 0.67700309\n",
      "Iteration 104, loss = 0.67660245\n",
      "Iteration 105, loss = 0.67621045\n",
      "Iteration 106, loss = 0.67622707\n",
      "Iteration 107, loss = 0.67613349\n",
      "Iteration 108, loss = 0.67588385\n",
      "Iteration 109, loss = 0.67575215\n",
      "Iteration 110, loss = 0.67542544\n",
      "Iteration 111, loss = 0.67525652\n",
      "Iteration 112, loss = 0.67507701\n",
      "Iteration 113, loss = 0.67497968\n",
      "Iteration 114, loss = 0.67472630\n",
      "Iteration 115, loss = 0.67453161\n",
      "Iteration 116, loss = 0.67437775\n",
      "Iteration 117, loss = 0.67443141\n",
      "Iteration 118, loss = 0.67411718\n",
      "Iteration 119, loss = 0.67405801\n",
      "Iteration 120, loss = 0.67393302\n",
      "Iteration 121, loss = 0.67403285\n",
      "Iteration 122, loss = 0.67359302\n",
      "Iteration 123, loss = 0.67325100\n",
      "Iteration 124, loss = 0.67330756\n",
      "Iteration 125, loss = 0.67356501\n",
      "Iteration 126, loss = 0.67301722\n",
      "Iteration 127, loss = 0.67270434\n",
      "Iteration 128, loss = 0.67252341\n",
      "Iteration 129, loss = 0.67241593\n",
      "Iteration 130, loss = 0.67222444\n",
      "Iteration 131, loss = 0.67195738\n",
      "Iteration 132, loss = 0.67192288\n",
      "Iteration 133, loss = 0.67206080\n",
      "Iteration 134, loss = 0.67164512\n",
      "Iteration 135, loss = 0.67123646\n",
      "Iteration 136, loss = 0.67124265\n",
      "Iteration 137, loss = 0.67132472\n",
      "Iteration 138, loss = 0.67100389\n",
      "Iteration 139, loss = 0.67075473\n",
      "Iteration 140, loss = 0.67049361\n",
      "Iteration 141, loss = 0.67029404\n",
      "Iteration 142, loss = 0.67017159\n",
      "Iteration 143, loss = 0.67025051\n",
      "Iteration 144, loss = 0.66972622\n",
      "Iteration 145, loss = 0.66951251\n",
      "Iteration 146, loss = 0.66926447\n",
      "Iteration 147, loss = 0.66927168\n",
      "Iteration 148, loss = 0.66957050\n",
      "Iteration 149, loss = 0.66895031\n",
      "Iteration 150, loss = 0.66860840\n",
      "Iteration 151, loss = 0.66840044\n",
      "Iteration 152, loss = 0.66833313\n",
      "Iteration 153, loss = 0.66813918\n",
      "Iteration 154, loss = 0.66805055\n",
      "Iteration 155, loss = 0.66791450\n",
      "Iteration 156, loss = 0.66804579\n",
      "Iteration 157, loss = 0.66758497\n",
      "Iteration 158, loss = 0.66724807\n",
      "Iteration 159, loss = 0.66704698\n",
      "Iteration 160, loss = 0.66692688\n",
      "Iteration 161, loss = 0.66655231\n",
      "Iteration 162, loss = 0.66642396\n",
      "Iteration 163, loss = 0.66631913\n",
      "Iteration 164, loss = 0.66630691\n",
      "Iteration 165, loss = 0.66580017\n",
      "Iteration 166, loss = 0.66567773\n",
      "Iteration 167, loss = 0.66551993\n",
      "Iteration 168, loss = 0.66527029\n",
      "Iteration 169, loss = 0.66506979\n",
      "Iteration 170, loss = 0.66494993\n",
      "Iteration 171, loss = 0.66495851\n",
      "Iteration 172, loss = 0.66447803\n",
      "Iteration 173, loss = 0.66420172\n",
      "Iteration 174, loss = 0.66403853\n",
      "Iteration 175, loss = 0.66383347\n",
      "Iteration 176, loss = 0.66379468\n",
      "Iteration 177, loss = 0.66367478\n",
      "Iteration 178, loss = 0.66382858\n",
      "Iteration 179, loss = 0.66335928\n",
      "Iteration 180, loss = 0.66296186\n",
      "Iteration 181, loss = 0.66280600\n",
      "Iteration 182, loss = 0.66290034\n",
      "Iteration 183, loss = 0.66241750\n",
      "Iteration 184, loss = 0.66204795\n",
      "Iteration 185, loss = 0.66190187\n",
      "Iteration 186, loss = 0.66179053\n",
      "Iteration 187, loss = 0.66187861\n",
      "Iteration 188, loss = 0.66128178\n",
      "Iteration 189, loss = 0.66100747\n",
      "Iteration 190, loss = 0.66086940\n",
      "Iteration 191, loss = 0.66096941\n",
      "Iteration 192, loss = 0.66106343\n",
      "Iteration 193, loss = 0.66060556\n",
      "Iteration 194, loss = 0.66031174\n",
      "Iteration 195, loss = 0.65996013\n",
      "Iteration 196, loss = 0.65953580\n",
      "Iteration 197, loss = 0.65981505\n",
      "Iteration 198, loss = 0.65989834\n",
      "Iteration 199, loss = 0.65941587\n",
      "Iteration 200, loss = 0.65913455\n",
      "Iteration 201, loss = 0.65887838\n",
      "Iteration 202, loss = 0.65859285\n",
      "Iteration 203, loss = 0.65820702\n",
      "Iteration 204, loss = 0.65783860\n",
      "Iteration 205, loss = 0.65780921\n",
      "Iteration 206, loss = 0.65793938\n",
      "Iteration 207, loss = 0.65745650\n",
      "Iteration 208, loss = 0.65711822\n",
      "Iteration 209, loss = 0.65674685\n",
      "Iteration 210, loss = 0.65657590\n",
      "Iteration 211, loss = 0.65643395\n",
      "Iteration 212, loss = 0.65607274\n",
      "Iteration 213, loss = 0.65589506\n",
      "Iteration 214, loss = 0.65572808\n",
      "Iteration 215, loss = 0.65574873\n",
      "Iteration 216, loss = 0.65526504\n",
      "Iteration 217, loss = 0.65492987\n",
      "Iteration 218, loss = 0.65472939\n",
      "Iteration 219, loss = 0.65458884\n",
      "Iteration 220, loss = 0.65473562\n",
      "Iteration 221, loss = 0.65426491\n",
      "Iteration 222, loss = 0.65384574\n",
      "Iteration 223, loss = 0.65361907\n",
      "Iteration 224, loss = 0.65335061\n",
      "Iteration 225, loss = 0.65328574\n",
      "Iteration 226, loss = 0.65282047\n",
      "Iteration 227, loss = 0.65251231\n",
      "Iteration 228, loss = 0.65221577\n",
      "Iteration 229, loss = 0.65201159\n",
      "Iteration 230, loss = 0.65176667\n",
      "Iteration 231, loss = 0.65159193\n",
      "Iteration 232, loss = 0.65130526\n",
      "Iteration 233, loss = 0.65135171\n",
      "Iteration 234, loss = 0.65085335\n",
      "Iteration 235, loss = 0.65047768\n",
      "Iteration 236, loss = 0.65020761\n",
      "Iteration 237, loss = 0.65001039\n",
      "Iteration 238, loss = 0.65007679\n",
      "Iteration 239, loss = 0.64956983\n",
      "Iteration 240, loss = 0.64919751\n",
      "Iteration 241, loss = 0.64894922\n",
      "Iteration 242, loss = 0.64891442\n",
      "Iteration 243, loss = 0.64908719\n",
      "Iteration 244, loss = 0.64851165\n",
      "Iteration 245, loss = 0.64812339\n",
      "Iteration 246, loss = 0.64775462\n",
      "Iteration 247, loss = 0.64741042\n",
      "Iteration 248, loss = 0.64712234\n",
      "Iteration 249, loss = 0.64688319\n",
      "Iteration 250, loss = 0.64668144\n",
      "Iteration 251, loss = 0.64628436\n",
      "Iteration 252, loss = 0.64603276\n",
      "Iteration 253, loss = 0.64582268\n",
      "Iteration 254, loss = 0.64574065\n",
      "Iteration 255, loss = 0.64527874\n",
      "Iteration 256, loss = 0.64492000\n",
      "Iteration 257, loss = 0.64461604\n",
      "Iteration 258, loss = 0.64433941\n",
      "Iteration 259, loss = 0.64404969\n",
      "Iteration 260, loss = 0.64395869\n",
      "Iteration 261, loss = 0.64356479\n",
      "Iteration 262, loss = 0.64337391\n",
      "Iteration 263, loss = 0.64304405\n",
      "Iteration 264, loss = 0.64268345\n",
      "Iteration 265, loss = 0.64239326\n",
      "Iteration 266, loss = 0.64227710\n",
      "Iteration 267, loss = 0.64186087\n",
      "Iteration 268, loss = 0.64142557\n",
      "Iteration 269, loss = 0.64121114\n",
      "Iteration 270, loss = 0.64128658\n",
      "Iteration 271, loss = 0.64068669\n",
      "Iteration 272, loss = 0.64029377\n",
      "Iteration 273, loss = 0.63989809\n",
      "Iteration 274, loss = 0.63959506\n",
      "Iteration 275, loss = 0.63940797\n",
      "Iteration 276, loss = 0.63903583\n",
      "Iteration 277, loss = 0.63876397\n",
      "Iteration 278, loss = 0.63839720\n",
      "Iteration 279, loss = 0.63807446\n",
      "Iteration 280, loss = 0.63774328\n",
      "Iteration 281, loss = 0.63761557\n",
      "Iteration 282, loss = 0.63723621\n",
      "Iteration 283, loss = 0.63694554\n",
      "Iteration 284, loss = 0.63652153\n",
      "Iteration 285, loss = 0.63615705\n",
      "Iteration 286, loss = 0.63585447\n",
      "Iteration 287, loss = 0.63554342\n",
      "Iteration 288, loss = 0.63521807\n",
      "Iteration 289, loss = 0.63496020\n",
      "Iteration 290, loss = 0.63457882\n",
      "Iteration 291, loss = 0.63437283\n",
      "Iteration 292, loss = 0.63385869\n",
      "Iteration 293, loss = 0.63345809\n",
      "Iteration 294, loss = 0.63312957\n",
      "Iteration 295, loss = 0.63282326\n",
      "Iteration 296, loss = 0.63245459\n",
      "Iteration 297, loss = 0.63215476\n",
      "Iteration 298, loss = 0.63194147\n",
      "Iteration 299, loss = 0.63132065\n",
      "Iteration 300, loss = 0.62939437\n",
      "Iteration 1, loss = 0.72996375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69459892\n",
      "Iteration 3, loss = 0.69529106\n",
      "Iteration 4, loss = 0.69701065\n",
      "Iteration 5, loss = 0.69580315\n",
      "Iteration 6, loss = 0.69309913\n",
      "Iteration 7, loss = 0.69160998\n",
      "Iteration 8, loss = 0.69071804\n",
      "Iteration 9, loss = 0.69020236\n",
      "Iteration 10, loss = 0.68989398\n",
      "Iteration 11, loss = 0.68944842\n",
      "Iteration 12, loss = 0.68922827\n",
      "Iteration 13, loss = 0.68903240\n",
      "Iteration 14, loss = 0.68890383\n",
      "Iteration 15, loss = 0.68877617\n",
      "Iteration 16, loss = 0.68867464\n",
      "Iteration 17, loss = 0.68842681\n",
      "Iteration 18, loss = 0.68822755\n",
      "Iteration 19, loss = 0.68781977\n",
      "Iteration 20, loss = 0.68756032\n",
      "Iteration 21, loss = 0.68728521\n",
      "Iteration 22, loss = 0.68711108\n",
      "Iteration 23, loss = 0.68701225\n",
      "Iteration 24, loss = 0.68666730\n",
      "Iteration 25, loss = 0.68650509\n",
      "Iteration 26, loss = 0.68660652\n",
      "Iteration 27, loss = 0.68651218\n",
      "Iteration 28, loss = 0.68600613\n",
      "Iteration 29, loss = 0.68576834\n",
      "Iteration 30, loss = 0.68547452\n",
      "Iteration 31, loss = 0.68528934\n",
      "Iteration 32, loss = 0.68513377\n",
      "Iteration 33, loss = 0.68496699\n",
      "Iteration 34, loss = 0.68482366\n",
      "Iteration 35, loss = 0.68462381\n",
      "Iteration 36, loss = 0.68444981\n",
      "Iteration 37, loss = 0.68428914\n",
      "Iteration 38, loss = 0.68411532\n",
      "Iteration 39, loss = 0.68402823\n",
      "Iteration 40, loss = 0.68377161\n",
      "Iteration 41, loss = 0.68362472\n",
      "Iteration 42, loss = 0.68343337\n",
      "Iteration 43, loss = 0.68337167\n",
      "Iteration 44, loss = 0.68307865\n",
      "Iteration 45, loss = 0.68287543\n",
      "Iteration 46, loss = 0.68267748\n",
      "Iteration 47, loss = 0.68249246\n",
      "Iteration 48, loss = 0.68231635\n",
      "Iteration 49, loss = 0.68212898\n",
      "Iteration 50, loss = 0.68199752\n",
      "Iteration 51, loss = 0.68196286\n",
      "Iteration 52, loss = 0.68187741\n",
      "Iteration 53, loss = 0.68149302\n",
      "Iteration 54, loss = 0.68130739\n",
      "Iteration 55, loss = 0.68110691\n",
      "Iteration 56, loss = 0.68092560\n",
      "Iteration 57, loss = 0.68074036\n",
      "Iteration 58, loss = 0.68052678\n",
      "Iteration 59, loss = 0.68032325\n",
      "Iteration 60, loss = 0.68014247\n",
      "Iteration 61, loss = 0.67994444\n",
      "Iteration 62, loss = 0.67972910\n",
      "Iteration 63, loss = 0.67950268\n",
      "Iteration 64, loss = 0.67930142\n",
      "Iteration 65, loss = 0.67911241\n",
      "Iteration 66, loss = 0.67888404\n",
      "Iteration 67, loss = 0.67868771\n",
      "Iteration 68, loss = 0.67849629\n",
      "Iteration 69, loss = 0.67832356\n",
      "Iteration 70, loss = 0.67810112\n",
      "Iteration 71, loss = 0.67789489\n",
      "Iteration 72, loss = 0.67769826\n",
      "Iteration 73, loss = 0.67748820\n",
      "Iteration 74, loss = 0.67729833\n",
      "Iteration 75, loss = 0.67711422\n",
      "Iteration 76, loss = 0.67693004\n",
      "Iteration 77, loss = 0.67672172\n",
      "Iteration 78, loss = 0.67652433\n",
      "Iteration 79, loss = 0.67636791\n",
      "Iteration 80, loss = 0.67612610\n",
      "Iteration 81, loss = 0.67592024\n",
      "Iteration 82, loss = 0.67572429\n",
      "Iteration 83, loss = 0.67552792\n",
      "Iteration 84, loss = 0.67532392\n",
      "Iteration 85, loss = 0.67513255\n",
      "Iteration 86, loss = 0.67493847\n",
      "Iteration 87, loss = 0.67473777\n",
      "Iteration 88, loss = 0.67453776\n",
      "Iteration 89, loss = 0.67431921\n",
      "Iteration 90, loss = 0.67411037\n",
      "Iteration 91, loss = 0.67391092\n",
      "Iteration 92, loss = 0.67369641\n",
      "Iteration 93, loss = 0.67350515\n",
      "Iteration 94, loss = 0.67329741\n",
      "Iteration 95, loss = 0.67308172\n",
      "Iteration 96, loss = 0.67289687\n",
      "Iteration 97, loss = 0.67268460\n",
      "Iteration 98, loss = 0.67246042\n",
      "Iteration 99, loss = 0.67226234\n",
      "Iteration 100, loss = 0.67203665\n",
      "Iteration 101, loss = 0.67182883\n",
      "Iteration 102, loss = 0.67163638\n",
      "Iteration 103, loss = 0.67141404\n",
      "Iteration 104, loss = 0.67122983\n",
      "Iteration 105, loss = 0.67101095\n",
      "Iteration 106, loss = 0.67079458\n",
      "Iteration 107, loss = 0.67057784\n",
      "Iteration 108, loss = 0.67035068\n",
      "Iteration 109, loss = 0.67013791\n",
      "Iteration 110, loss = 0.66993288\n",
      "Iteration 111, loss = 0.66972828\n",
      "Iteration 112, loss = 0.66949584\n",
      "Iteration 113, loss = 0.66926238\n",
      "Iteration 114, loss = 0.66907172\n",
      "Iteration 115, loss = 0.66884717\n",
      "Iteration 116, loss = 0.66862438\n",
      "Iteration 117, loss = 0.66839979\n",
      "Iteration 118, loss = 0.66818229\n",
      "Iteration 119, loss = 0.66794548\n",
      "Iteration 120, loss = 0.66772668\n",
      "Iteration 121, loss = 0.66749121\n",
      "Iteration 122, loss = 0.66726956\n",
      "Iteration 123, loss = 0.66703930\n",
      "Iteration 124, loss = 0.66681969\n",
      "Iteration 125, loss = 0.66660907\n",
      "Iteration 126, loss = 0.66637575\n",
      "Iteration 127, loss = 0.66616124\n",
      "Iteration 128, loss = 0.66590881\n",
      "Iteration 129, loss = 0.66567704\n",
      "Iteration 130, loss = 0.66545117\n",
      "Iteration 131, loss = 0.66524079\n",
      "Iteration 132, loss = 0.66498126\n",
      "Iteration 133, loss = 0.66477630\n",
      "Iteration 134, loss = 0.66452549\n",
      "Iteration 135, loss = 0.66428770\n",
      "Iteration 136, loss = 0.66406751\n",
      "Iteration 137, loss = 0.66382156\n",
      "Iteration 138, loss = 0.66358487\n",
      "Iteration 139, loss = 0.66334598\n",
      "Iteration 140, loss = 0.66311142\n",
      "Iteration 141, loss = 0.66286334\n",
      "Iteration 142, loss = 0.66265206\n",
      "Iteration 143, loss = 0.66242774\n",
      "Iteration 144, loss = 0.66225328\n",
      "Iteration 145, loss = 0.66193010\n",
      "Iteration 146, loss = 0.66169673\n",
      "Iteration 147, loss = 0.66141988\n",
      "Iteration 148, loss = 0.66116684\n",
      "Iteration 149, loss = 0.66093268\n",
      "Iteration 150, loss = 0.66070910\n",
      "Iteration 151, loss = 0.66044874\n",
      "Iteration 152, loss = 0.66020847\n",
      "Iteration 153, loss = 0.65998608\n",
      "Iteration 154, loss = 0.65975928\n",
      "Iteration 155, loss = 0.65946185\n",
      "Iteration 156, loss = 0.65918689\n",
      "Iteration 157, loss = 0.65892917\n",
      "Iteration 158, loss = 0.65867643\n",
      "Iteration 159, loss = 0.65841481\n",
      "Iteration 160, loss = 0.65815288\n",
      "Iteration 161, loss = 0.65791609\n",
      "Iteration 162, loss = 0.65766627\n",
      "Iteration 163, loss = 0.65746075\n",
      "Iteration 164, loss = 0.65712809\n",
      "Iteration 165, loss = 0.65685259\n",
      "Iteration 166, loss = 0.65659299\n",
      "Iteration 167, loss = 0.65633911\n",
      "Iteration 168, loss = 0.65607593\n",
      "Iteration 169, loss = 0.65581278\n",
      "Iteration 170, loss = 0.65557652\n",
      "Iteration 171, loss = 0.65535575\n",
      "Iteration 172, loss = 0.65502521\n",
      "Iteration 173, loss = 0.65473757\n",
      "Iteration 174, loss = 0.65447700\n",
      "Iteration 175, loss = 0.65419265\n",
      "Iteration 176, loss = 0.65393526\n",
      "Iteration 177, loss = 0.65366159\n",
      "Iteration 178, loss = 0.65343679\n",
      "Iteration 179, loss = 0.65310393\n",
      "Iteration 180, loss = 0.65283247\n",
      "Iteration 181, loss = 0.65257783\n",
      "Iteration 182, loss = 0.65227752\n",
      "Iteration 183, loss = 0.65202425\n",
      "Iteration 184, loss = 0.65172367\n",
      "Iteration 185, loss = 0.65142969\n",
      "Iteration 186, loss = 0.65113364\n",
      "Iteration 187, loss = 0.65087213\n",
      "Iteration 188, loss = 0.65057547\n",
      "Iteration 189, loss = 0.65026774\n",
      "Iteration 190, loss = 0.64997676\n",
      "Iteration 191, loss = 0.64969838\n",
      "Iteration 192, loss = 0.64940624\n",
      "Iteration 193, loss = 0.64913039\n",
      "Iteration 194, loss = 0.64885641\n",
      "Iteration 195, loss = 0.64854553\n",
      "Iteration 196, loss = 0.64823451\n",
      "Iteration 197, loss = 0.64794336\n",
      "Iteration 198, loss = 0.64763603\n",
      "Iteration 199, loss = 0.64732930\n",
      "Iteration 200, loss = 0.64702565\n",
      "Iteration 201, loss = 0.64674514\n",
      "Iteration 202, loss = 0.64642869\n",
      "Iteration 203, loss = 0.64609825\n",
      "Iteration 204, loss = 0.64576227\n",
      "Iteration 205, loss = 0.64538774\n",
      "Iteration 206, loss = 0.64503999\n",
      "Iteration 207, loss = 0.64472356\n",
      "Iteration 208, loss = 0.64441375\n",
      "Iteration 209, loss = 0.64406880\n",
      "Iteration 210, loss = 0.64375796\n",
      "Iteration 211, loss = 0.64344498\n",
      "Iteration 212, loss = 0.64312031\n",
      "Iteration 213, loss = 0.64277148\n",
      "Iteration 214, loss = 0.64245567\n",
      "Iteration 215, loss = 0.64212255\n",
      "Iteration 216, loss = 0.64179505\n",
      "Iteration 217, loss = 0.64146990\n",
      "Iteration 218, loss = 0.64113380\n",
      "Iteration 219, loss = 0.64080392\n",
      "Iteration 220, loss = 0.64047116\n",
      "Iteration 221, loss = 0.64012693\n",
      "Iteration 222, loss = 0.63976668\n",
      "Iteration 223, loss = 0.63942783\n",
      "Iteration 224, loss = 0.63907383\n",
      "Iteration 225, loss = 0.63874243\n",
      "Iteration 226, loss = 0.63838995\n",
      "Iteration 227, loss = 0.63802290\n",
      "Iteration 228, loss = 0.63767378\n",
      "Iteration 229, loss = 0.63733445\n",
      "Iteration 230, loss = 0.63699380\n",
      "Iteration 231, loss = 0.63661918\n",
      "Iteration 232, loss = 0.63626729\n",
      "Iteration 233, loss = 0.63591392\n",
      "Iteration 234, loss = 0.63552718\n",
      "Iteration 235, loss = 0.63515173\n",
      "Iteration 236, loss = 0.63479954\n",
      "Iteration 237, loss = 0.63441110\n",
      "Iteration 238, loss = 0.63405037\n",
      "Iteration 239, loss = 0.63366097\n",
      "Iteration 240, loss = 0.63328503\n",
      "Iteration 241, loss = 0.63291234\n",
      "Iteration 242, loss = 0.63254187\n",
      "Iteration 243, loss = 0.63215278\n",
      "Iteration 244, loss = 0.63176699\n",
      "Iteration 245, loss = 0.63137390\n",
      "Iteration 246, loss = 0.63097126\n",
      "Iteration 247, loss = 0.63057039\n",
      "Iteration 248, loss = 0.63017364\n",
      "Iteration 249, loss = 0.62978263\n",
      "Iteration 250, loss = 0.62938088\n",
      "Iteration 251, loss = 0.62895880\n",
      "Iteration 252, loss = 0.62855163\n",
      "Iteration 253, loss = 0.62811768\n",
      "Iteration 254, loss = 0.62755466\n",
      "Iteration 255, loss = 0.62666601\n",
      "Iteration 256, loss = 0.62496164\n",
      "Iteration 257, loss = 0.62388515\n",
      "Iteration 258, loss = 0.62293130\n",
      "Iteration 259, loss = 0.62227505\n",
      "Iteration 260, loss = 0.62178681\n",
      "Iteration 261, loss = 0.62128838\n",
      "Iteration 262, loss = 0.62083403\n",
      "Iteration 263, loss = 0.62037407\n",
      "Iteration 264, loss = 0.61985789\n",
      "Iteration 265, loss = 0.61933402\n",
      "Iteration 266, loss = 0.61884148\n",
      "Iteration 267, loss = 0.61835680\n",
      "Iteration 268, loss = 0.61787818\n",
      "Iteration 269, loss = 0.61739552\n",
      "Iteration 270, loss = 0.61691122\n",
      "Iteration 271, loss = 0.61643053\n",
      "Iteration 272, loss = 0.61593682\n",
      "Iteration 273, loss = 0.61544329\n",
      "Iteration 274, loss = 0.61493348\n",
      "Iteration 275, loss = 0.61442865\n",
      "Iteration 276, loss = 0.61392575\n",
      "Iteration 277, loss = 0.61342758\n",
      "Iteration 278, loss = 0.61292170\n",
      "Iteration 279, loss = 0.61240921\n",
      "Iteration 280, loss = 0.61189978\n",
      "Iteration 281, loss = 0.61137581\n",
      "Iteration 282, loss = 0.61085117\n",
      "Iteration 283, loss = 0.61032531\n",
      "Iteration 284, loss = 0.60980528\n",
      "Iteration 285, loss = 0.60928814\n",
      "Iteration 286, loss = 0.60875614\n",
      "Iteration 287, loss = 0.60823569\n",
      "Iteration 288, loss = 0.60771279\n",
      "Iteration 289, loss = 0.60716322\n",
      "Iteration 290, loss = 0.60664339\n",
      "Iteration 291, loss = 0.60611503\n",
      "Iteration 292, loss = 0.60559555\n",
      "Iteration 293, loss = 0.60508939\n",
      "Iteration 294, loss = 0.60455026\n",
      "Iteration 295, loss = 0.60399803\n",
      "Iteration 296, loss = 0.60350840\n",
      "Iteration 297, loss = 0.60296817\n",
      "Iteration 298, loss = 0.60246413\n",
      "Iteration 299, loss = 0.60190888\n",
      "Iteration 300, loss = 0.60140957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73268849\n",
      "Iteration 2, loss = 2.01061601\n",
      "Iteration 3, loss = 0.72514534\n",
      "Iteration 4, loss = 1.33590023\n",
      "Iteration 5, loss = 0.94224109\n",
      "Iteration 6, loss = 0.71902372\n",
      "Iteration 7, loss = 0.69800015\n",
      "Iteration 8, loss = 0.71979614\n",
      "Iteration 9, loss = 0.70610618\n",
      "Iteration 10, loss = 0.69266260\n",
      "Iteration 11, loss = 0.70890194\n",
      "Iteration 12, loss = 0.70467351\n",
      "Iteration 13, loss = 0.69259385\n",
      "Iteration 14, loss = 0.69823047\n",
      "Iteration 15, loss = 0.70490265\n",
      "Iteration 16, loss = 0.69650137\n",
      "Iteration 17, loss = 0.69257866\n",
      "Iteration 18, loss = 0.69939054\n",
      "Iteration 19, loss = 0.69886056\n",
      "Iteration 20, loss = 0.69200178\n",
      "Iteration 21, loss = 0.69447027\n",
      "Iteration 22, loss = 0.69840011\n",
      "Iteration 23, loss = 0.69368646\n",
      "Iteration 24, loss = 0.69135983\n",
      "Iteration 25, loss = 0.69534480\n",
      "Iteration 26, loss = 0.69443341\n",
      "Iteration 27, loss = 0.69068503\n",
      "Iteration 28, loss = 0.69250881\n",
      "Iteration 29, loss = 0.69381739\n",
      "Iteration 30, loss = 0.69076592\n",
      "Iteration 31, loss = 0.69044694\n",
      "Iteration 32, loss = 0.69148318\n",
      "Iteration 33, loss = 0.69050488\n",
      "Iteration 34, loss = 0.68979418\n",
      "Iteration 35, loss = 0.69058087\n",
      "Iteration 36, loss = 0.68982807\n",
      "Iteration 37, loss = 0.68932720\n",
      "Iteration 38, loss = 0.68985847\n",
      "Iteration 39, loss = 0.68922372\n",
      "Iteration 40, loss = 0.68882018\n",
      "Iteration 41, loss = 0.68916211\n",
      "Iteration 42, loss = 0.68858922\n",
      "Iteration 43, loss = 0.68831286\n",
      "Iteration 44, loss = 0.68849200\n",
      "Iteration 45, loss = 0.68795559\n",
      "Iteration 46, loss = 0.68778496\n",
      "Iteration 47, loss = 0.68781252\n",
      "Iteration 48, loss = 0.68731946\n",
      "Iteration 49, loss = 0.68723264\n",
      "Iteration 50, loss = 0.68711386\n",
      "Iteration 51, loss = 0.68668921\n",
      "Iteration 52, loss = 0.68663752\n",
      "Iteration 53, loss = 0.68639068\n",
      "Iteration 54, loss = 0.68606166\n",
      "Iteration 55, loss = 0.68598384\n",
      "Iteration 56, loss = 0.68565376\n",
      "Iteration 57, loss = 0.68542191\n",
      "Iteration 58, loss = 0.68526103\n",
      "Iteration 59, loss = 0.68491619\n",
      "Iteration 60, loss = 0.68474072\n",
      "Iteration 61, loss = 0.68447844\n",
      "Iteration 62, loss = 0.68417968\n",
      "Iteration 63, loss = 0.68398675\n",
      "Iteration 64, loss = 0.68366262\n",
      "Iteration 65, loss = 0.68341857\n",
      "Iteration 66, loss = 0.68315181\n",
      "Iteration 67, loss = 0.68283448\n",
      "Iteration 68, loss = 0.68259027\n",
      "Iteration 69, loss = 0.68226138\n",
      "Iteration 70, loss = 0.68197797\n",
      "Iteration 71, loss = 0.68167475\n",
      "Iteration 72, loss = 0.68134291\n",
      "Iteration 73, loss = 0.68104827\n",
      "Iteration 74, loss = 0.69134978\n",
      "Iteration 75, loss = 0.69937613\n",
      "Iteration 76, loss = 0.68414903\n",
      "Iteration 77, loss = 0.69307918\n",
      "Iteration 78, loss = 0.68459655\n",
      "Iteration 79, loss = 0.68568016\n",
      "Iteration 80, loss = 0.68648663\n",
      "Iteration 81, loss = 0.67970347\n",
      "Iteration 82, loss = 0.68680360\n",
      "Iteration 83, loss = 0.67738861\n",
      "Iteration 84, loss = 0.68391910\n",
      "Iteration 85, loss = 0.67836556\n",
      "Iteration 86, loss = 0.67925628\n",
      "Iteration 87, loss = 0.67987687\n",
      "Iteration 88, loss = 0.67577506\n",
      "Iteration 89, loss = 0.67943270\n",
      "Iteration 90, loss = 0.67486995\n",
      "Iteration 91, loss = 0.67694783\n",
      "Iteration 92, loss = 0.67535139\n",
      "Iteration 93, loss = 0.67415302\n",
      "Iteration 94, loss = 0.67533885\n",
      "Iteration 95, loss = 0.67246992\n",
      "Iteration 96, loss = 0.67410930\n",
      "Iteration 97, loss = 0.67186350\n",
      "Iteration 98, loss = 0.67218781\n",
      "Iteration 99, loss = 0.67149616\n",
      "Iteration 100, loss = 0.67036620\n",
      "Iteration 101, loss = 0.67074030\n",
      "Iteration 102, loss = 0.66897459\n",
      "Iteration 103, loss = 0.66951195\n",
      "Iteration 104, loss = 0.66788141\n",
      "Iteration 105, loss = 0.66801567\n",
      "Iteration 106, loss = 0.66683060\n",
      "Iteration 107, loss = 0.66645282\n",
      "Iteration 108, loss = 0.66560914\n",
      "Iteration 109, loss = 0.66482831\n",
      "Iteration 110, loss = 0.66417706\n",
      "Iteration 111, loss = 0.66323016\n",
      "Iteration 112, loss = 0.66257712\n",
      "Iteration 113, loss = 0.66156381\n",
      "Iteration 114, loss = 0.66078302\n",
      "Iteration 115, loss = 0.65981406\n",
      "Iteration 116, loss = 0.65881510\n",
      "Iteration 117, loss = 0.65794050\n",
      "Iteration 118, loss = 0.65667615\n",
      "Iteration 119, loss = 0.65588266\n",
      "Iteration 120, loss = 0.65444694\n",
      "Iteration 121, loss = 0.65349169\n",
      "Iteration 122, loss = 0.65219782\n",
      "Iteration 123, loss = 0.65078742\n",
      "Iteration 124, loss = 0.64967452\n",
      "Iteration 125, loss = 0.64814151\n",
      "Iteration 126, loss = 0.64659555\n",
      "Iteration 127, loss = 0.64526009\n",
      "Iteration 128, loss = 0.64368536\n",
      "Iteration 129, loss = 0.64186642\n",
      "Iteration 130, loss = 0.64009127\n",
      "Iteration 131, loss = 0.63840189\n",
      "Iteration 132, loss = 0.63671425\n",
      "Iteration 133, loss = 0.63505660\n",
      "Iteration 134, loss = 0.63364852\n",
      "Iteration 135, loss = 0.63347950\n",
      "Iteration 136, loss = 0.64865514\n",
      "Iteration 137, loss = 0.66858993\n",
      "Iteration 138, loss = 0.64642888\n",
      "Iteration 139, loss = 0.62346242\n",
      "Iteration 140, loss = 0.64374192\n",
      "Iteration 141, loss = 0.64337976\n",
      "Iteration 142, loss = 0.61948692\n",
      "Iteration 143, loss = 0.63033193\n",
      "Iteration 144, loss = 0.63527220\n",
      "Iteration 145, loss = 0.61587749\n",
      "Iteration 146, loss = 0.62098230\n",
      "Iteration 147, loss = 0.62744808\n",
      "Iteration 148, loss = 0.61241640\n",
      "Iteration 149, loss = 0.61246641\n",
      "Iteration 150, loss = 0.61991163\n",
      "Iteration 151, loss = 0.60985141\n",
      "Iteration 152, loss = 0.60441564\n",
      "Iteration 153, loss = 0.61103187\n",
      "Iteration 154, loss = 0.60814189\n",
      "Iteration 155, loss = 0.59898748\n",
      "Iteration 156, loss = 0.59962500\n",
      "Iteration 157, loss = 0.60331986\n",
      "Iteration 158, loss = 0.59876870\n",
      "Iteration 159, loss = 0.59166945\n",
      "Iteration 160, loss = 0.59066757\n",
      "Iteration 161, loss = 0.59327635\n",
      "Iteration 162, loss = 0.59294879\n",
      "Iteration 163, loss = 0.58811838\n",
      "Iteration 164, loss = 0.58243120\n",
      "Iteration 165, loss = 0.57911238\n",
      "Iteration 166, loss = 0.57845533\n",
      "Iteration 167, loss = 0.57962569\n",
      "Iteration 168, loss = 0.58243111\n",
      "Iteration 169, loss = 0.58733543\n",
      "Iteration 170, loss = 0.59354969\n",
      "Iteration 171, loss = 0.59627564\n",
      "Iteration 172, loss = 0.58861402\n",
      "Iteration 173, loss = 0.57295441\n",
      "Iteration 174, loss = 0.56101193\n",
      "Iteration 175, loss = 0.55921716\n",
      "Iteration 176, loss = 0.56545154\n",
      "Iteration 177, loss = 0.57486021\n",
      "Iteration 178, loss = 0.58070232\n",
      "Iteration 179, loss = 0.57554569\n",
      "Iteration 180, loss = 0.56071673\n",
      "Iteration 181, loss = 0.54760114\n",
      "Iteration 182, loss = 0.54385345\n",
      "Iteration 183, loss = 0.54850234\n",
      "Iteration 184, loss = 0.55746276\n",
      "Iteration 185, loss = 0.54952992\n",
      "Iteration 186, loss = 0.54917189\n",
      "Iteration 187, loss = 0.57570639\n",
      "Iteration 188, loss = 0.64327774\n",
      "Iteration 189, loss = 0.66189689\n",
      "Iteration 190, loss = 0.56631891\n",
      "Iteration 191, loss = 0.53066852\n",
      "Iteration 192, loss = 0.59802900\n",
      "Iteration 193, loss = 0.58401717\n",
      "Iteration 194, loss = 0.52064005\n",
      "Iteration 195, loss = 0.54284724\n",
      "Iteration 196, loss = 0.58136428\n",
      "Iteration 197, loss = 0.54546158\n",
      "Iteration 198, loss = 0.51023757\n",
      "Iteration 199, loss = 0.52361582\n",
      "Iteration 200, loss = 0.54831181\n",
      "Iteration 201, loss = 0.52532140\n",
      "Iteration 202, loss = 0.50291247\n",
      "Iteration 203, loss = 0.51153682\n",
      "Iteration 204, loss = 0.52213490\n",
      "Iteration 205, loss = 0.54439501\n",
      "Iteration 206, loss = 0.53900325\n",
      "Iteration 207, loss = 0.50843622\n",
      "Iteration 208, loss = 0.49161707\n",
      "Iteration 209, loss = 0.50387011\n",
      "Iteration 210, loss = 0.52043276\n",
      "Iteration 211, loss = 0.51269784\n",
      "Iteration 212, loss = 0.49088591\n",
      "Iteration 213, loss = 0.48248102\n",
      "Iteration 214, loss = 0.49184830\n",
      "Iteration 215, loss = 0.50209765\n",
      "Iteration 216, loss = 0.49779087\n",
      "Iteration 217, loss = 0.48308558\n",
      "Iteration 218, loss = 0.47815016\n",
      "Iteration 219, loss = 0.49304078\n",
      "Iteration 220, loss = 0.52780038\n",
      "Iteration 221, loss = 0.50697710\n",
      "Iteration 222, loss = 0.49861803\n",
      "Iteration 223, loss = 0.54145883\n",
      "Iteration 224, loss = 0.58640854\n",
      "Iteration 225, loss = 0.53889973\n",
      "Iteration 226, loss = 0.46413362\n",
      "Iteration 227, loss = 0.47498895\n",
      "Iteration 228, loss = 0.52998960\n",
      "Iteration 229, loss = 0.51391365\n",
      "Iteration 230, loss = 0.45808361\n",
      "Iteration 231, loss = 0.46137830\n",
      "Iteration 232, loss = 0.49962688\n",
      "Iteration 233, loss = 0.48942946\n",
      "Iteration 234, loss = 0.44975373\n",
      "Iteration 235, loss = 0.45097289\n",
      "Iteration 236, loss = 0.47760357\n",
      "Iteration 237, loss = 0.47231901\n",
      "Iteration 238, loss = 0.44384589\n",
      "Iteration 239, loss = 0.43683154\n",
      "Iteration 240, loss = 0.45359295\n",
      "Iteration 241, loss = 0.50461884\n",
      "Iteration 242, loss = 0.51511035\n",
      "Iteration 243, loss = 0.45744924\n",
      "Iteration 244, loss = 0.42779631\n",
      "Iteration 245, loss = 0.45928339\n",
      "Iteration 246, loss = 0.48208541\n",
      "Iteration 247, loss = 0.44897216\n",
      "Iteration 248, loss = 0.42066093\n",
      "Iteration 249, loss = 0.43689356\n",
      "Iteration 250, loss = 0.45770759\n",
      "Iteration 251, loss = 0.44161547\n",
      "Iteration 252, loss = 0.41607010\n",
      "Iteration 253, loss = 0.41741397\n",
      "Iteration 254, loss = 0.43464428\n",
      "Iteration 255, loss = 0.43634242\n",
      "Iteration 256, loss = 0.41864908\n",
      "Iteration 257, loss = 0.40499635\n",
      "Iteration 258, loss = 0.40783456\n",
      "Iteration 259, loss = 0.41829800\n",
      "Iteration 260, loss = 0.42163495\n",
      "Iteration 261, loss = 0.41326334\n",
      "Iteration 262, loss = 0.40096198\n",
      "Iteration 263, loss = 0.39358978\n",
      "Iteration 264, loss = 0.39346980\n",
      "Iteration 265, loss = 0.39807106\n",
      "Iteration 266, loss = 0.40384805\n",
      "Iteration 267, loss = 0.40815418\n",
      "Iteration 268, loss = 0.40957234\n",
      "Iteration 269, loss = 0.40800395\n",
      "Iteration 270, loss = 0.40415576\n",
      "Iteration 271, loss = 0.39933072\n",
      "Iteration 272, loss = 0.39459712\n",
      "Iteration 273, loss = 0.39083993\n",
      "Iteration 274, loss = 0.38853343\n",
      "Iteration 275, loss = 0.38828769\n",
      "Iteration 276, loss = 0.39093029\n",
      "Iteration 277, loss = 0.39811884\n",
      "Iteration 278, loss = 0.41168048\n",
      "Iteration 279, loss = 0.43226002\n",
      "Iteration 280, loss = 0.45264153\n",
      "Iteration 281, loss = 0.45707586\n",
      "Iteration 282, loss = 0.41978342\n",
      "Iteration 283, loss = 0.39172824\n",
      "Iteration 284, loss = 0.38578354\n",
      "Iteration 285, loss = 0.38727222\n",
      "Iteration 286, loss = 0.39701108\n",
      "Iteration 287, loss = 0.41000736\n",
      "Iteration 288, loss = 0.42084540\n",
      "Iteration 289, loss = 0.41646865\n",
      "Iteration 290, loss = 0.39492523\n",
      "Iteration 291, loss = 0.36717912\n",
      "Iteration 292, loss = 0.34728816\n",
      "Iteration 293, loss = 0.34115331\n",
      "Iteration 294, loss = 0.34681769\n",
      "Iteration 295, loss = 0.35940343\n",
      "Iteration 296, loss = 0.37329936\n",
      "Iteration 297, loss = 0.38172181\n",
      "Iteration 298, loss = 0.38016371\n",
      "Iteration 299, loss = 0.36808852\n",
      "Iteration 300, loss = 0.35179312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72984577\n",
      "Iteration 2, loss = 1.84268555\n",
      "Iteration 3, loss = 0.70410783\n",
      "Iteration 4, loss = 1.15866282\n",
      "Iteration 5, loss = 0.96375445\n",
      "Iteration 6, loss = 0.74105259\n",
      "Iteration 7, loss = 0.71890719\n",
      "Iteration 8, loss = 0.80595944\n",
      "Iteration 9, loss = 0.79358600\n",
      "Iteration 10, loss = 0.72676768\n",
      "Iteration 11, loss = 0.69280004\n",
      "Iteration 12, loss = 0.72130747\n",
      "Iteration 13, loss = 0.73301410\n",
      "Iteration 14, loss = 0.71652338\n",
      "Iteration 15, loss = 0.69557462\n",
      "Iteration 16, loss = 0.69555241\n",
      "Iteration 17, loss = 0.71126411\n",
      "Iteration 18, loss = 0.71715673\n",
      "Iteration 19, loss = 0.70550840\n",
      "Iteration 20, loss = 0.69311818\n",
      "Iteration 21, loss = 0.69576438\n",
      "Iteration 22, loss = 0.70614569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72996375\n",
      "Iteration 2, loss = 1.86423861\n",
      "Iteration 3, loss = 0.71412389\n",
      "Iteration 4, loss = 1.16269512\n",
      "Iteration 5, loss = 1.00853823\n",
      "Iteration 6, loss = 0.72895269\n",
      "Iteration 7, loss = 0.72977922\n",
      "Iteration 8, loss = 0.84484249\n",
      "Iteration 9, loss = 0.82404423\n",
      "Iteration 10, loss = 0.72113662\n",
      "Iteration 11, loss = 0.70225411\n",
      "Iteration 12, loss = 0.77000854\n",
      "Iteration 13, loss = 0.77887794\n",
      "Iteration 14, loss = 0.72556767\n",
      "Iteration 15, loss = 0.69215932\n",
      "Iteration 16, loss = 0.71936066\n",
      "Iteration 17, loss = 0.74904031\n",
      "Iteration 18, loss = 0.73301183\n",
      "Iteration 19, loss = 0.69867749\n",
      "Iteration 20, loss = 0.69602479\n",
      "Iteration 21, loss = 0.72062748\n",
      "Iteration 22, loss = 0.72655337\n",
      "Iteration 23, loss = 0.70552244\n",
      "Iteration 24, loss = 0.69144752\n",
      "Iteration 25, loss = 0.70242841\n",
      "Iteration 26, loss = 0.71469408\n",
      "Iteration 27, loss = 0.70707881\n",
      "Iteration 28, loss = 0.69294453\n",
      "Iteration 29, loss = 0.69409021\n",
      "Iteration 30, loss = 0.70349377\n",
      "Iteration 31, loss = 0.70392697\n",
      "Iteration 32, loss = 0.69530169\n",
      "Iteration 33, loss = 0.69122427\n",
      "Iteration 34, loss = 0.69636339\n",
      "Iteration 35, loss = 0.70000732\n",
      "Iteration 36, loss = 0.69575545\n",
      "Iteration 37, loss = 0.69096013\n",
      "Iteration 38, loss = 0.69262801\n",
      "Iteration 39, loss = 0.69567567\n",
      "Iteration 40, loss = 0.69496414\n",
      "Iteration 41, loss = 0.69174723\n",
      "Iteration 42, loss = 0.69096199\n",
      "Iteration 43, loss = 0.69298286\n",
      "Iteration 44, loss = 0.69375474\n",
      "Iteration 45, loss = 0.69190056\n",
      "Iteration 46, loss = 0.69046026\n",
      "Iteration 47, loss = 0.69133849\n",
      "Iteration 48, loss = 0.69238435\n",
      "Iteration 49, loss = 0.69154568\n",
      "Iteration 50, loss = 0.69023601\n",
      "Iteration 51, loss = 0.69042784\n",
      "Iteration 52, loss = 0.69126275\n",
      "Iteration 53, loss = 0.69095740\n",
      "Iteration 54, loss = 0.68997274\n",
      "Iteration 55, loss = 0.68986283\n",
      "Iteration 56, loss = 0.69042298\n",
      "Iteration 57, loss = 0.69032432\n",
      "Iteration 58, loss = 0.68962357\n",
      "Iteration 59, loss = 0.68943565\n",
      "Iteration 60, loss = 0.68978036\n",
      "Iteration 61, loss = 0.68972734\n",
      "Iteration 62, loss = 0.68922453\n",
      "Iteration 63, loss = 0.68904687\n",
      "Iteration 64, loss = 0.68924776\n",
      "Iteration 65, loss = 0.68917789\n",
      "Iteration 66, loss = 0.68880241\n",
      "Iteration 67, loss = 0.69578362\n",
      "Iteration 68, loss = 0.68937518\n",
      "Iteration 69, loss = 0.69056715\n",
      "Iteration 70, loss = 0.68944885\n",
      "Iteration 71, loss = 0.68824472\n",
      "Iteration 72, loss = 0.68890636\n",
      "Iteration 73, loss = 0.68947351\n",
      "Iteration 74, loss = 0.68852645\n",
      "Iteration 75, loss = 0.68784376\n",
      "Iteration 76, loss = 0.68839232\n",
      "Iteration 77, loss = 0.68855719\n",
      "Iteration 78, loss = 0.68777258\n",
      "Iteration 79, loss = 0.68746931\n",
      "Iteration 80, loss = 0.68787130\n",
      "Iteration 81, loss = 0.68775159\n",
      "Iteration 82, loss = 0.68715753\n",
      "Iteration 83, loss = 0.68709663\n",
      "Iteration 84, loss = 0.68731492\n",
      "Iteration 85, loss = 0.68703142\n",
      "Iteration 86, loss = 0.68664247\n",
      "Iteration 87, loss = 0.68669622\n",
      "Iteration 88, loss = 0.68672160\n",
      "Iteration 89, loss = 0.68639144\n",
      "Iteration 90, loss = 0.68618707\n",
      "Iteration 91, loss = 0.68623939\n",
      "Iteration 92, loss = 0.68610885\n",
      "Iteration 93, loss = 0.68582542\n",
      "Iteration 94, loss = 0.68574318\n",
      "Iteration 95, loss = 0.68571741\n",
      "Iteration 96, loss = 0.68550529\n",
      "Iteration 97, loss = 0.68531347\n",
      "Iteration 98, loss = 0.68526644\n",
      "Iteration 99, loss = 0.68514524\n",
      "Iteration 100, loss = 0.68493278\n",
      "Iteration 101, loss = 0.68481708\n",
      "Iteration 102, loss = 0.68473390\n",
      "Iteration 103, loss = 0.68455498\n",
      "Iteration 104, loss = 0.68438944\n",
      "Iteration 105, loss = 0.68429431\n",
      "Iteration 106, loss = 0.68415291\n",
      "Iteration 107, loss = 0.68397300\n",
      "Iteration 108, loss = 0.68384724\n",
      "Iteration 109, loss = 0.68372327\n",
      "Iteration 110, loss = 0.68355085\n",
      "Iteration 111, loss = 0.68339910\n",
      "Iteration 112, loss = 0.68327299\n",
      "Iteration 113, loss = 0.68311259\n",
      "Iteration 114, loss = 0.68294673\n",
      "Iteration 115, loss = 0.68280862\n",
      "Iteration 116, loss = 0.68265530\n",
      "Iteration 117, loss = 0.68248435\n",
      "Iteration 118, loss = 0.68233262\n",
      "Iteration 119, loss = 0.68217963\n",
      "Iteration 120, loss = 0.68203154\n",
      "Iteration 121, loss = 0.68184441\n",
      "Iteration 122, loss = 0.68168690\n",
      "Iteration 123, loss = 0.68151388\n",
      "Iteration 124, loss = 0.68134208\n",
      "Iteration 125, loss = 0.68117754\n",
      "Iteration 126, loss = 0.68100221\n",
      "Iteration 127, loss = 0.68082365\n",
      "Iteration 128, loss = 0.68065129\n",
      "Iteration 129, loss = 0.68047194\n",
      "Iteration 130, loss = 0.68028743\n",
      "Iteration 131, loss = 0.68010716\n",
      "Iteration 132, loss = 0.67992240\n",
      "Iteration 133, loss = 0.67973202\n",
      "Iteration 134, loss = 0.67954401\n",
      "Iteration 135, loss = 0.67935284\n",
      "Iteration 136, loss = 0.67915625\n",
      "Iteration 137, loss = 0.67896054\n",
      "Iteration 138, loss = 0.67876223\n",
      "Iteration 139, loss = 0.67855895\n",
      "Iteration 140, loss = 0.67835549\n",
      "Iteration 141, loss = 0.67814950\n",
      "Iteration 142, loss = 0.67793896\n",
      "Iteration 143, loss = 0.67819184\n",
      "Iteration 144, loss = 0.67787200\n",
      "Iteration 145, loss = 0.67768851\n",
      "Iteration 146, loss = 0.67701532\n",
      "Iteration 147, loss = 0.67717124\n",
      "Iteration 148, loss = 0.67658747\n",
      "Iteration 149, loss = 0.67645041\n",
      "Iteration 150, loss = 0.67629544\n",
      "Iteration 151, loss = 0.67577639\n",
      "Iteration 152, loss = 0.67564384\n",
      "Iteration 153, loss = 0.67520190\n",
      "Iteration 154, loss = 0.67487323\n",
      "Iteration 155, loss = 0.67423853\n",
      "Iteration 156, loss = 0.67389557\n",
      "Iteration 157, loss = 0.67320530\n",
      "Iteration 158, loss = 0.67264020\n",
      "Iteration 159, loss = 0.67181893\n",
      "Iteration 160, loss = 0.67392308\n",
      "Iteration 161, loss = 0.67315054\n",
      "Iteration 162, loss = 0.67499661\n",
      "Iteration 163, loss = 0.67815512\n",
      "Iteration 164, loss = 0.67051675\n",
      "Iteration 165, loss = 0.67696874\n",
      "Iteration 166, loss = 0.66952266\n",
      "Iteration 167, loss = 0.67253248\n",
      "Iteration 168, loss = 0.66910469\n",
      "Iteration 169, loss = 0.66995477\n",
      "Iteration 170, loss = 0.66858805\n",
      "Iteration 171, loss = 0.66722709\n",
      "Iteration 172, loss = 0.66775950\n",
      "Iteration 173, loss = 0.66469634\n",
      "Iteration 174, loss = 0.66620551\n",
      "Iteration 175, loss = 0.66302042\n",
      "Iteration 176, loss = 0.66362976\n",
      "Iteration 177, loss = 0.66202749\n",
      "Iteration 178, loss = 0.66053426\n",
      "Iteration 179, loss = 0.66074430\n",
      "Iteration 180, loss = 0.65809533\n",
      "Iteration 181, loss = 0.65810010\n",
      "Iteration 182, loss = 0.65672259\n",
      "Iteration 183, loss = 0.66063922\n",
      "Iteration 184, loss = 0.69724312\n",
      "Iteration 185, loss = 0.67074352\n",
      "Iteration 186, loss = 0.66962483\n",
      "Iteration 187, loss = 0.66516351\n",
      "Iteration 188, loss = 0.65669606\n",
      "Iteration 189, loss = 0.71233185\n",
      "Iteration 190, loss = 0.64180612\n",
      "Iteration 191, loss = 0.69803782\n",
      "Iteration 192, loss = 0.64243263\n",
      "Iteration 193, loss = 0.68080509\n",
      "Iteration 194, loss = 0.65149005\n",
      "Iteration 195, loss = 0.66844350\n",
      "Iteration 196, loss = 0.64980562\n",
      "Iteration 197, loss = 0.65911482\n",
      "Iteration 198, loss = 0.64742018\n",
      "Iteration 199, loss = 0.65235296\n",
      "Iteration 200, loss = 0.64471858\n",
      "Iteration 201, loss = 0.64697275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70079682\n",
      "Iteration 2, loss = 0.69546857\n",
      "Iteration 3, loss = 0.69405868\n",
      "Iteration 4, loss = 0.69110545\n",
      "Iteration 5, loss = 0.69064748\n",
      "Iteration 6, loss = 0.68975111\n",
      "Iteration 7, loss = 0.68942280\n",
      "Iteration 8, loss = 0.68914721\n",
      "Iteration 9, loss = 0.68885064\n",
      "Iteration 10, loss = 0.68856596\n",
      "Iteration 11, loss = 0.68827019\n",
      "Iteration 12, loss = 0.68796546\n",
      "Iteration 13, loss = 0.68764918\n",
      "Iteration 14, loss = 0.68730694\n",
      "Iteration 15, loss = 0.68694686\n",
      "Iteration 16, loss = 0.68656037\n",
      "Iteration 17, loss = 0.68623016\n",
      "Iteration 18, loss = 0.68589360\n",
      "Iteration 19, loss = 0.68553103\n",
      "Iteration 20, loss = 0.68515133\n",
      "Iteration 21, loss = 0.68477304\n",
      "Iteration 22, loss = 0.68440673\n",
      "Iteration 23, loss = 0.68403055\n",
      "Iteration 24, loss = 0.68365135\n",
      "Iteration 25, loss = 0.68326327\n",
      "Iteration 26, loss = 0.68288075\n",
      "Iteration 27, loss = 0.68248297\n",
      "Iteration 28, loss = 0.68207767\n",
      "Iteration 29, loss = 0.68165531\n",
      "Iteration 30, loss = 0.68123857\n",
      "Iteration 31, loss = 0.68082998\n",
      "Iteration 32, loss = 0.68042201\n",
      "Iteration 33, loss = 0.68003094\n",
      "Iteration 34, loss = 0.67963208\n",
      "Iteration 35, loss = 0.67924785\n",
      "Iteration 36, loss = 0.67883895\n",
      "Iteration 37, loss = 0.67842919\n",
      "Iteration 38, loss = 0.67802390\n",
      "Iteration 39, loss = 0.67760765\n",
      "Iteration 40, loss = 0.67720552\n",
      "Iteration 41, loss = 0.67680107\n",
      "Iteration 42, loss = 0.67639626\n",
      "Iteration 43, loss = 0.67597871\n",
      "Iteration 44, loss = 0.67553564\n",
      "Iteration 45, loss = 0.67507373\n",
      "Iteration 46, loss = 0.67462434\n",
      "Iteration 47, loss = 0.67420209\n",
      "Iteration 48, loss = 0.67376707\n",
      "Iteration 49, loss = 0.67335104\n",
      "Iteration 50, loss = 0.67292043\n",
      "Iteration 51, loss = 0.67248140\n",
      "Iteration 52, loss = 0.67206728\n",
      "Iteration 53, loss = 0.67165749\n",
      "Iteration 54, loss = 0.67122662\n",
      "Iteration 55, loss = 0.67080591\n",
      "Iteration 56, loss = 0.67038558\n",
      "Iteration 57, loss = 0.66996633\n",
      "Iteration 58, loss = 0.66953467\n",
      "Iteration 59, loss = 0.66911923\n",
      "Iteration 60, loss = 0.66870043\n",
      "Iteration 61, loss = 0.66826733\n",
      "Iteration 62, loss = 0.66785404\n",
      "Iteration 63, loss = 0.66740819\n",
      "Iteration 64, loss = 0.66698024\n",
      "Iteration 65, loss = 0.66655695\n",
      "Iteration 66, loss = 0.66612262\n",
      "Iteration 67, loss = 0.66568782\n",
      "Iteration 68, loss = 0.66526241\n",
      "Iteration 69, loss = 0.66481949\n",
      "Iteration 70, loss = 0.66439427\n",
      "Iteration 71, loss = 0.66396762\n",
      "Iteration 72, loss = 0.66351460\n",
      "Iteration 73, loss = 0.66308281\n",
      "Iteration 74, loss = 0.66266246\n",
      "Iteration 75, loss = 0.66220325\n",
      "Iteration 76, loss = 0.66172636\n",
      "Iteration 77, loss = 0.66123425\n",
      "Iteration 78, loss = 0.66065515\n",
      "Iteration 79, loss = 0.66000051\n",
      "Iteration 80, loss = 0.65934828\n",
      "Iteration 81, loss = 0.65883427\n",
      "Iteration 82, loss = 0.65835202\n",
      "Iteration 83, loss = 0.65788749\n",
      "Iteration 84, loss = 0.65742866\n",
      "Iteration 85, loss = 0.65696815\n",
      "Iteration 86, loss = 0.65650432\n",
      "Iteration 87, loss = 0.65604496\n",
      "Iteration 88, loss = 0.65557345\n",
      "Iteration 89, loss = 0.65510596\n",
      "Iteration 90, loss = 0.65464075\n",
      "Iteration 91, loss = 0.65416466\n",
      "Iteration 92, loss = 0.65370157\n",
      "Iteration 93, loss = 0.65322574\n",
      "Iteration 94, loss = 0.65274463\n",
      "Iteration 95, loss = 0.65226891\n",
      "Iteration 96, loss = 0.65178576\n",
      "Iteration 97, loss = 0.65131580\n",
      "Iteration 98, loss = 0.65082007\n",
      "Iteration 99, loss = 0.65034722\n",
      "Iteration 100, loss = 0.64984853\n",
      "Iteration 101, loss = 0.64936719\n",
      "Iteration 102, loss = 0.64888295\n",
      "Iteration 103, loss = 0.64839658\n",
      "Iteration 104, loss = 0.64789764\n",
      "Iteration 105, loss = 0.64741367\n",
      "Iteration 106, loss = 0.64691766\n",
      "Iteration 107, loss = 0.64643681\n",
      "Iteration 108, loss = 0.64593291\n",
      "Iteration 109, loss = 0.64543175\n",
      "Iteration 110, loss = 0.64492214\n",
      "Iteration 111, loss = 0.64443255\n",
      "Iteration 112, loss = 0.64391353\n",
      "Iteration 113, loss = 0.64341394\n",
      "Iteration 114, loss = 0.64290212\n",
      "Iteration 115, loss = 0.64239949\n",
      "Iteration 116, loss = 0.64187461\n",
      "Iteration 117, loss = 0.64136718\n",
      "Iteration 118, loss = 0.64086419\n",
      "Iteration 119, loss = 0.64035133\n",
      "Iteration 120, loss = 0.63981840\n",
      "Iteration 121, loss = 0.63930241\n",
      "Iteration 122, loss = 0.63876661\n",
      "Iteration 123, loss = 0.63824782\n",
      "Iteration 124, loss = 0.63771694\n",
      "Iteration 125, loss = 0.63719834\n",
      "Iteration 126, loss = 0.63666948\n",
      "Iteration 127, loss = 0.63613694\n",
      "Iteration 128, loss = 0.63557066\n",
      "Iteration 129, loss = 0.63502510\n",
      "Iteration 130, loss = 0.63447507\n",
      "Iteration 131, loss = 0.63393045\n",
      "Iteration 132, loss = 0.63339140\n",
      "Iteration 133, loss = 0.63284157\n",
      "Iteration 134, loss = 0.63230371\n",
      "Iteration 135, loss = 0.63174884\n",
      "Iteration 136, loss = 0.63119649\n",
      "Iteration 137, loss = 0.63065989\n",
      "Iteration 138, loss = 0.63010740\n",
      "Iteration 139, loss = 0.62953539\n",
      "Iteration 140, loss = 0.62896698\n",
      "Iteration 141, loss = 0.62839878\n",
      "Iteration 142, loss = 0.62783030\n",
      "Iteration 143, loss = 0.62725522\n",
      "Iteration 144, loss = 0.62667384\n",
      "Iteration 145, loss = 0.62609567\n",
      "Iteration 146, loss = 0.62551222\n",
      "Iteration 147, loss = 0.62493386\n",
      "Iteration 148, loss = 0.62434695\n",
      "Iteration 149, loss = 0.62376105\n",
      "Iteration 150, loss = 0.62317229\n",
      "Iteration 151, loss = 0.62257508\n",
      "Iteration 152, loss = 0.62197087\n",
      "Iteration 153, loss = 0.62137004\n",
      "Iteration 154, loss = 0.62077087\n",
      "Iteration 155, loss = 0.62017004\n",
      "Iteration 156, loss = 0.61955573\n",
      "Iteration 157, loss = 0.61894876\n",
      "Iteration 158, loss = 0.61833442\n",
      "Iteration 159, loss = 0.61772393\n",
      "Iteration 160, loss = 0.61710359\n",
      "Iteration 161, loss = 0.61648433\n",
      "Iteration 162, loss = 0.61586554\n",
      "Iteration 163, loss = 0.61525276\n",
      "Iteration 164, loss = 0.61462465\n",
      "Iteration 165, loss = 0.61398956\n",
      "Iteration 166, loss = 0.61336446\n",
      "Iteration 167, loss = 0.61272643\n",
      "Iteration 168, loss = 0.61210530\n",
      "Iteration 169, loss = 0.61147347\n",
      "Iteration 170, loss = 0.61083165\n",
      "Iteration 171, loss = 0.61019223\n",
      "Iteration 172, loss = 0.60959741\n",
      "Iteration 173, loss = 0.60897978\n",
      "Iteration 174, loss = 0.60835926\n",
      "Iteration 175, loss = 0.60777232\n",
      "Iteration 176, loss = 0.60719989\n",
      "Iteration 177, loss = 0.60670970\n",
      "Iteration 178, loss = 0.60626685\n",
      "Iteration 179, loss = 0.60594473\n",
      "Iteration 180, loss = 0.60596016\n",
      "Iteration 181, loss = 0.60620030\n",
      "Iteration 182, loss = 0.60716650\n",
      "Iteration 183, loss = 0.60844108\n",
      "Iteration 184, loss = 0.61107080\n",
      "Iteration 185, loss = 0.61482743\n",
      "Iteration 186, loss = 0.62200506\n",
      "Iteration 187, loss = 0.63145536\n",
      "Iteration 188, loss = 0.65153909\n",
      "Iteration 189, loss = 0.67102667\n",
      "Iteration 190, loss = 0.72314321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70389361\n",
      "Iteration 2, loss = 0.69920083\n",
      "Iteration 3, loss = 0.69793444\n",
      "Iteration 4, loss = 0.69672488\n",
      "Iteration 5, loss = 0.69641040\n",
      "Iteration 6, loss = 0.69619282\n",
      "Iteration 7, loss = 0.69591605\n",
      "Iteration 8, loss = 0.69564435\n",
      "Iteration 9, loss = 0.69530562\n",
      "Iteration 10, loss = 0.69500893\n",
      "Iteration 11, loss = 0.69485339\n",
      "Iteration 12, loss = 0.69467739\n",
      "Iteration 13, loss = 0.69448291\n",
      "Iteration 14, loss = 0.69427088\n",
      "Iteration 15, loss = 0.69399981\n",
      "Iteration 16, loss = 0.69374127\n",
      "Iteration 17, loss = 0.69347183\n",
      "Iteration 18, loss = 0.69323326\n",
      "Iteration 19, loss = 0.69293551\n",
      "Iteration 20, loss = 0.69267841\n",
      "Iteration 21, loss = 0.69237829\n",
      "Iteration 22, loss = 0.69211125\n",
      "Iteration 23, loss = 0.69183209\n",
      "Iteration 24, loss = 0.69156769\n",
      "Iteration 25, loss = 0.69129200\n",
      "Iteration 26, loss = 0.69102346\n",
      "Iteration 27, loss = 0.69080576\n",
      "Iteration 28, loss = 0.69058052\n",
      "Iteration 29, loss = 0.69036619\n",
      "Iteration 30, loss = 0.69009543\n",
      "Iteration 31, loss = 0.68985331\n",
      "Iteration 32, loss = 0.68960614\n",
      "Iteration 33, loss = 0.68934140\n",
      "Iteration 34, loss = 0.68907170\n",
      "Iteration 35, loss = 0.68883614\n",
      "Iteration 36, loss = 0.68858261\n",
      "Iteration 37, loss = 0.68833366\n",
      "Iteration 38, loss = 0.68808418\n",
      "Iteration 39, loss = 0.68783054\n",
      "Iteration 40, loss = 0.68758139\n",
      "Iteration 41, loss = 0.68735388\n",
      "Iteration 42, loss = 0.68710016\n",
      "Iteration 43, loss = 0.68684354\n",
      "Iteration 44, loss = 0.68658559\n",
      "Iteration 45, loss = 0.68632980\n",
      "Iteration 46, loss = 0.68611365\n",
      "Iteration 47, loss = 0.68583569\n",
      "Iteration 48, loss = 0.68555947\n",
      "Iteration 49, loss = 0.68531919\n",
      "Iteration 50, loss = 0.68508181\n",
      "Iteration 51, loss = 0.68482172\n",
      "Iteration 52, loss = 0.68456430\n",
      "Iteration 53, loss = 0.68433795\n",
      "Iteration 54, loss = 0.68407890\n",
      "Iteration 55, loss = 0.68381347\n",
      "Iteration 56, loss = 0.68354582\n",
      "Iteration 57, loss = 0.68329619\n",
      "Iteration 58, loss = 0.68305009\n",
      "Iteration 59, loss = 0.68279847\n",
      "Iteration 60, loss = 0.68254297\n",
      "Iteration 61, loss = 0.68232195\n",
      "Iteration 62, loss = 0.68207601\n",
      "Iteration 63, loss = 0.68182117\n",
      "Iteration 64, loss = 0.68158471\n",
      "Iteration 65, loss = 0.68133470\n",
      "Iteration 66, loss = 0.68107044\n",
      "Iteration 67, loss = 0.68078290\n",
      "Iteration 68, loss = 0.68062552\n",
      "Iteration 69, loss = 0.68027565\n",
      "Iteration 70, loss = 0.68002148\n",
      "Iteration 71, loss = 0.67975660\n",
      "Iteration 72, loss = 0.67950203\n",
      "Iteration 73, loss = 0.67926588\n",
      "Iteration 74, loss = 0.67900293\n",
      "Iteration 75, loss = 0.67873679\n",
      "Iteration 76, loss = 0.67847192\n",
      "Iteration 77, loss = 0.67820766\n",
      "Iteration 78, loss = 0.67798209\n",
      "Iteration 79, loss = 0.67770981\n",
      "Iteration 80, loss = 0.67749490\n",
      "Iteration 81, loss = 0.67722434\n",
      "Iteration 82, loss = 0.67696873\n",
      "Iteration 83, loss = 0.67671080\n",
      "Iteration 84, loss = 0.67644885\n",
      "Iteration 85, loss = 0.67619295\n",
      "Iteration 86, loss = 0.67593730\n",
      "Iteration 87, loss = 0.67566224\n",
      "Iteration 88, loss = 0.67539504\n",
      "Iteration 89, loss = 0.67513405\n",
      "Iteration 90, loss = 0.67486215\n",
      "Iteration 91, loss = 0.67461523\n",
      "Iteration 92, loss = 0.67435074\n",
      "Iteration 93, loss = 0.67410189\n",
      "Iteration 94, loss = 0.67383333\n",
      "Iteration 95, loss = 0.67356998\n",
      "Iteration 96, loss = 0.67332796\n",
      "Iteration 97, loss = 0.67308311\n",
      "Iteration 98, loss = 0.67280543\n",
      "Iteration 99, loss = 0.67252947\n",
      "Iteration 100, loss = 0.67225788\n",
      "Iteration 101, loss = 0.67199316\n",
      "Iteration 102, loss = 0.67173434\n",
      "Iteration 103, loss = 0.67146102\n",
      "Iteration 104, loss = 0.67120138\n",
      "Iteration 105, loss = 0.67093513\n",
      "Iteration 106, loss = 0.67066478\n",
      "Iteration 107, loss = 0.67039945\n",
      "Iteration 108, loss = 0.67013435\n",
      "Iteration 109, loss = 0.66986735\n",
      "Iteration 110, loss = 0.66960535\n",
      "Iteration 111, loss = 0.66933378\n",
      "Iteration 112, loss = 0.66909834\n",
      "Iteration 113, loss = 0.66882995\n",
      "Iteration 114, loss = 0.66854115\n",
      "Iteration 115, loss = 0.66824933\n",
      "Iteration 116, loss = 0.66797906\n",
      "Iteration 117, loss = 0.66770718\n",
      "Iteration 118, loss = 0.66743525\n",
      "Iteration 119, loss = 0.66716641\n",
      "Iteration 120, loss = 0.66688749\n",
      "Iteration 121, loss = 0.66664529\n",
      "Iteration 122, loss = 0.66638881\n",
      "Iteration 123, loss = 0.66608040\n",
      "Iteration 124, loss = 0.66579182\n",
      "Iteration 125, loss = 0.66550379\n",
      "Iteration 126, loss = 0.66522520\n",
      "Iteration 127, loss = 0.66499202\n",
      "Iteration 128, loss = 0.66476827\n",
      "Iteration 129, loss = 0.66450675\n",
      "Iteration 130, loss = 0.66418848\n",
      "Iteration 131, loss = 0.66389337\n",
      "Iteration 132, loss = 0.66360165\n",
      "Iteration 133, loss = 0.66329046\n",
      "Iteration 134, loss = 0.66305020\n",
      "Iteration 135, loss = 0.66272416\n",
      "Iteration 136, loss = 0.66241720\n",
      "Iteration 137, loss = 0.66214936\n",
      "Iteration 138, loss = 0.66189891\n",
      "Iteration 139, loss = 0.66161099\n",
      "Iteration 140, loss = 0.66131789\n",
      "Iteration 141, loss = 0.66098948\n",
      "Iteration 142, loss = 0.66076460\n",
      "Iteration 143, loss = 0.66042630\n",
      "Iteration 144, loss = 0.66012351\n",
      "Iteration 145, loss = 0.65984865\n",
      "Iteration 146, loss = 0.65956999\n",
      "Iteration 147, loss = 0.65922010\n",
      "Iteration 148, loss = 0.65900266\n",
      "Iteration 149, loss = 0.65868689\n",
      "Iteration 150, loss = 0.65839024\n",
      "Iteration 151, loss = 0.65809396\n",
      "Iteration 152, loss = 0.65776346\n",
      "Iteration 153, loss = 0.65745357\n",
      "Iteration 154, loss = 0.65718206\n",
      "Iteration 155, loss = 0.65683378\n",
      "Iteration 156, loss = 0.65653396\n",
      "Iteration 157, loss = 0.65624128\n",
      "Iteration 158, loss = 0.65593236\n",
      "Iteration 159, loss = 0.65561212\n",
      "Iteration 160, loss = 0.65532135\n",
      "Iteration 161, loss = 0.65500008\n",
      "Iteration 162, loss = 0.65470274\n",
      "Iteration 163, loss = 0.65441240\n",
      "Iteration 164, loss = 0.65410454\n",
      "Iteration 165, loss = 0.65375805\n",
      "Iteration 166, loss = 0.65345823\n",
      "Iteration 167, loss = 0.65313781\n",
      "Iteration 168, loss = 0.65283389\n",
      "Iteration 169, loss = 0.65252241\n",
      "Iteration 170, loss = 0.65219447\n",
      "Iteration 171, loss = 0.65186243\n",
      "Iteration 172, loss = 0.65157610\n",
      "Iteration 173, loss = 0.65127669\n",
      "Iteration 174, loss = 0.65096423\n",
      "Iteration 175, loss = 0.65063148\n",
      "Iteration 176, loss = 0.65029965\n",
      "Iteration 177, loss = 0.64998296\n",
      "Iteration 178, loss = 0.64964427\n",
      "Iteration 179, loss = 0.64931465\n",
      "Iteration 180, loss = 0.64896869\n",
      "Iteration 181, loss = 0.64866742\n",
      "Iteration 182, loss = 0.64832402\n",
      "Iteration 183, loss = 0.64799978\n",
      "Iteration 184, loss = 0.64764421\n",
      "Iteration 185, loss = 0.64731976\n",
      "Iteration 186, loss = 0.64696488\n",
      "Iteration 187, loss = 0.64665921\n",
      "Iteration 188, loss = 0.64628200\n",
      "Iteration 189, loss = 0.64597064\n",
      "Iteration 190, loss = 0.64562277\n",
      "Iteration 191, loss = 0.64528845\n",
      "Iteration 192, loss = 0.64492014\n",
      "Iteration 193, loss = 0.64461901\n",
      "Iteration 194, loss = 0.64424594\n",
      "Iteration 195, loss = 0.64389217\n",
      "Iteration 196, loss = 0.64357263\n",
      "Iteration 197, loss = 0.64321604\n",
      "Iteration 198, loss = 0.64287306\n",
      "Iteration 199, loss = 0.64251041\n",
      "Iteration 200, loss = 0.64215448\n",
      "Iteration 201, loss = 0.64178292\n",
      "Iteration 202, loss = 0.64143440\n",
      "Iteration 203, loss = 0.64108712\n",
      "Iteration 204, loss = 0.64073491\n",
      "Iteration 205, loss = 0.64035398\n",
      "Iteration 206, loss = 0.64001267\n",
      "Iteration 207, loss = 0.63965084\n",
      "Iteration 208, loss = 0.63929927\n",
      "Iteration 209, loss = 0.63892731\n",
      "Iteration 210, loss = 0.63853632\n",
      "Iteration 211, loss = 0.63814907\n",
      "Iteration 212, loss = 0.63779508\n",
      "Iteration 213, loss = 0.63740697\n",
      "Iteration 214, loss = 0.63707158\n",
      "Iteration 215, loss = 0.63667606\n",
      "Iteration 216, loss = 0.63628999\n",
      "Iteration 217, loss = 0.63589286\n",
      "Iteration 218, loss = 0.63551666\n",
      "Iteration 219, loss = 0.63515132\n",
      "Iteration 220, loss = 0.63475705\n",
      "Iteration 221, loss = 0.63435222\n",
      "Iteration 222, loss = 0.63398016\n",
      "Iteration 223, loss = 0.63359930\n",
      "Iteration 224, loss = 0.63320637\n",
      "Iteration 225, loss = 0.63278602\n",
      "Iteration 226, loss = 0.63240614\n",
      "Iteration 227, loss = 0.63202123\n",
      "Iteration 228, loss = 0.63160858\n",
      "Iteration 229, loss = 0.63122963\n",
      "Iteration 230, loss = 0.63080378\n",
      "Iteration 231, loss = 0.63039148\n",
      "Iteration 232, loss = 0.63004197\n",
      "Iteration 233, loss = 0.62960350\n",
      "Iteration 234, loss = 0.62916251\n",
      "Iteration 235, loss = 0.62875990\n",
      "Iteration 236, loss = 0.62835163\n",
      "Iteration 237, loss = 0.62791875\n",
      "Iteration 238, loss = 0.62751820\n",
      "Iteration 239, loss = 0.62709411\n",
      "Iteration 240, loss = 0.62667149\n",
      "Iteration 241, loss = 0.62624821\n",
      "Iteration 242, loss = 0.62583617\n",
      "Iteration 243, loss = 0.62541182\n",
      "Iteration 244, loss = 0.62495179\n",
      "Iteration 245, loss = 0.62452873\n",
      "Iteration 246, loss = 0.62413308\n",
      "Iteration 247, loss = 0.62372255\n",
      "Iteration 248, loss = 0.62326739\n",
      "Iteration 249, loss = 0.62281068\n",
      "Iteration 250, loss = 0.62237802\n",
      "Iteration 251, loss = 0.62198651\n",
      "Iteration 252, loss = 0.62149212\n",
      "Iteration 253, loss = 0.62107597\n",
      "Iteration 254, loss = 0.62063182\n",
      "Iteration 255, loss = 0.62013587\n",
      "Iteration 256, loss = 0.61964195\n",
      "Iteration 257, loss = 0.61917921\n",
      "Iteration 258, loss = 0.61872108\n",
      "Iteration 259, loss = 0.61827045\n",
      "Iteration 260, loss = 0.61784462\n",
      "Iteration 261, loss = 0.61741802\n",
      "Iteration 262, loss = 0.61689827\n",
      "Iteration 263, loss = 0.61641573\n",
      "Iteration 264, loss = 0.61596602\n",
      "Iteration 265, loss = 0.61550583\n",
      "Iteration 266, loss = 0.61500193\n",
      "Iteration 267, loss = 0.61451656\n",
      "Iteration 268, loss = 0.61403886\n",
      "Iteration 269, loss = 0.61354453\n",
      "Iteration 270, loss = 0.61304029\n",
      "Iteration 271, loss = 0.61255224\n",
      "Iteration 272, loss = 0.61206212\n",
      "Iteration 273, loss = 0.61156982\n",
      "Iteration 274, loss = 0.61107243\n",
      "Iteration 275, loss = 0.61059364\n",
      "Iteration 276, loss = 0.61013110\n",
      "Iteration 277, loss = 0.60959302\n",
      "Iteration 278, loss = 0.60909225\n",
      "Iteration 279, loss = 0.60857687\n",
      "Iteration 280, loss = 0.60807373\n",
      "Iteration 281, loss = 0.60756269\n",
      "Iteration 282, loss = 0.60707064\n",
      "Iteration 283, loss = 0.60653776\n",
      "Iteration 284, loss = 0.60599390\n",
      "Iteration 285, loss = 0.60546874\n",
      "Iteration 286, loss = 0.60494058\n",
      "Iteration 287, loss = 0.60441437\n",
      "Iteration 288, loss = 0.60388100\n",
      "Iteration 289, loss = 0.60336306\n",
      "Iteration 290, loss = 0.60282644\n",
      "Iteration 291, loss = 0.60226723\n",
      "Iteration 292, loss = 0.60172081\n",
      "Iteration 293, loss = 0.60116397\n",
      "Iteration 294, loss = 0.60061051\n",
      "Iteration 295, loss = 0.60007236\n",
      "Iteration 296, loss = 0.59951488\n",
      "Iteration 297, loss = 0.59895140\n",
      "Iteration 298, loss = 0.59839003\n",
      "Iteration 299, loss = 0.59782576\n",
      "Iteration 300, loss = 0.59726044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70167987\n",
      "Iteration 2, loss = 0.69682106\n",
      "Iteration 3, loss = 0.69551798\n",
      "Iteration 4, loss = 0.69342230\n",
      "Iteration 5, loss = 0.69280304\n",
      "Iteration 6, loss = 0.69216005\n",
      "Iteration 7, loss = 0.69177131\n",
      "Iteration 8, loss = 0.69146882\n",
      "Iteration 9, loss = 0.69117961\n",
      "Iteration 10, loss = 0.69088015\n",
      "Iteration 11, loss = 0.69058011\n",
      "Iteration 12, loss = 0.69029922\n",
      "Iteration 13, loss = 0.69002301\n",
      "Iteration 14, loss = 0.68973436\n",
      "Iteration 15, loss = 0.68943866\n",
      "Iteration 16, loss = 0.68914621\n",
      "Iteration 17, loss = 0.68883981\n",
      "Iteration 18, loss = 0.68853327\n",
      "Iteration 19, loss = 0.68821817\n",
      "Iteration 20, loss = 0.68790993\n",
      "Iteration 21, loss = 0.68758030\n",
      "Iteration 22, loss = 0.68723717\n",
      "Iteration 23, loss = 0.68689796\n",
      "Iteration 24, loss = 0.68652995\n",
      "Iteration 25, loss = 0.68605936\n",
      "Iteration 26, loss = 0.68564290\n",
      "Iteration 27, loss = 0.68529115\n",
      "Iteration 28, loss = 0.68500518\n",
      "Iteration 29, loss = 0.68468136\n",
      "Iteration 30, loss = 0.68435080\n",
      "Iteration 31, loss = 0.68401565\n",
      "Iteration 32, loss = 0.68367363\n",
      "Iteration 33, loss = 0.68332584\n",
      "Iteration 34, loss = 0.68296107\n",
      "Iteration 35, loss = 0.68258562\n",
      "Iteration 36, loss = 0.68222708\n",
      "Iteration 37, loss = 0.68187772\n",
      "Iteration 38, loss = 0.68154051\n",
      "Iteration 39, loss = 0.68119771\n",
      "Iteration 40, loss = 0.68085433\n",
      "Iteration 41, loss = 0.68049978\n",
      "Iteration 42, loss = 0.68015049\n",
      "Iteration 43, loss = 0.67979375\n",
      "Iteration 44, loss = 0.67944920\n",
      "Iteration 45, loss = 0.67908944\n",
      "Iteration 46, loss = 0.67873342\n",
      "Iteration 47, loss = 0.67838446\n",
      "Iteration 48, loss = 0.67803010\n",
      "Iteration 49, loss = 0.67767117\n",
      "Iteration 50, loss = 0.67731834\n",
      "Iteration 51, loss = 0.67696000\n",
      "Iteration 52, loss = 0.67660613\n",
      "Iteration 53, loss = 0.67624955\n",
      "Iteration 54, loss = 0.67589911\n",
      "Iteration 55, loss = 0.67554054\n",
      "Iteration 56, loss = 0.67517899\n",
      "Iteration 57, loss = 0.67482297\n",
      "Iteration 58, loss = 0.67446197\n",
      "Iteration 59, loss = 0.67410696\n",
      "Iteration 60, loss = 0.67374770\n",
      "Iteration 61, loss = 0.67339309\n",
      "Iteration 62, loss = 0.67302861\n",
      "Iteration 63, loss = 0.67266411\n",
      "Iteration 64, loss = 0.67230429\n",
      "Iteration 65, loss = 0.67194387\n",
      "Iteration 66, loss = 0.67158783\n",
      "Iteration 67, loss = 0.67122422\n",
      "Iteration 68, loss = 0.67086021\n",
      "Iteration 69, loss = 0.67049335\n",
      "Iteration 70, loss = 0.67013150\n",
      "Iteration 71, loss = 0.66976700\n",
      "Iteration 72, loss = 0.66939830\n",
      "Iteration 73, loss = 0.66903468\n",
      "Iteration 74, loss = 0.66866327\n",
      "Iteration 75, loss = 0.66829995\n",
      "Iteration 76, loss = 0.66793560\n",
      "Iteration 77, loss = 0.66756066\n",
      "Iteration 78, loss = 0.66719459\n",
      "Iteration 79, loss = 0.66682397\n",
      "Iteration 80, loss = 0.66645578\n",
      "Iteration 81, loss = 0.66608316\n",
      "Iteration 82, loss = 0.66570882\n",
      "Iteration 83, loss = 0.66533848\n",
      "Iteration 84, loss = 0.66496293\n",
      "Iteration 85, loss = 0.66458934\n",
      "Iteration 86, loss = 0.66421494\n",
      "Iteration 87, loss = 0.66384102\n",
      "Iteration 88, loss = 0.66346351\n",
      "Iteration 89, loss = 0.66308745\n",
      "Iteration 90, loss = 0.66270755\n",
      "Iteration 91, loss = 0.66232833\n",
      "Iteration 92, loss = 0.66194967\n",
      "Iteration 93, loss = 0.66156842\n",
      "Iteration 94, loss = 0.66118734\n",
      "Iteration 95, loss = 0.66080450\n",
      "Iteration 96, loss = 0.66042142\n",
      "Iteration 97, loss = 0.66003683\n",
      "Iteration 98, loss = 0.65965153\n",
      "Iteration 99, loss = 0.65926537\n",
      "Iteration 100, loss = 0.65887823\n",
      "Iteration 101, loss = 0.65848977\n",
      "Iteration 102, loss = 0.65809933\n",
      "Iteration 103, loss = 0.65770305\n",
      "Iteration 104, loss = 0.65730660\n",
      "Iteration 105, loss = 0.65691528\n",
      "Iteration 106, loss = 0.65652013\n",
      "Iteration 107, loss = 0.65612993\n",
      "Iteration 108, loss = 0.65572869\n",
      "Iteration 109, loss = 0.65532996\n",
      "Iteration 110, loss = 0.65493407\n",
      "Iteration 111, loss = 0.65453477\n",
      "Iteration 112, loss = 0.65413221\n",
      "Iteration 113, loss = 0.65373229\n",
      "Iteration 114, loss = 0.65332513\n",
      "Iteration 115, loss = 0.65292056\n",
      "Iteration 116, loss = 0.65251540\n",
      "Iteration 117, loss = 0.65210430\n",
      "Iteration 118, loss = 0.65169452\n",
      "Iteration 119, loss = 0.65128423\n",
      "Iteration 120, loss = 0.65087343\n",
      "Iteration 121, loss = 0.65046421\n",
      "Iteration 122, loss = 0.65004684\n",
      "Iteration 123, loss = 0.64963552\n",
      "Iteration 124, loss = 0.64921387\n",
      "Iteration 125, loss = 0.64879623\n",
      "Iteration 126, loss = 0.64837607\n",
      "Iteration 127, loss = 0.64795631\n",
      "Iteration 128, loss = 0.64753814\n",
      "Iteration 129, loss = 0.64711967\n",
      "Iteration 130, loss = 0.64669127\n",
      "Iteration 131, loss = 0.64625901\n",
      "Iteration 132, loss = 0.64583036\n",
      "Iteration 133, loss = 0.64540071\n",
      "Iteration 134, loss = 0.64496827\n",
      "Iteration 135, loss = 0.64453886\n",
      "Iteration 136, loss = 0.64410560\n",
      "Iteration 137, loss = 0.64366691\n",
      "Iteration 138, loss = 0.64323296\n",
      "Iteration 139, loss = 0.64279024\n",
      "Iteration 140, loss = 0.64235101\n",
      "Iteration 141, loss = 0.64190627\n",
      "Iteration 142, loss = 0.64146215\n",
      "Iteration 143, loss = 0.64101364\n",
      "Iteration 144, loss = 0.64056628\n",
      "Iteration 145, loss = 0.64012198\n",
      "Iteration 146, loss = 0.63967051\n",
      "Iteration 147, loss = 0.63921280\n",
      "Iteration 148, loss = 0.63874634\n",
      "Iteration 149, loss = 0.63827500\n",
      "Iteration 150, loss = 0.63780241\n",
      "Iteration 151, loss = 0.63733352\n",
      "Iteration 152, loss = 0.63686433\n",
      "Iteration 153, loss = 0.63639166\n",
      "Iteration 154, loss = 0.63592471\n",
      "Iteration 155, loss = 0.63545779\n",
      "Iteration 156, loss = 0.63499078\n",
      "Iteration 157, loss = 0.63451522\n",
      "Iteration 158, loss = 0.63403901\n",
      "Iteration 159, loss = 0.63356563\n",
      "Iteration 160, loss = 0.63308949\n",
      "Iteration 161, loss = 0.63260557\n",
      "Iteration 162, loss = 0.63212333\n",
      "Iteration 163, loss = 0.63163957\n",
      "Iteration 164, loss = 0.63115621\n",
      "Iteration 165, loss = 0.63066458\n",
      "Iteration 166, loss = 0.63017325\n",
      "Iteration 167, loss = 0.62968495\n",
      "Iteration 168, loss = 0.62919261\n",
      "Iteration 169, loss = 0.62869256\n",
      "Iteration 170, loss = 0.62819023\n",
      "Iteration 171, loss = 0.62768664\n",
      "Iteration 172, loss = 0.62718376\n",
      "Iteration 173, loss = 0.62667692\n",
      "Iteration 174, loss = 0.62616932\n",
      "Iteration 175, loss = 0.62565876\n",
      "Iteration 176, loss = 0.62514780\n",
      "Iteration 177, loss = 0.62463258\n",
      "Iteration 178, loss = 0.62411508\n",
      "Iteration 179, loss = 0.62359868\n",
      "Iteration 180, loss = 0.62308830\n",
      "Iteration 181, loss = 0.62256942\n",
      "Iteration 182, loss = 0.62204973\n",
      "Iteration 183, loss = 0.62152107\n",
      "Iteration 184, loss = 0.62099217\n",
      "Iteration 185, loss = 0.62043786\n",
      "Iteration 186, loss = 0.61983570\n",
      "Iteration 187, loss = 0.61911900\n",
      "Iteration 188, loss = 0.61827584\n",
      "Iteration 189, loss = 0.61752272\n",
      "Iteration 190, loss = 0.61691873\n",
      "Iteration 191, loss = 0.61635987\n",
      "Iteration 192, loss = 0.61579792\n",
      "Iteration 193, loss = 0.61523974\n",
      "Iteration 194, loss = 0.61466389\n",
      "Iteration 195, loss = 0.61408932\n",
      "Iteration 196, loss = 0.61351359\n",
      "Iteration 197, loss = 0.61293417\n",
      "Iteration 198, loss = 0.61235294\n",
      "Iteration 199, loss = 0.61176313\n",
      "Iteration 200, loss = 0.61118351\n",
      "Iteration 201, loss = 0.61059982\n",
      "Iteration 202, loss = 0.61001399\n",
      "Iteration 203, loss = 0.60942762\n",
      "Iteration 204, loss = 0.60883604\n",
      "Iteration 205, loss = 0.60824314\n",
      "Iteration 206, loss = 0.60767263\n",
      "Iteration 207, loss = 0.60712818\n",
      "Iteration 208, loss = 0.60658494\n",
      "Iteration 209, loss = 0.60604153\n",
      "Iteration 210, loss = 0.60551882\n",
      "Iteration 211, loss = 0.60507661\n",
      "Iteration 212, loss = 0.60466440\n",
      "Iteration 213, loss = 0.60444759\n",
      "Iteration 214, loss = 0.60438357\n",
      "Iteration 215, loss = 0.60456561\n",
      "Iteration 216, loss = 0.60547008\n",
      "Iteration 217, loss = 0.60718591\n",
      "Iteration 218, loss = 0.61163208\n",
      "Iteration 219, loss = 0.61828031\n",
      "Iteration 220, loss = 0.63440742\n",
      "Iteration 221, loss = 0.65267877\n",
      "Iteration 222, loss = 0.70532094\n",
      "Iteration 223, loss = 0.76584272\n",
      "Iteration 224, loss = 0.75595335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70079682\n",
      "Iteration 2, loss = 3.03734363\n",
      "Iteration 3, loss = 0.92762492\n",
      "Iteration 4, loss = 0.70089101\n",
      "Iteration 5, loss = 0.73398040\n",
      "Iteration 6, loss = 0.70491774\n",
      "Iteration 7, loss = 0.69925980\n",
      "Iteration 8, loss = 0.70883083\n",
      "Iteration 9, loss = 0.69231538\n",
      "Iteration 10, loss = 0.72085123\n",
      "Iteration 11, loss = 0.68931165\n",
      "Iteration 12, loss = 0.71260959\n",
      "Iteration 13, loss = 0.68849244\n",
      "Iteration 14, loss = 0.70796115\n",
      "Iteration 15, loss = 0.68848308\n",
      "Iteration 16, loss = 0.70310530\n",
      "Iteration 17, loss = 0.68764119\n",
      "Iteration 18, loss = 0.69964795\n",
      "Iteration 19, loss = 0.68687672\n",
      "Iteration 20, loss = 0.69654888\n",
      "Iteration 21, loss = 0.68591643\n",
      "Iteration 22, loss = 0.69397143\n",
      "Iteration 23, loss = 0.68482427\n",
      "Iteration 24, loss = 0.69168473\n",
      "Iteration 25, loss = 0.68367158\n",
      "Iteration 26, loss = 0.68961157\n",
      "Iteration 27, loss = 0.68252877\n",
      "Iteration 28, loss = 0.68757305\n",
      "Iteration 29, loss = 0.68150807\n",
      "Iteration 30, loss = 0.68547608\n",
      "Iteration 31, loss = 0.68069168\n",
      "Iteration 32, loss = 0.68321059\n",
      "Iteration 33, loss = 0.68008683\n",
      "Iteration 34, loss = 0.68081911\n",
      "Iteration 35, loss = 0.67959756\n",
      "Iteration 36, loss = 0.67847615\n",
      "Iteration 37, loss = 0.67894252\n",
      "Iteration 38, loss = 0.67649190\n",
      "Iteration 39, loss = 0.67777322\n",
      "Iteration 40, loss = 0.67514911\n",
      "Iteration 41, loss = 0.67592640\n",
      "Iteration 42, loss = 0.67438393\n",
      "Iteration 43, loss = 0.67370385\n",
      "Iteration 44, loss = 0.67362503\n",
      "Iteration 45, loss = 0.67182485\n",
      "Iteration 46, loss = 0.67217715\n",
      "Iteration 47, loss = 0.67070493\n",
      "Iteration 48, loss = 0.67008398\n",
      "Iteration 49, loss = 0.66976723\n",
      "Iteration 50, loss = 0.66827145\n",
      "Iteration 51, loss = 0.66810235\n",
      "Iteration 52, loss = 0.66715557\n",
      "Iteration 53, loss = 0.66604611\n",
      "Iteration 54, loss = 0.66576258\n",
      "Iteration 55, loss = 0.66462215\n",
      "Iteration 56, loss = 0.66370880\n",
      "Iteration 57, loss = 0.66324181\n",
      "Iteration 58, loss = 0.66211186\n",
      "Iteration 59, loss = 0.66119013\n",
      "Iteration 60, loss = 0.66060598\n",
      "Iteration 61, loss = 0.65955204\n",
      "Iteration 62, loss = 0.65851924\n",
      "Iteration 63, loss = 0.65782359\n",
      "Iteration 64, loss = 0.65689245\n",
      "Iteration 65, loss = 0.65576247\n",
      "Iteration 66, loss = 0.65486249\n",
      "Iteration 67, loss = 0.65402497\n",
      "Iteration 68, loss = 0.65296110\n",
      "Iteration 69, loss = 0.65183369\n",
      "Iteration 70, loss = 0.65084960\n",
      "Iteration 71, loss = 0.64990577\n",
      "Iteration 72, loss = 0.64883611\n",
      "Iteration 73, loss = 0.64766227\n",
      "Iteration 74, loss = 0.64650467\n",
      "Iteration 75, loss = 0.64540856\n",
      "Iteration 76, loss = 0.64432755\n",
      "Iteration 77, loss = 0.64320401\n",
      "Iteration 78, loss = 0.64201772\n",
      "Iteration 79, loss = 0.64078182\n",
      "Iteration 80, loss = 0.63951527\n",
      "Iteration 81, loss = 0.63823211\n",
      "Iteration 82, loss = 0.63693940\n",
      "Iteration 83, loss = 0.63564870\n",
      "Iteration 84, loss = 0.63438985\n",
      "Iteration 85, loss = 0.63326073\n",
      "Iteration 86, loss = 0.63256614\n",
      "Iteration 87, loss = 0.63333341\n",
      "Iteration 88, loss = 0.63897615\n",
      "Iteration 89, loss = 0.65966595\n",
      "Iteration 90, loss = 0.70801994\n",
      "Iteration 91, loss = 0.74606624\n",
      "Iteration 92, loss = 0.66659978\n",
      "Iteration 93, loss = 0.62447232\n",
      "Iteration 94, loss = 0.68193056\n",
      "Iteration 95, loss = 0.67807636\n",
      "Iteration 96, loss = 0.62107160\n",
      "Iteration 97, loss = 0.64919101\n",
      "Iteration 98, loss = 0.66407498\n",
      "Iteration 99, loss = 0.61992460\n",
      "Iteration 100, loss = 0.63494600\n",
      "Iteration 101, loss = 0.64946401\n",
      "Iteration 102, loss = 0.61678569\n",
      "Iteration 103, loss = 0.62662280\n",
      "Iteration 104, loss = 0.63829422\n",
      "Iteration 105, loss = 0.61348581\n",
      "Iteration 106, loss = 0.61978499\n",
      "Iteration 107, loss = 0.62892991\n",
      "Iteration 108, loss = 0.61037151\n",
      "Iteration 109, loss = 0.61318679\n",
      "Iteration 110, loss = 0.62134195\n",
      "Iteration 111, loss = 0.60785670\n",
      "Iteration 112, loss = 0.60665422\n",
      "Iteration 113, loss = 0.61441771\n",
      "Iteration 114, loss = 0.60605238\n",
      "Iteration 115, loss = 0.60073981\n",
      "Iteration 116, loss = 0.60689735\n",
      "Iteration 117, loss = 0.60448331\n",
      "Iteration 118, loss = 0.59678954\n",
      "Iteration 119, loss = 0.59833222\n",
      "Iteration 120, loss = 0.60096427\n",
      "Iteration 121, loss = 0.59578731\n",
      "Iteration 122, loss = 0.59136870\n",
      "Iteration 123, loss = 0.59301177\n",
      "Iteration 124, loss = 0.59393868\n",
      "Iteration 125, loss = 0.59005577\n",
      "Iteration 126, loss = 0.58592869\n",
      "Iteration 127, loss = 0.58543039\n",
      "Iteration 128, loss = 0.58647123\n",
      "Iteration 129, loss = 0.58553392\n",
      "Iteration 130, loss = 0.58231674\n",
      "Iteration 131, loss = 0.57888097\n",
      "Iteration 132, loss = 0.57683271\n",
      "Iteration 133, loss = 0.57618344\n",
      "Iteration 134, loss = 0.57618021\n",
      "Iteration 135, loss = 0.57625713\n",
      "Iteration 136, loss = 0.57627329\n",
      "Iteration 137, loss = 0.57658856\n",
      "Iteration 138, loss = 0.57774367\n",
      "Iteration 139, loss = 0.58085561\n",
      "Iteration 140, loss = 0.58707547\n",
      "Iteration 141, loss = 0.59785250\n",
      "Iteration 142, loss = 0.61053668\n",
      "Iteration 143, loss = 0.61763296\n",
      "Iteration 144, loss = 0.60670066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70389361\n",
      "Iteration 2, loss = 3.37495044\n",
      "Iteration 3, loss = 0.71839452\n",
      "Iteration 4, loss = 2.50287516\n",
      "Iteration 5, loss = 1.20101480\n",
      "Iteration 6, loss = 0.77155866\n",
      "Iteration 7, loss = 1.21371553\n",
      "Iteration 8, loss = 1.11046435\n",
      "Iteration 9, loss = 0.72277080\n",
      "Iteration 10, loss = 0.94294130\n",
      "Iteration 11, loss = 1.05957149\n",
      "Iteration 12, loss = 0.79528216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70167987\n",
      "Iteration 2, loss = 3.05118160\n",
      "Iteration 3, loss = 0.94160362\n",
      "Iteration 4, loss = 0.70134971\n",
      "Iteration 5, loss = 0.74334087\n",
      "Iteration 6, loss = 0.73088955\n",
      "Iteration 7, loss = 0.69523331\n",
      "Iteration 8, loss = 0.70758598\n",
      "Iteration 9, loss = 0.71064434\n",
      "Iteration 10, loss = 0.69401286\n",
      "Iteration 11, loss = 0.71176898\n",
      "Iteration 12, loss = 0.68932194\n",
      "Iteration 13, loss = 0.69217904\n",
      "Iteration 14, loss = 0.69287704\n",
      "Iteration 15, loss = 0.68883642\n",
      "Iteration 16, loss = 0.68713640\n",
      "Iteration 17, loss = 0.68942952\n",
      "Iteration 18, loss = 0.68825064\n",
      "Iteration 19, loss = 0.68665226\n",
      "Iteration 20, loss = 0.68536699\n",
      "Iteration 21, loss = 0.68454847\n",
      "Iteration 22, loss = 0.68537711\n",
      "Iteration 23, loss = 0.69028767\n",
      "Iteration 24, loss = 0.69560978\n",
      "Iteration 25, loss = 0.68909280\n",
      "Iteration 26, loss = 0.68207582\n",
      "Iteration 27, loss = 0.68748022\n",
      "Iteration 28, loss = 0.69004411\n",
      "Iteration 29, loss = 0.68256056\n",
      "Iteration 30, loss = 0.67991623\n",
      "Iteration 31, loss = 0.68446208\n",
      "Iteration 32, loss = 0.68406691\n",
      "Iteration 33, loss = 0.67832779\n",
      "Iteration 34, loss = 0.67706117\n",
      "Iteration 35, loss = 0.68007614\n",
      "Iteration 36, loss = 0.67976968\n",
      "Iteration 37, loss = 0.67550974\n",
      "Iteration 38, loss = 0.67330287\n",
      "Iteration 39, loss = 0.67468596\n",
      "Iteration 40, loss = 0.67573202\n",
      "Iteration 41, loss = 0.67383553\n",
      "Iteration 42, loss = 0.67064525\n",
      "Iteration 43, loss = 0.66899796\n",
      "Iteration 44, loss = 0.66931238\n",
      "Iteration 45, loss = 0.67008107\n",
      "Iteration 46, loss = 0.66989794\n",
      "Iteration 47, loss = 0.66838267\n",
      "Iteration 48, loss = 0.66617259\n",
      "Iteration 49, loss = 0.66404628\n",
      "Iteration 50, loss = 0.66219749\n",
      "Iteration 51, loss = 0.65952776\n",
      "Iteration 52, loss = 0.73044295\n",
      "Iteration 53, loss = 0.82132896\n",
      "Iteration 54, loss = 0.68947591\n",
      "Iteration 55, loss = 0.69314542\n",
      "Iteration 56, loss = 0.75900590\n",
      "Iteration 57, loss = 0.66532279\n",
      "Iteration 58, loss = 0.71767177\n",
      "Iteration 59, loss = 0.67504534\n",
      "Iteration 60, loss = 0.67548691\n",
      "Iteration 61, loss = 0.69069325\n",
      "Iteration 62, loss = 0.66757401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70008233\n",
      "Iteration 2, loss = 0.69443973\n",
      "Iteration 3, loss = 0.69386686\n",
      "Iteration 4, loss = 0.69406767\n",
      "Iteration 5, loss = 0.69347365\n",
      "Iteration 6, loss = 0.69277400\n",
      "Iteration 7, loss = 0.69236913\n",
      "Iteration 8, loss = 0.69220107\n",
      "Iteration 9, loss = 0.69208195\n",
      "Iteration 10, loss = 0.69194324\n",
      "Iteration 11, loss = 0.69172267\n",
      "Iteration 12, loss = 0.69153552\n",
      "Iteration 13, loss = 0.69137398\n",
      "Iteration 14, loss = 0.69121442\n",
      "Iteration 15, loss = 0.69100968\n",
      "Iteration 16, loss = 0.69087927\n",
      "Iteration 17, loss = 0.69071303\n",
      "Iteration 18, loss = 0.69055746\n",
      "Iteration 19, loss = 0.69041904\n",
      "Iteration 20, loss = 0.69031799\n",
      "Iteration 21, loss = 0.69006649\n",
      "Iteration 22, loss = 0.68990605\n",
      "Iteration 23, loss = 0.68974455\n",
      "Iteration 24, loss = 0.68956688\n",
      "Iteration 25, loss = 0.68939275\n",
      "Iteration 26, loss = 0.68921719\n",
      "Iteration 27, loss = 0.68908245\n",
      "Iteration 28, loss = 0.68887402\n",
      "Iteration 29, loss = 0.68868009\n",
      "Iteration 30, loss = 0.68852705\n",
      "Iteration 31, loss = 0.68834023\n",
      "Iteration 32, loss = 0.68815472\n",
      "Iteration 33, loss = 0.68798553\n",
      "Iteration 34, loss = 0.68779950\n",
      "Iteration 35, loss = 0.68763558\n",
      "Iteration 36, loss = 0.68745288\n",
      "Iteration 37, loss = 0.68727909\n",
      "Iteration 38, loss = 0.68710836\n",
      "Iteration 39, loss = 0.68692819\n",
      "Iteration 40, loss = 0.68672729\n",
      "Iteration 41, loss = 0.68654146\n",
      "Iteration 42, loss = 0.68638693\n",
      "Iteration 43, loss = 0.68617449\n",
      "Iteration 44, loss = 0.68600054\n",
      "Iteration 45, loss = 0.68580304\n",
      "Iteration 46, loss = 0.68561427\n",
      "Iteration 47, loss = 0.68542366\n",
      "Iteration 48, loss = 0.68526946\n",
      "Iteration 49, loss = 0.68506017\n",
      "Iteration 50, loss = 0.68488644\n",
      "Iteration 51, loss = 0.68474196\n",
      "Iteration 52, loss = 0.68450262\n",
      "Iteration 53, loss = 0.68433153\n",
      "Iteration 54, loss = 0.68410167\n",
      "Iteration 55, loss = 0.68392562\n",
      "Iteration 56, loss = 0.68376625\n",
      "Iteration 57, loss = 0.68354518\n",
      "Iteration 58, loss = 0.68334976\n",
      "Iteration 59, loss = 0.68318005\n",
      "Iteration 60, loss = 0.68298888\n",
      "Iteration 61, loss = 0.68279893\n",
      "Iteration 62, loss = 0.68260573\n",
      "Iteration 63, loss = 0.68241956\n",
      "Iteration 64, loss = 0.68221498\n",
      "Iteration 65, loss = 0.68201428\n",
      "Iteration 66, loss = 0.68186309\n",
      "Iteration 67, loss = 0.68164301\n",
      "Iteration 68, loss = 0.68149281\n",
      "Iteration 69, loss = 0.68124872\n",
      "Iteration 70, loss = 0.68106013\n",
      "Iteration 71, loss = 0.68086123\n",
      "Iteration 72, loss = 0.68068242\n",
      "Iteration 73, loss = 0.68048019\n",
      "Iteration 74, loss = 0.68026962\n",
      "Iteration 75, loss = 0.68015417\n",
      "Iteration 76, loss = 0.67992756\n",
      "Iteration 77, loss = 0.67971083\n",
      "Iteration 78, loss = 0.67952719\n",
      "Iteration 79, loss = 0.67932956\n",
      "Iteration 80, loss = 0.67914539\n",
      "Iteration 81, loss = 0.67891509\n",
      "Iteration 82, loss = 0.67871289\n",
      "Iteration 83, loss = 0.67851976\n",
      "Iteration 84, loss = 0.67844903\n",
      "Iteration 85, loss = 0.67833438\n",
      "Iteration 86, loss = 0.67792623\n",
      "Iteration 87, loss = 0.67777961\n",
      "Iteration 88, loss = 0.67757402\n",
      "Iteration 89, loss = 0.67739825\n",
      "Iteration 90, loss = 0.67732617\n",
      "Iteration 91, loss = 0.67690649\n",
      "Iteration 92, loss = 0.67670310\n",
      "Iteration 93, loss = 0.67646738\n",
      "Iteration 94, loss = 0.67624891\n",
      "Iteration 95, loss = 0.67606496\n",
      "Iteration 96, loss = 0.67596158\n",
      "Iteration 97, loss = 0.67598469\n",
      "Iteration 98, loss = 0.67555205\n",
      "Iteration 99, loss = 0.67541778\n",
      "Iteration 100, loss = 0.67539349\n",
      "Iteration 101, loss = 0.67503592\n",
      "Iteration 102, loss = 0.67462477\n",
      "Iteration 103, loss = 0.67442080\n",
      "Iteration 104, loss = 0.67425280\n",
      "Iteration 105, loss = 0.67409785\n",
      "Iteration 106, loss = 0.67403501\n",
      "Iteration 107, loss = 0.67359054\n",
      "Iteration 108, loss = 0.67355800\n",
      "Iteration 109, loss = 0.67347385\n",
      "Iteration 110, loss = 0.67309246\n",
      "Iteration 111, loss = 0.67272353\n",
      "Iteration 112, loss = 0.67252067\n",
      "Iteration 113, loss = 0.67229462\n",
      "Iteration 114, loss = 0.67209149\n",
      "Iteration 115, loss = 0.67194636\n",
      "Iteration 116, loss = 0.67174226\n",
      "Iteration 117, loss = 0.67164184\n",
      "Iteration 118, loss = 0.67122518\n",
      "Iteration 119, loss = 0.67104462\n",
      "Iteration 120, loss = 0.67080209\n",
      "Iteration 121, loss = 0.67068903\n",
      "Iteration 122, loss = 0.67063465\n",
      "Iteration 123, loss = 0.67018901\n",
      "Iteration 124, loss = 0.66999991\n",
      "Iteration 125, loss = 0.66974246\n",
      "Iteration 126, loss = 0.66968014\n",
      "Iteration 127, loss = 0.66962439\n",
      "Iteration 128, loss = 0.66930629\n",
      "Iteration 129, loss = 0.66898199\n",
      "Iteration 130, loss = 0.66870528\n",
      "Iteration 131, loss = 0.66848389\n",
      "Iteration 132, loss = 0.66822171\n",
      "Iteration 133, loss = 0.66799979\n",
      "Iteration 134, loss = 0.66782669\n",
      "Iteration 135, loss = 0.66766637\n",
      "Iteration 136, loss = 0.66731980\n",
      "Iteration 137, loss = 0.66707064\n",
      "Iteration 138, loss = 0.66687117\n",
      "Iteration 139, loss = 0.66666739\n",
      "Iteration 140, loss = 0.66641839\n",
      "Iteration 141, loss = 0.66619851\n",
      "Iteration 142, loss = 0.66606109\n",
      "Iteration 143, loss = 0.66601550\n",
      "Iteration 144, loss = 0.66570764\n",
      "Iteration 145, loss = 0.66539126\n",
      "Iteration 146, loss = 0.66494417\n",
      "Iteration 147, loss = 0.66486385\n",
      "Iteration 148, loss = 0.66482156\n",
      "Iteration 149, loss = 0.66442897\n",
      "Iteration 150, loss = 0.66400634\n",
      "Iteration 151, loss = 0.66390850\n",
      "Iteration 152, loss = 0.66378474\n",
      "Iteration 153, loss = 0.66337562\n",
      "Iteration 154, loss = 0.66299023\n",
      "Iteration 155, loss = 0.66272881\n",
      "Iteration 156, loss = 0.66270833\n",
      "Iteration 157, loss = 0.66262492\n",
      "Iteration 158, loss = 0.66227628\n",
      "Iteration 159, loss = 0.66187007\n",
      "Iteration 160, loss = 0.66155841\n",
      "Iteration 161, loss = 0.66127358\n",
      "Iteration 162, loss = 0.66109915\n",
      "Iteration 163, loss = 0.66087109\n",
      "Iteration 164, loss = 0.66054490\n",
      "Iteration 165, loss = 0.66033997\n",
      "Iteration 166, loss = 0.65999294\n",
      "Iteration 167, loss = 0.65973075\n",
      "Iteration 168, loss = 0.65956401\n",
      "Iteration 169, loss = 0.65932258\n",
      "Iteration 170, loss = 0.65895946\n",
      "Iteration 171, loss = 0.65880694\n",
      "Iteration 172, loss = 0.65841029\n",
      "Iteration 173, loss = 0.65823922\n",
      "Iteration 174, loss = 0.65799817\n",
      "Iteration 175, loss = 0.65767987\n",
      "Iteration 176, loss = 0.65740937\n",
      "Iteration 177, loss = 0.65721866\n",
      "Iteration 178, loss = 0.65696753\n",
      "Iteration 179, loss = 0.65655458\n",
      "Iteration 180, loss = 0.65646685\n",
      "Iteration 181, loss = 0.65637796\n",
      "Iteration 182, loss = 0.65602162\n",
      "Iteration 183, loss = 0.65562853\n",
      "Iteration 184, loss = 0.65519965\n",
      "Iteration 185, loss = 0.65491564\n",
      "Iteration 186, loss = 0.65492981\n",
      "Iteration 187, loss = 0.65475848\n",
      "Iteration 188, loss = 0.65437021\n",
      "Iteration 189, loss = 0.65398340\n",
      "Iteration 190, loss = 0.65352675\n",
      "Iteration 191, loss = 0.65323614\n",
      "Iteration 192, loss = 0.65308918\n",
      "Iteration 193, loss = 0.65280395\n",
      "Iteration 194, loss = 0.65238741\n",
      "Iteration 195, loss = 0.65221354\n",
      "Iteration 196, loss = 0.65196206\n",
      "Iteration 197, loss = 0.65151019\n",
      "Iteration 198, loss = 0.65135942\n",
      "Iteration 199, loss = 0.65105379\n",
      "Iteration 200, loss = 0.65065129\n",
      "Iteration 201, loss = 0.65045531\n",
      "Iteration 202, loss = 0.65018047\n",
      "Iteration 203, loss = 0.64977002\n",
      "Iteration 204, loss = 0.64954642\n",
      "Iteration 205, loss = 0.64930873\n",
      "Iteration 206, loss = 0.64886867\n",
      "Iteration 207, loss = 0.64864746\n",
      "Iteration 208, loss = 0.64827347\n",
      "Iteration 209, loss = 0.64800599\n",
      "Iteration 210, loss = 0.64776318\n",
      "Iteration 211, loss = 0.64757816\n",
      "Iteration 212, loss = 0.64704384\n",
      "Iteration 213, loss = 0.64673517\n",
      "Iteration 214, loss = 0.64649395\n",
      "Iteration 215, loss = 0.64609618\n",
      "Iteration 216, loss = 0.64582188\n",
      "Iteration 217, loss = 0.64555661\n",
      "Iteration 218, loss = 0.64541286\n",
      "Iteration 219, loss = 0.64493460\n",
      "Iteration 220, loss = 0.64451313\n",
      "Iteration 221, loss = 0.64418049\n",
      "Iteration 222, loss = 0.64386477\n",
      "Iteration 223, loss = 0.64366934\n",
      "Iteration 224, loss = 0.64354499\n",
      "Iteration 225, loss = 0.64300422\n",
      "Iteration 226, loss = 0.64262352\n",
      "Iteration 227, loss = 0.64235615\n",
      "Iteration 228, loss = 0.64188818\n",
      "Iteration 229, loss = 0.64154828\n",
      "Iteration 230, loss = 0.64123554\n",
      "Iteration 231, loss = 0.64097930\n",
      "Iteration 232, loss = 0.64056377\n",
      "Iteration 233, loss = 0.64023892\n",
      "Iteration 234, loss = 0.63986880\n",
      "Iteration 235, loss = 0.63951741\n",
      "Iteration 236, loss = 0.63917042\n",
      "Iteration 237, loss = 0.63886654\n",
      "Iteration 238, loss = 0.63855022\n",
      "Iteration 239, loss = 0.63831208\n",
      "Iteration 240, loss = 0.63779354\n",
      "Iteration 241, loss = 0.63748900\n",
      "Iteration 242, loss = 0.63708567\n",
      "Iteration 243, loss = 0.63686652\n",
      "Iteration 244, loss = 0.63639161\n",
      "Iteration 245, loss = 0.63603472\n",
      "Iteration 246, loss = 0.63570251\n",
      "Iteration 247, loss = 0.63537699\n",
      "Iteration 248, loss = 0.63493082\n",
      "Iteration 249, loss = 0.63469502\n",
      "Iteration 250, loss = 0.63420172\n",
      "Iteration 251, loss = 0.63394315\n",
      "Iteration 252, loss = 0.63349696\n",
      "Iteration 253, loss = 0.63320251\n",
      "Iteration 254, loss = 0.63267193\n",
      "Iteration 255, loss = 0.63225376\n",
      "Iteration 256, loss = 0.63186722\n",
      "Iteration 257, loss = 0.63148420\n",
      "Iteration 258, loss = 0.63112360\n",
      "Iteration 259, loss = 0.63093197\n",
      "Iteration 260, loss = 0.63037864\n",
      "Iteration 261, loss = 0.63007074\n",
      "Iteration 262, loss = 0.62970475\n",
      "Iteration 263, loss = 0.62918202\n",
      "Iteration 264, loss = 0.62873935\n",
      "Iteration 265, loss = 0.62845765\n",
      "Iteration 266, loss = 0.62822803\n",
      "Iteration 267, loss = 0.62759710\n",
      "Iteration 268, loss = 0.62712567\n",
      "Iteration 269, loss = 0.62671412\n",
      "Iteration 270, loss = 0.62629653\n",
      "Iteration 271, loss = 0.62588265\n",
      "Iteration 272, loss = 0.62558173\n",
      "Iteration 273, loss = 0.62541904\n",
      "Iteration 274, loss = 0.62480324\n",
      "Iteration 275, loss = 0.62427250\n",
      "Iteration 276, loss = 0.62391132\n",
      "Iteration 277, loss = 0.62344610\n",
      "Iteration 278, loss = 0.62297693\n",
      "Iteration 279, loss = 0.62260002\n",
      "Iteration 280, loss = 0.62213764\n",
      "Iteration 281, loss = 0.62166974\n",
      "Iteration 282, loss = 0.62127700\n",
      "Iteration 283, loss = 0.62093926\n",
      "Iteration 284, loss = 0.62047037\n",
      "Iteration 285, loss = 0.62000867\n",
      "Iteration 286, loss = 0.61949671\n",
      "Iteration 287, loss = 0.61906187\n",
      "Iteration 288, loss = 0.61858567\n",
      "Iteration 289, loss = 0.61809221\n",
      "Iteration 290, loss = 0.61768070\n",
      "Iteration 291, loss = 0.61722366\n",
      "Iteration 292, loss = 0.61678838\n",
      "Iteration 293, loss = 0.61628502\n",
      "Iteration 294, loss = 0.61576935\n",
      "Iteration 295, loss = 0.61531273\n",
      "Iteration 296, loss = 0.61484258\n",
      "Iteration 297, loss = 0.61433517\n",
      "Iteration 298, loss = 0.61388331\n",
      "Iteration 299, loss = 0.61342731\n",
      "Iteration 300, loss = 0.61300735\n",
      "Iteration 1, loss = 0.69905928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69171103\n",
      "Iteration 3, loss = 0.69072116\n",
      "Iteration 4, loss = 0.69120972\n",
      "Iteration 5, loss = 0.69073265\n",
      "Iteration 6, loss = 0.69012114\n",
      "Iteration 7, loss = 0.68980609\n",
      "Iteration 8, loss = 0.68970972\n",
      "Iteration 9, loss = 0.68948138\n",
      "Iteration 10, loss = 0.68918299\n",
      "Iteration 11, loss = 0.68916709\n",
      "Iteration 12, loss = 0.68887466\n",
      "Iteration 13, loss = 0.68876256\n",
      "Iteration 14, loss = 0.68859525\n",
      "Iteration 15, loss = 0.68849266\n",
      "Iteration 16, loss = 0.68841520\n",
      "Iteration 17, loss = 0.68821593\n",
      "Iteration 18, loss = 0.68799477\n",
      "Iteration 19, loss = 0.68791164\n",
      "Iteration 20, loss = 0.68772193\n",
      "Iteration 21, loss = 0.68762070\n",
      "Iteration 22, loss = 0.68749887\n",
      "Iteration 23, loss = 0.68740794\n",
      "Iteration 24, loss = 0.68727239\n",
      "Iteration 25, loss = 0.68711809\n",
      "Iteration 26, loss = 0.68692346\n",
      "Iteration 27, loss = 0.68684312\n",
      "Iteration 28, loss = 0.68665842\n",
      "Iteration 29, loss = 0.68657084\n",
      "Iteration 30, loss = 0.68642499\n",
      "Iteration 31, loss = 0.68636223\n",
      "Iteration 32, loss = 0.68617165\n",
      "Iteration 33, loss = 0.68604844\n",
      "Iteration 34, loss = 0.68580929\n",
      "Iteration 35, loss = 0.68573630\n",
      "Iteration 36, loss = 0.68552639\n",
      "Iteration 37, loss = 0.68541874\n",
      "Iteration 38, loss = 0.68529157\n",
      "Iteration 39, loss = 0.68514528\n",
      "Iteration 40, loss = 0.68503075\n",
      "Iteration 41, loss = 0.68485196\n",
      "Iteration 42, loss = 0.68472144\n",
      "Iteration 43, loss = 0.68461228\n",
      "Iteration 44, loss = 0.68444782\n",
      "Iteration 45, loss = 0.68423511\n",
      "Iteration 46, loss = 0.68414779\n",
      "Iteration 47, loss = 0.68401191\n",
      "Iteration 48, loss = 0.68391845\n",
      "Iteration 49, loss = 0.68375411\n",
      "Iteration 50, loss = 0.68362378\n",
      "Iteration 51, loss = 0.68343658\n",
      "Iteration 52, loss = 0.68328693\n",
      "Iteration 53, loss = 0.68313940\n",
      "Iteration 54, loss = 0.68304471\n",
      "Iteration 55, loss = 0.68290468\n",
      "Iteration 56, loss = 0.68269420\n",
      "Iteration 57, loss = 0.68259389\n",
      "Iteration 58, loss = 0.68244223\n",
      "Iteration 59, loss = 0.68230048\n",
      "Iteration 60, loss = 0.68212105\n",
      "Iteration 61, loss = 0.68188106\n",
      "Iteration 62, loss = 0.68172783\n",
      "Iteration 63, loss = 0.68161268\n",
      "Iteration 64, loss = 0.68152255\n",
      "Iteration 65, loss = 0.68142320\n",
      "Iteration 66, loss = 0.68119235\n",
      "Iteration 67, loss = 0.68106929\n",
      "Iteration 68, loss = 0.68091218\n",
      "Iteration 69, loss = 0.68077936\n",
      "Iteration 70, loss = 0.68062135\n",
      "Iteration 71, loss = 0.68055291\n",
      "Iteration 72, loss = 0.68028689\n",
      "Iteration 73, loss = 0.68019749\n",
      "Iteration 74, loss = 0.67999512\n",
      "Iteration 75, loss = 0.67996751\n",
      "Iteration 76, loss = 0.67969667\n",
      "Iteration 77, loss = 0.67958340\n",
      "Iteration 78, loss = 0.67956667\n",
      "Iteration 79, loss = 0.67923841\n",
      "Iteration 80, loss = 0.67913942\n",
      "Iteration 81, loss = 0.67902235\n",
      "Iteration 82, loss = 0.67877053\n",
      "Iteration 83, loss = 0.67870376\n",
      "Iteration 84, loss = 0.67850846\n",
      "Iteration 85, loss = 0.67847474\n",
      "Iteration 86, loss = 0.67809166\n",
      "Iteration 87, loss = 0.67791834\n",
      "Iteration 88, loss = 0.67777263\n",
      "Iteration 89, loss = 0.67766909\n",
      "Iteration 90, loss = 0.67757285\n",
      "Iteration 91, loss = 0.67742749\n",
      "Iteration 92, loss = 0.67730814\n",
      "Iteration 93, loss = 0.67716935\n",
      "Iteration 94, loss = 0.67686046\n",
      "Iteration 95, loss = 0.67666530\n",
      "Iteration 96, loss = 0.67660048\n",
      "Iteration 97, loss = 0.67658138\n",
      "Iteration 98, loss = 0.67637205\n",
      "Iteration 99, loss = 0.67630094\n",
      "Iteration 100, loss = 0.67617817\n",
      "Iteration 101, loss = 0.67578157\n",
      "Iteration 102, loss = 0.67556836\n",
      "Iteration 103, loss = 0.67555243\n",
      "Iteration 104, loss = 0.67532838\n",
      "Iteration 105, loss = 0.67524503\n",
      "Iteration 106, loss = 0.67495936\n",
      "Iteration 107, loss = 0.67486756\n",
      "Iteration 108, loss = 0.67464844\n",
      "Iteration 109, loss = 0.67454687\n",
      "Iteration 110, loss = 0.67428441\n",
      "Iteration 111, loss = 0.67420933\n",
      "Iteration 112, loss = 0.67404571\n",
      "Iteration 113, loss = 0.67391375\n",
      "Iteration 114, loss = 0.67358862\n",
      "Iteration 115, loss = 0.67351487\n",
      "Iteration 116, loss = 0.67325927\n",
      "Iteration 117, loss = 0.67311568\n",
      "Iteration 118, loss = 0.67302172\n",
      "Iteration 119, loss = 0.67301759\n",
      "Iteration 120, loss = 0.67269954\n",
      "Iteration 121, loss = 0.67260611\n",
      "Iteration 122, loss = 0.67227456\n",
      "Iteration 123, loss = 0.67223205\n",
      "Iteration 124, loss = 0.67197463\n",
      "Iteration 125, loss = 0.67187325\n",
      "Iteration 126, loss = 0.67163981\n",
      "Iteration 127, loss = 0.67164841\n",
      "Iteration 128, loss = 0.67144924\n",
      "Iteration 129, loss = 0.67143341\n",
      "Iteration 130, loss = 0.67092916\n",
      "Iteration 131, loss = 0.67081336\n",
      "Iteration 132, loss = 0.67063141\n",
      "Iteration 133, loss = 0.67044543\n",
      "Iteration 134, loss = 0.67030089\n",
      "Iteration 135, loss = 0.67025215\n",
      "Iteration 136, loss = 0.66996610\n",
      "Iteration 137, loss = 0.66993700\n",
      "Iteration 138, loss = 0.66952697\n",
      "Iteration 139, loss = 0.66945353\n",
      "Iteration 140, loss = 0.66921982\n",
      "Iteration 141, loss = 0.66927622\n",
      "Iteration 142, loss = 0.66885192\n",
      "Iteration 143, loss = 0.66893156\n",
      "Iteration 144, loss = 0.66851976\n",
      "Iteration 145, loss = 0.66879976\n",
      "Iteration 146, loss = 0.66811577\n",
      "Iteration 147, loss = 0.66813495\n",
      "Iteration 148, loss = 0.66795566\n",
      "Iteration 149, loss = 0.66785537\n",
      "Iteration 150, loss = 0.66727099\n",
      "Iteration 151, loss = 0.66719339\n",
      "Iteration 152, loss = 0.66698964\n",
      "Iteration 153, loss = 0.66710286\n",
      "Iteration 154, loss = 0.66658729\n",
      "Iteration 155, loss = 0.66650532\n",
      "Iteration 156, loss = 0.66626621\n",
      "Iteration 157, loss = 0.66639027\n",
      "Iteration 158, loss = 0.66581207\n",
      "Iteration 159, loss = 0.66584557\n",
      "Iteration 160, loss = 0.66557216\n",
      "Iteration 161, loss = 0.66539285\n",
      "Iteration 162, loss = 0.66547088\n",
      "Iteration 163, loss = 0.66547088\n",
      "Iteration 164, loss = 0.66456113\n",
      "Iteration 165, loss = 0.66437838\n",
      "Iteration 166, loss = 0.66440672\n",
      "Iteration 167, loss = 0.66442291\n",
      "Iteration 168, loss = 0.66404053\n",
      "Iteration 169, loss = 0.66384976\n",
      "Iteration 170, loss = 0.66371045\n",
      "Iteration 171, loss = 0.66371953\n",
      "Iteration 172, loss = 0.66307762\n",
      "Iteration 173, loss = 0.66292773\n",
      "Iteration 174, loss = 0.66274860\n",
      "Iteration 175, loss = 0.66273729\n",
      "Iteration 176, loss = 0.66236064\n",
      "Iteration 177, loss = 0.66246368\n",
      "Iteration 178, loss = 0.66187086\n",
      "Iteration 179, loss = 0.66198218\n",
      "Iteration 180, loss = 0.66166747\n",
      "Iteration 181, loss = 0.66162331\n",
      "Iteration 182, loss = 0.66097657\n",
      "Iteration 183, loss = 0.66086030\n",
      "Iteration 184, loss = 0.66081344\n",
      "Iteration 185, loss = 0.66097321\n",
      "Iteration 186, loss = 0.66041038\n",
      "Iteration 187, loss = 0.66044316\n",
      "Iteration 188, loss = 0.65977446\n",
      "Iteration 189, loss = 0.65967015\n",
      "Iteration 190, loss = 0.65952662\n",
      "Iteration 191, loss = 0.65932022\n",
      "Iteration 192, loss = 0.65907224\n",
      "Iteration 193, loss = 0.65892618\n",
      "Iteration 194, loss = 0.65876366\n",
      "Iteration 195, loss = 0.65843005\n",
      "Iteration 196, loss = 0.65812633\n",
      "Iteration 197, loss = 0.65810144\n",
      "Iteration 198, loss = 0.65788106\n",
      "Iteration 199, loss = 0.65771838\n",
      "Iteration 200, loss = 0.65726797\n",
      "Iteration 201, loss = 0.65712835\n",
      "Iteration 202, loss = 0.65683345\n",
      "Iteration 203, loss = 0.65673313\n",
      "Iteration 204, loss = 0.65641435\n",
      "Iteration 205, loss = 0.65625719\n",
      "Iteration 206, loss = 0.65600734\n",
      "Iteration 207, loss = 0.65578420\n",
      "Iteration 208, loss = 0.65558744\n",
      "Iteration 209, loss = 0.65546271\n",
      "Iteration 210, loss = 0.65498334\n",
      "Iteration 211, loss = 0.65496606\n",
      "Iteration 212, loss = 0.65487763\n",
      "Iteration 213, loss = 0.65472952\n",
      "Iteration 214, loss = 0.65389843\n",
      "Iteration 215, loss = 0.65374770\n",
      "Iteration 216, loss = 0.65382279\n",
      "Iteration 217, loss = 0.65375213\n",
      "Iteration 218, loss = 0.65293000\n",
      "Iteration 219, loss = 0.65263117\n",
      "Iteration 220, loss = 0.65248575\n",
      "Iteration 221, loss = 0.65222106\n",
      "Iteration 222, loss = 0.65210445\n",
      "Iteration 223, loss = 0.65178353\n",
      "Iteration 224, loss = 0.65149134\n",
      "Iteration 225, loss = 0.65136988\n",
      "Iteration 226, loss = 0.65159567\n",
      "Iteration 227, loss = 0.65078090\n",
      "Iteration 228, loss = 0.65073545\n",
      "Iteration 229, loss = 0.65041464\n",
      "Iteration 230, loss = 0.65050154\n",
      "Iteration 231, loss = 0.64966174\n",
      "Iteration 232, loss = 0.64987162\n",
      "Iteration 233, loss = 0.64970454\n",
      "Iteration 234, loss = 0.64948676\n",
      "Iteration 235, loss = 0.64892082\n",
      "Iteration 236, loss = 0.64908517\n",
      "Iteration 237, loss = 0.64841057\n",
      "Iteration 238, loss = 0.64859438\n",
      "Iteration 239, loss = 0.64818970\n",
      "Iteration 240, loss = 0.64832208\n",
      "Iteration 241, loss = 0.64727449\n",
      "Iteration 242, loss = 0.64712529\n",
      "Iteration 243, loss = 0.64700302\n",
      "Iteration 244, loss = 0.64709895\n",
      "Iteration 245, loss = 0.64615258\n",
      "Iteration 246, loss = 0.64656382\n",
      "Iteration 247, loss = 0.64607192\n",
      "Iteration 248, loss = 0.64634734\n",
      "Iteration 249, loss = 0.64497787\n",
      "Iteration 250, loss = 0.64484795\n",
      "Iteration 251, loss = 0.64504118\n",
      "Iteration 252, loss = 0.64475788\n",
      "Iteration 253, loss = 0.64382985\n",
      "Iteration 254, loss = 0.64379012\n",
      "Iteration 255, loss = 0.64355690\n",
      "Iteration 256, loss = 0.64399691\n",
      "Iteration 257, loss = 0.64272996\n",
      "Iteration 258, loss = 0.64245016\n",
      "Iteration 259, loss = 0.64229956\n",
      "Iteration 260, loss = 0.64226032\n",
      "Iteration 261, loss = 0.64185418\n",
      "Iteration 262, loss = 0.64134331\n",
      "Iteration 263, loss = 0.64158724\n",
      "Iteration 264, loss = 0.64101343\n",
      "Iteration 265, loss = 0.64098955\n",
      "Iteration 266, loss = 0.64081090\n",
      "Iteration 267, loss = 0.64044667\n",
      "Iteration 268, loss = 0.63962918\n",
      "Iteration 269, loss = 0.63994228\n",
      "Iteration 270, loss = 0.63942207\n",
      "Iteration 271, loss = 0.63925107\n",
      "Iteration 272, loss = 0.63844086\n",
      "Iteration 273, loss = 0.63823824\n",
      "Iteration 274, loss = 0.63809798\n",
      "Iteration 275, loss = 0.63828625\n",
      "Iteration 276, loss = 0.63706056\n",
      "Iteration 277, loss = 0.63742314\n",
      "Iteration 278, loss = 0.63714983\n",
      "Iteration 279, loss = 0.63670734\n",
      "Iteration 280, loss = 0.63583359\n",
      "Iteration 281, loss = 0.63618856\n",
      "Iteration 282, loss = 0.63544343\n",
      "Iteration 283, loss = 0.63570454\n",
      "Iteration 284, loss = 0.63492257\n",
      "Iteration 285, loss = 0.63508436\n",
      "Iteration 286, loss = 0.63411892\n",
      "Iteration 287, loss = 0.63352494\n",
      "Iteration 288, loss = 0.63362874\n",
      "Iteration 289, loss = 0.63341829\n",
      "Iteration 290, loss = 0.63275137\n",
      "Iteration 291, loss = 0.63291956\n",
      "Iteration 292, loss = 0.63194879\n",
      "Iteration 293, loss = 0.63164270\n",
      "Iteration 294, loss = 0.63126880\n",
      "Iteration 295, loss = 0.63126790\n",
      "Iteration 296, loss = 0.63104495\n",
      "Iteration 297, loss = 0.63030175\n",
      "Iteration 298, loss = 0.63068219\n",
      "Iteration 299, loss = 0.62939668\n",
      "Iteration 300, loss = 0.62932037\n",
      "Iteration 1, loss = 0.69831745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69177098\n",
      "Iteration 3, loss = 0.69099511\n",
      "Iteration 4, loss = 0.69129557\n",
      "Iteration 5, loss = 0.69067843\n",
      "Iteration 6, loss = 0.69005501\n",
      "Iteration 7, loss = 0.68960973\n",
      "Iteration 8, loss = 0.68959467\n",
      "Iteration 9, loss = 0.68949533\n",
      "Iteration 10, loss = 0.68921773\n",
      "Iteration 11, loss = 0.68900534\n",
      "Iteration 12, loss = 0.68886835\n",
      "Iteration 13, loss = 0.68862882\n",
      "Iteration 14, loss = 0.68860526\n",
      "Iteration 15, loss = 0.68850182\n",
      "Iteration 16, loss = 0.68835855\n",
      "Iteration 17, loss = 0.68815041\n",
      "Iteration 18, loss = 0.68813044\n",
      "Iteration 19, loss = 0.68785870\n",
      "Iteration 20, loss = 0.68776901\n",
      "Iteration 21, loss = 0.68767046\n",
      "Iteration 22, loss = 0.68756070\n",
      "Iteration 23, loss = 0.68734555\n",
      "Iteration 24, loss = 0.68718803\n",
      "Iteration 25, loss = 0.68705465\n",
      "Iteration 26, loss = 0.68699641\n",
      "Iteration 27, loss = 0.68680304\n",
      "Iteration 28, loss = 0.68685383\n",
      "Iteration 29, loss = 0.68644915\n",
      "Iteration 30, loss = 0.68636148\n",
      "Iteration 31, loss = 0.68627000\n",
      "Iteration 32, loss = 0.68614620\n",
      "Iteration 33, loss = 0.68597743\n",
      "Iteration 34, loss = 0.68598807\n",
      "Iteration 35, loss = 0.68584502\n",
      "Iteration 36, loss = 0.68567813\n",
      "Iteration 37, loss = 0.68542052\n",
      "Iteration 38, loss = 0.68516923\n",
      "Iteration 39, loss = 0.68496246\n",
      "Iteration 40, loss = 0.68487804\n",
      "Iteration 41, loss = 0.68472403\n",
      "Iteration 42, loss = 0.68457044\n",
      "Iteration 43, loss = 0.68462456\n",
      "Iteration 44, loss = 0.68425562\n",
      "Iteration 45, loss = 0.68413990\n",
      "Iteration 46, loss = 0.68398969\n",
      "Iteration 47, loss = 0.68378311\n",
      "Iteration 48, loss = 0.68367265\n",
      "Iteration 49, loss = 0.68357840\n",
      "Iteration 50, loss = 0.68358334\n",
      "Iteration 51, loss = 0.68329164\n",
      "Iteration 52, loss = 0.68320000\n",
      "Iteration 53, loss = 0.68303779\n",
      "Iteration 54, loss = 0.68293564\n",
      "Iteration 55, loss = 0.68256240\n",
      "Iteration 56, loss = 0.68235643\n",
      "Iteration 57, loss = 0.68227237\n",
      "Iteration 58, loss = 0.68227938\n",
      "Iteration 59, loss = 0.68217219\n",
      "Iteration 60, loss = 0.68181138\n",
      "Iteration 61, loss = 0.68164772\n",
      "Iteration 62, loss = 0.68143721\n",
      "Iteration 63, loss = 0.68130455\n",
      "Iteration 64, loss = 0.68127790\n",
      "Iteration 65, loss = 0.68131434\n",
      "Iteration 66, loss = 0.68115629\n",
      "Iteration 67, loss = 0.68083119\n",
      "Iteration 68, loss = 0.68052890\n",
      "Iteration 69, loss = 0.68037598\n",
      "Iteration 70, loss = 0.68029339\n",
      "Iteration 71, loss = 0.68000785\n",
      "Iteration 72, loss = 0.67987965\n",
      "Iteration 73, loss = 0.67976750\n",
      "Iteration 74, loss = 0.67951386\n",
      "Iteration 75, loss = 0.67944050\n",
      "Iteration 76, loss = 0.67920262\n",
      "Iteration 77, loss = 0.67913082\n",
      "Iteration 78, loss = 0.67888181\n",
      "Iteration 79, loss = 0.67879531\n",
      "Iteration 80, loss = 0.67857102\n",
      "Iteration 81, loss = 0.67848923\n",
      "Iteration 82, loss = 0.67824124\n",
      "Iteration 83, loss = 0.67812704\n",
      "Iteration 84, loss = 0.67799764\n",
      "Iteration 85, loss = 0.67782245\n",
      "Iteration 86, loss = 0.67767521\n",
      "Iteration 87, loss = 0.67754595\n",
      "Iteration 88, loss = 0.67750680\n",
      "Iteration 89, loss = 0.67744666\n",
      "Iteration 90, loss = 0.67712289\n",
      "Iteration 91, loss = 0.67720101\n",
      "Iteration 92, loss = 0.67661716\n",
      "Iteration 93, loss = 0.67649177\n",
      "Iteration 94, loss = 0.67629369\n",
      "Iteration 95, loss = 0.67613335\n",
      "Iteration 96, loss = 0.67605252\n",
      "Iteration 97, loss = 0.67595967\n",
      "Iteration 98, loss = 0.67619917\n",
      "Iteration 99, loss = 0.67579442\n",
      "Iteration 100, loss = 0.67526578\n",
      "Iteration 101, loss = 0.67509193\n",
      "Iteration 102, loss = 0.67498865\n",
      "Iteration 103, loss = 0.67472672\n",
      "Iteration 104, loss = 0.67457795\n",
      "Iteration 105, loss = 0.67455411\n",
      "Iteration 106, loss = 0.67456349\n",
      "Iteration 107, loss = 0.67425559\n",
      "Iteration 108, loss = 0.67422094\n",
      "Iteration 109, loss = 0.67408521\n",
      "Iteration 110, loss = 0.67367690\n",
      "Iteration 111, loss = 0.67368536\n",
      "Iteration 112, loss = 0.67344273\n",
      "Iteration 113, loss = 0.67342002\n",
      "Iteration 114, loss = 0.67281877\n",
      "Iteration 115, loss = 0.67266708\n",
      "Iteration 116, loss = 0.67254808\n",
      "Iteration 117, loss = 0.67223856\n",
      "Iteration 118, loss = 0.67214633\n",
      "Iteration 119, loss = 0.67230921\n",
      "Iteration 120, loss = 0.67189389\n",
      "Iteration 121, loss = 0.67168370\n",
      "Iteration 122, loss = 0.67193068\n",
      "Iteration 123, loss = 0.67160126\n",
      "Iteration 124, loss = 0.67122248\n",
      "Iteration 125, loss = 0.67121363\n",
      "Iteration 126, loss = 0.67118449\n",
      "Iteration 127, loss = 0.67047746\n",
      "Iteration 128, loss = 0.67046236\n",
      "Iteration 129, loss = 0.67069974\n",
      "Iteration 130, loss = 0.67046065\n",
      "Iteration 131, loss = 0.66998396\n",
      "Iteration 132, loss = 0.66997911\n",
      "Iteration 133, loss = 0.66977684\n",
      "Iteration 134, loss = 0.66927690\n",
      "Iteration 135, loss = 0.66941345\n",
      "Iteration 136, loss = 0.66887551\n",
      "Iteration 137, loss = 0.66859575\n",
      "Iteration 138, loss = 0.66857395\n",
      "Iteration 139, loss = 0.66869202\n",
      "Iteration 140, loss = 0.66830807\n",
      "Iteration 141, loss = 0.66794235\n",
      "Iteration 142, loss = 0.66768342\n",
      "Iteration 143, loss = 0.66778478\n",
      "Iteration 144, loss = 0.66766763\n",
      "Iteration 145, loss = 0.66714415\n",
      "Iteration 146, loss = 0.66697800\n",
      "Iteration 147, loss = 0.66724362\n",
      "Iteration 148, loss = 0.66677110\n",
      "Iteration 149, loss = 0.66638823\n",
      "Iteration 150, loss = 0.66659828\n",
      "Iteration 151, loss = 0.66604352\n",
      "Iteration 152, loss = 0.66590786\n",
      "Iteration 153, loss = 0.66596420\n",
      "Iteration 154, loss = 0.66542638\n",
      "Iteration 155, loss = 0.66543427\n",
      "Iteration 156, loss = 0.66529606\n",
      "Iteration 157, loss = 0.66487517\n",
      "Iteration 158, loss = 0.66497128\n",
      "Iteration 159, loss = 0.66468486\n",
      "Iteration 160, loss = 0.66414565\n",
      "Iteration 161, loss = 0.66388282\n",
      "Iteration 162, loss = 0.66371954\n",
      "Iteration 163, loss = 0.66354589\n",
      "Iteration 164, loss = 0.66358835\n",
      "Iteration 165, loss = 0.66351633\n",
      "Iteration 166, loss = 0.66310712\n",
      "Iteration 167, loss = 0.66265115\n",
      "Iteration 168, loss = 0.66247568\n",
      "Iteration 169, loss = 0.66279372\n",
      "Iteration 170, loss = 0.66222521\n",
      "Iteration 171, loss = 0.66178416\n",
      "Iteration 172, loss = 0.66151026\n",
      "Iteration 173, loss = 0.66132336\n",
      "Iteration 174, loss = 0.66117990\n",
      "Iteration 175, loss = 0.66115743\n",
      "Iteration 176, loss = 0.66117219\n",
      "Iteration 177, loss = 0.66080191\n",
      "Iteration 178, loss = 0.66042295\n",
      "Iteration 179, loss = 0.65987225\n",
      "Iteration 180, loss = 0.65959898\n",
      "Iteration 181, loss = 0.65956030\n",
      "Iteration 182, loss = 0.65925335\n",
      "Iteration 183, loss = 0.65879258\n",
      "Iteration 184, loss = 0.65838847\n",
      "Iteration 185, loss = 0.65819053\n",
      "Iteration 186, loss = 0.65851802\n",
      "Iteration 187, loss = 0.65800253\n",
      "Iteration 188, loss = 0.65764008\n",
      "Iteration 189, loss = 0.65715756\n",
      "Iteration 190, loss = 0.65701553\n",
      "Iteration 191, loss = 0.65694143\n",
      "Iteration 192, loss = 0.65644738\n",
      "Iteration 193, loss = 0.65686878\n",
      "Iteration 194, loss = 0.65627589\n",
      "Iteration 195, loss = 0.65584497\n",
      "Iteration 196, loss = 0.65538103\n",
      "Iteration 197, loss = 0.65528601\n",
      "Iteration 198, loss = 0.65514178\n",
      "Iteration 199, loss = 0.65473093\n",
      "Iteration 200, loss = 0.65430854\n",
      "Iteration 201, loss = 0.65403498\n",
      "Iteration 202, loss = 0.65379513\n",
      "Iteration 203, loss = 0.65369901\n",
      "Iteration 204, loss = 0.65348826\n",
      "Iteration 205, loss = 0.65312800\n",
      "Iteration 206, loss = 0.65273920\n",
      "Iteration 207, loss = 0.65242643\n",
      "Iteration 208, loss = 0.65242942\n",
      "Iteration 209, loss = 0.65232257\n",
      "Iteration 210, loss = 0.65195454\n",
      "Iteration 211, loss = 0.65154925\n",
      "Iteration 212, loss = 0.65112226\n",
      "Iteration 213, loss = 0.65107368\n",
      "Iteration 214, loss = 0.65085415\n",
      "Iteration 215, loss = 0.65049593\n",
      "Iteration 216, loss = 0.65000456\n",
      "Iteration 217, loss = 0.64968543\n",
      "Iteration 218, loss = 0.64939271\n",
      "Iteration 219, loss = 0.64910510\n",
      "Iteration 220, loss = 0.64883087\n",
      "Iteration 221, loss = 0.64877542\n",
      "Iteration 222, loss = 0.64862340\n",
      "Iteration 223, loss = 0.64828183\n",
      "Iteration 224, loss = 0.64788429\n",
      "Iteration 225, loss = 0.64739041\n",
      "Iteration 226, loss = 0.64711987\n",
      "Iteration 227, loss = 0.64678647\n",
      "Iteration 228, loss = 0.64614620\n",
      "Iteration 229, loss = 0.64559357\n",
      "Iteration 230, loss = 0.64527807\n",
      "Iteration 231, loss = 0.64501493\n",
      "Iteration 232, loss = 0.64469937\n",
      "Iteration 233, loss = 0.64448591\n",
      "Iteration 234, loss = 0.64425554\n",
      "Iteration 235, loss = 0.64376338\n",
      "Iteration 236, loss = 0.64345383\n",
      "Iteration 237, loss = 0.64325072\n",
      "Iteration 238, loss = 0.64290270\n",
      "Iteration 239, loss = 0.64253159\n",
      "Iteration 240, loss = 0.64223509\n",
      "Iteration 241, loss = 0.64195820\n",
      "Iteration 242, loss = 0.64159573\n",
      "Iteration 243, loss = 0.64129543\n",
      "Iteration 244, loss = 0.64090524\n",
      "Iteration 245, loss = 0.64058654\n",
      "Iteration 246, loss = 0.64033763\n",
      "Iteration 247, loss = 0.64003132\n",
      "Iteration 248, loss = 0.63959249\n",
      "Iteration 249, loss = 0.63926606\n",
      "Iteration 250, loss = 0.63893872\n",
      "Iteration 251, loss = 0.63866917\n",
      "Iteration 252, loss = 0.63844416\n",
      "Iteration 253, loss = 0.63815645\n",
      "Iteration 254, loss = 0.63763305\n",
      "Iteration 255, loss = 0.63726162\n",
      "Iteration 256, loss = 0.63703431\n",
      "Iteration 257, loss = 0.63681213\n",
      "Iteration 258, loss = 0.63640698\n",
      "Iteration 259, loss = 0.63593816\n",
      "Iteration 260, loss = 0.63556024\n",
      "Iteration 261, loss = 0.63527424\n",
      "Iteration 262, loss = 0.63502120\n",
      "Iteration 263, loss = 0.63473302\n",
      "Iteration 264, loss = 0.63438541\n",
      "Iteration 265, loss = 0.63384898\n",
      "Iteration 266, loss = 0.63354760\n",
      "Iteration 267, loss = 0.63326904\n",
      "Iteration 268, loss = 0.63290467\n",
      "Iteration 269, loss = 0.63242663\n",
      "Iteration 270, loss = 0.63206371\n",
      "Iteration 271, loss = 0.63165920\n",
      "Iteration 272, loss = 0.63133377\n",
      "Iteration 273, loss = 0.63105857\n",
      "Iteration 274, loss = 0.63063145\n",
      "Iteration 275, loss = 0.63022098\n",
      "Iteration 276, loss = 0.62990082\n",
      "Iteration 277, loss = 0.62972955\n",
      "Iteration 278, loss = 0.62948519\n",
      "Iteration 279, loss = 0.62901831\n",
      "Iteration 280, loss = 0.62857958\n",
      "Iteration 281, loss = 0.62799527\n",
      "Iteration 282, loss = 0.62771747\n",
      "Iteration 283, loss = 0.62733317\n",
      "Iteration 284, loss = 0.62689624\n",
      "Iteration 285, loss = 0.62654914\n",
      "Iteration 286, loss = 0.62603674\n",
      "Iteration 287, loss = 0.62563543\n",
      "Iteration 288, loss = 0.62544711\n",
      "Iteration 289, loss = 0.62535410\n",
      "Iteration 290, loss = 0.62477321\n",
      "Iteration 291, loss = 0.62434805\n",
      "Iteration 292, loss = 0.62388758\n",
      "Iteration 293, loss = 0.62338585\n",
      "Iteration 294, loss = 0.62283534\n",
      "Iteration 295, loss = 0.62255455\n",
      "Iteration 296, loss = 0.62209491\n",
      "Iteration 297, loss = 0.62166058\n",
      "Iteration 298, loss = 0.62137844\n",
      "Iteration 299, loss = 0.62076953\n",
      "Iteration 300, loss = 0.62033949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70008233\n",
      "Iteration 2, loss = 1.02072724\n",
      "Iteration 3, loss = 1.12227122\n",
      "Iteration 4, loss = 1.02537884\n",
      "Iteration 5, loss = 0.69266763\n",
      "Iteration 6, loss = 0.72452072\n",
      "Iteration 7, loss = 0.70812645\n",
      "Iteration 8, loss = 0.69441082\n",
      "Iteration 9, loss = 0.70418044\n",
      "Iteration 10, loss = 0.70746505\n",
      "Iteration 11, loss = 0.69730945\n",
      "Iteration 12, loss = 0.69310087\n",
      "Iteration 13, loss = 0.69510136\n",
      "Iteration 14, loss = 0.69730723\n",
      "Iteration 15, loss = 0.69584339\n",
      "Iteration 16, loss = 0.69315973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69905928\n",
      "Iteration 2, loss = 1.03549096\n",
      "Iteration 3, loss = 1.30257976\n",
      "Iteration 4, loss = 1.20153687\n",
      "Iteration 5, loss = 0.78183390\n",
      "Iteration 6, loss = 0.73634741\n",
      "Iteration 7, loss = 0.95289474\n",
      "Iteration 8, loss = 1.00473869\n",
      "Iteration 9, loss = 0.88419499\n",
      "Iteration 10, loss = 0.73117590\n",
      "Iteration 11, loss = 0.70167318\n",
      "Iteration 12, loss = 0.77385110\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69831745\n",
      "Iteration 2, loss = 1.36489250\n",
      "Iteration 3, loss = 1.76127094\n",
      "Iteration 4, loss = 1.34444280\n",
      "Iteration 5, loss = 0.99045410\n",
      "Iteration 6, loss = 0.70145678\n",
      "Iteration 7, loss = 0.71341446\n",
      "Iteration 8, loss = 0.71013800\n",
      "Iteration 9, loss = 0.69339268\n",
      "Iteration 10, loss = 0.69730376\n",
      "Iteration 11, loss = 0.69274047\n",
      "Iteration 12, loss = 0.69670761\n",
      "Iteration 13, loss = 0.69263556\n",
      "Iteration 14, loss = 0.70205346\n",
      "Iteration 15, loss = 0.69262568\n",
      "Iteration 16, loss = 0.69884290\n",
      "Iteration 17, loss = 0.69368074\n",
      "Iteration 18, loss = 0.69523373\n",
      "Iteration 19, loss = 0.69404950\n",
      "Iteration 20, loss = 0.69216625\n",
      "Iteration 21, loss = 0.69393213\n",
      "Iteration 22, loss = 0.68984484\n",
      "Iteration 23, loss = 0.69358285\n",
      "Iteration 24, loss = 0.68895873\n",
      "Iteration 25, loss = 0.69006865\n",
      "Iteration 26, loss = 0.68928834\n",
      "Iteration 27, loss = 0.68780591\n",
      "Iteration 28, loss = 0.68872007\n",
      "Iteration 29, loss = 0.68674449\n",
      "Iteration 30, loss = 0.68705359\n",
      "Iteration 31, loss = 0.68647806\n",
      "Iteration 32, loss = 0.68519332\n",
      "Iteration 33, loss = 0.68564220\n",
      "Iteration 34, loss = 0.68432085\n",
      "Iteration 35, loss = 0.68381078\n",
      "Iteration 36, loss = 0.68375300\n",
      "Iteration 37, loss = 0.68243500\n",
      "Iteration 38, loss = 0.68217986\n",
      "Iteration 39, loss = 0.68178261\n",
      "Iteration 40, loss = 0.68062241\n",
      "Iteration 41, loss = 0.68031455\n",
      "Iteration 42, loss = 0.67982886\n",
      "Iteration 43, loss = 0.67876703\n",
      "Iteration 44, loss = 0.67828459\n",
      "Iteration 45, loss = 0.67783869\n",
      "Iteration 46, loss = 0.67687193\n",
      "Iteration 47, loss = 0.67614426\n",
      "Iteration 48, loss = 0.67570384\n",
      "Iteration 49, loss = 0.67493611\n",
      "Iteration 50, loss = 0.67401269\n",
      "Iteration 51, loss = 0.67336212\n",
      "Iteration 52, loss = 0.67278367\n",
      "Iteration 53, loss = 0.67196931\n",
      "Iteration 54, loss = 0.67105649\n",
      "Iteration 55, loss = 0.67028498\n",
      "Iteration 56, loss = 0.66961473\n",
      "Iteration 57, loss = 0.66886711\n",
      "Iteration 58, loss = 0.66798622\n",
      "Iteration 59, loss = 0.66705843\n",
      "Iteration 60, loss = 0.66617154\n",
      "Iteration 61, loss = 0.66534045\n",
      "Iteration 62, loss = 0.66453165\n",
      "Iteration 63, loss = 0.66371220\n",
      "Iteration 64, loss = 0.66287551\n",
      "Iteration 65, loss = 0.66203580\n",
      "Iteration 66, loss = 0.66123596\n",
      "Iteration 67, loss = 0.66055262\n",
      "Iteration 68, loss = 0.66016652\n",
      "Iteration 69, loss = 0.66044173\n",
      "Iteration 70, loss = 0.66211326\n",
      "Iteration 71, loss = 0.66582321\n",
      "Iteration 72, loss = 0.67035408\n",
      "Iteration 73, loss = 0.66956629\n",
      "Iteration 74, loss = 0.66066620\n",
      "Iteration 75, loss = 0.65243578\n",
      "Iteration 76, loss = 0.65321224\n",
      "Iteration 77, loss = 0.65868514\n",
      "Iteration 78, loss = 0.65916271\n",
      "Iteration 79, loss = 0.65263523\n",
      "Iteration 80, loss = 0.65170710\n",
      "Iteration 81, loss = 0.66771083\n",
      "Iteration 82, loss = 0.69280385\n",
      "Iteration 83, loss = 0.67541030\n",
      "Iteration 84, loss = 0.64400595\n",
      "Iteration 85, loss = 0.66092294\n",
      "Iteration 86, loss = 0.67008038\n",
      "Iteration 87, loss = 0.64352413\n",
      "Iteration 88, loss = 0.65111253\n",
      "Iteration 89, loss = 0.66027673\n",
      "Iteration 90, loss = 0.64021225\n",
      "Iteration 91, loss = 0.64646247\n",
      "Iteration 92, loss = 0.65225449\n",
      "Iteration 93, loss = 0.63701679\n",
      "Iteration 94, loss = 0.64218747\n",
      "Iteration 95, loss = 0.64583143\n",
      "Iteration 96, loss = 0.63417061\n",
      "Iteration 97, loss = 0.63771002\n",
      "Iteration 98, loss = 0.64063684\n",
      "Iteration 99, loss = 0.63161407\n",
      "Iteration 100, loss = 0.63299326\n",
      "Iteration 101, loss = 0.63611837\n",
      "Iteration 102, loss = 0.62941196\n",
      "Iteration 103, loss = 0.62823102\n",
      "Iteration 104, loss = 0.63152614\n",
      "Iteration 105, loss = 0.62762164\n",
      "Iteration 106, loss = 0.62401797\n",
      "Iteration 107, loss = 0.62612534\n",
      "Iteration 108, loss = 0.62561544\n",
      "Iteration 109, loss = 0.62139384\n",
      "Iteration 110, loss = 0.62029842\n",
      "Iteration 111, loss = 0.62155819\n",
      "Iteration 112, loss = 0.62006635\n",
      "Iteration 113, loss = 0.61674814\n",
      "Iteration 114, loss = 0.61545895\n",
      "Iteration 115, loss = 0.61590803\n",
      "Iteration 116, loss = 0.61527034\n",
      "Iteration 117, loss = 0.61288886\n",
      "Iteration 118, loss = 0.61044744\n",
      "Iteration 119, loss = 0.60922176\n",
      "Iteration 120, loss = 0.60889971\n",
      "Iteration 121, loss = 0.60852398\n",
      "Iteration 122, loss = 0.60754218\n",
      "Iteration 123, loss = 0.60597445\n",
      "Iteration 124, loss = 0.60413954\n",
      "Iteration 125, loss = 0.60230305\n",
      "Iteration 126, loss = 0.60060976\n",
      "Iteration 127, loss = 0.59910654\n",
      "Iteration 128, loss = 0.59788227\n",
      "Iteration 129, loss = 0.59721543\n",
      "Iteration 130, loss = 0.59800208\n",
      "Iteration 131, loss = 0.60284799\n",
      "Iteration 132, loss = 0.61895444\n",
      "Iteration 133, loss = 0.66078580\n",
      "Iteration 134, loss = 0.71048930\n",
      "Iteration 135, loss = 0.67393918\n",
      "Iteration 136, loss = 0.59846524\n",
      "Iteration 137, loss = 0.59476048\n",
      "Iteration 138, loss = 0.64617741\n",
      "Iteration 139, loss = 0.63991986\n",
      "Iteration 140, loss = 0.58745521\n",
      "Iteration 141, loss = 0.59381912\n",
      "Iteration 142, loss = 0.62805121\n",
      "Iteration 143, loss = 0.60585311\n",
      "Iteration 144, loss = 0.57752660\n",
      "Iteration 145, loss = 0.59738373\n",
      "Iteration 146, loss = 0.71702979\n",
      "Iteration 147, loss = 0.89636766\n",
      "Iteration 148, loss = 0.59330330\n",
      "Iteration 149, loss = 0.75534778\n",
      "Iteration 150, loss = 0.66759992\n",
      "Iteration 151, loss = 0.62512258\n",
      "Iteration 152, loss = 0.69780744\n",
      "Iteration 153, loss = 0.58935305\n",
      "Iteration 154, loss = 0.69792843\n",
      "Iteration 155, loss = 0.57879810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73268849\n",
      "Iteration 2, loss = 0.73493840\n",
      "Iteration 3, loss = 0.69332071\n",
      "Iteration 4, loss = 0.69386355\n",
      "Iteration 5, loss = 0.69378774\n",
      "Iteration 6, loss = 0.69281532\n",
      "Iteration 7, loss = 0.69253532\n",
      "Iteration 8, loss = 0.69223298\n",
      "Iteration 9, loss = 0.69206862\n",
      "Iteration 10, loss = 0.69194482\n",
      "Iteration 11, loss = 0.69180461\n",
      "Iteration 12, loss = 0.69163726\n",
      "Iteration 13, loss = 0.69138564\n",
      "Iteration 14, loss = 0.69116217\n",
      "Iteration 15, loss = 0.69099080\n",
      "Iteration 16, loss = 0.69081438\n",
      "Iteration 17, loss = 0.69063244\n",
      "Iteration 18, loss = 0.69044561\n",
      "Iteration 19, loss = 0.69025911\n",
      "Iteration 20, loss = 0.69006239\n",
      "Iteration 21, loss = 0.68985833\n",
      "Iteration 22, loss = 0.68964759\n",
      "Iteration 23, loss = 0.68943995\n",
      "Iteration 24, loss = 0.68923614\n",
      "Iteration 25, loss = 0.68905515\n",
      "Iteration 26, loss = 0.68896566\n",
      "Iteration 27, loss = 0.68921010\n",
      "Iteration 28, loss = 0.69068943\n",
      "Iteration 29, loss = 0.69684247\n",
      "Iteration 30, loss = 0.72022681\n",
      "Iteration 31, loss = 0.79967127\n",
      "Iteration 32, loss = 0.96578606\n",
      "Iteration 33, loss = 0.99086301\n",
      "Iteration 34, loss = 0.73452598\n",
      "Iteration 35, loss = 0.69067336\n",
      "Iteration 36, loss = 0.69077558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72984577\n",
      "Iteration 2, loss = 0.73950654\n",
      "Iteration 3, loss = 0.69384863\n",
      "Iteration 4, loss = 0.69489786\n",
      "Iteration 5, loss = 0.69408357\n",
      "Iteration 6, loss = 0.69284158\n",
      "Iteration 7, loss = 0.69264250\n",
      "Iteration 8, loss = 0.69255494\n",
      "Iteration 9, loss = 0.69246532\n",
      "Iteration 10, loss = 0.69235614\n",
      "Iteration 11, loss = 0.69223450\n",
      "Iteration 12, loss = 0.69210114\n",
      "Iteration 13, loss = 0.69196035\n",
      "Iteration 14, loss = 0.69181293\n",
      "Iteration 15, loss = 0.69165970\n",
      "Iteration 16, loss = 0.69150096\n",
      "Iteration 17, loss = 0.69133688\n",
      "Iteration 18, loss = 0.69116793\n",
      "Iteration 19, loss = 0.69099405\n",
      "Iteration 20, loss = 0.69081597\n",
      "Iteration 21, loss = 0.69063406\n",
      "Iteration 22, loss = 0.69044817\n",
      "Iteration 23, loss = 0.69025940\n",
      "Iteration 24, loss = 0.69006607\n",
      "Iteration 25, loss = 0.68986990\n",
      "Iteration 26, loss = 0.68967035\n",
      "Iteration 27, loss = 0.68947118\n",
      "Iteration 28, loss = 0.68927476\n",
      "Iteration 29, loss = 0.68910752\n",
      "Iteration 30, loss = 0.68911081\n",
      "Iteration 31, loss = 0.68965085\n",
      "Iteration 32, loss = 0.69228212\n",
      "Iteration 33, loss = 0.70180175\n",
      "Iteration 34, loss = 0.73438513\n",
      "Iteration 35, loss = 0.81993454\n",
      "Iteration 36, loss = 0.87558534\n",
      "Iteration 37, loss = 0.77455315\n",
      "Iteration 38, loss = 0.70255571\n",
      "Iteration 39, loss = 0.69243201\n",
      "Iteration 40, loss = 0.69156269\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72996375\n",
      "Iteration 2, loss = 0.73687044\n",
      "Iteration 3, loss = 0.69305895\n",
      "Iteration 4, loss = 0.69416273\n",
      "Iteration 5, loss = 0.69340795\n",
      "Iteration 6, loss = 0.69257888\n",
      "Iteration 7, loss = 0.69224449\n",
      "Iteration 8, loss = 0.69213016\n",
      "Iteration 9, loss = 0.69201179\n",
      "Iteration 10, loss = 0.69180720\n",
      "Iteration 11, loss = 0.69156375\n",
      "Iteration 12, loss = 0.69133948\n",
      "Iteration 13, loss = 0.69122029\n",
      "Iteration 14, loss = 0.69106865\n",
      "Iteration 15, loss = 0.69091014\n",
      "Iteration 16, loss = 0.69078174\n",
      "Iteration 17, loss = 0.69076585\n",
      "Iteration 18, loss = 0.69111527\n",
      "Iteration 19, loss = 0.69272312\n",
      "Iteration 20, loss = 0.69905592\n",
      "Iteration 21, loss = 0.72141937\n",
      "Iteration 22, loss = 0.77357850\n",
      "Iteration 23, loss = 0.86381061\n",
      "Iteration 24, loss = 0.88185195\n",
      "Iteration 25, loss = 0.79047579\n",
      "Iteration 26, loss = 0.71140524\n",
      "Iteration 27, loss = 0.69199327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73268849\n",
      "Iteration 2, loss = 17.90414690\n",
      "Iteration 3, loss = 2.71615352\n",
      "Iteration 4, loss = 11.31325866\n",
      "Iteration 5, loss = 4.53019582\n",
      "Iteration 6, loss = 6.33317919\n",
      "Iteration 7, loss = 6.33723204\n",
      "Iteration 8, loss = 1.79108581\n",
      "Iteration 9, loss = 5.61642617\n",
      "Iteration 10, loss = 7.54412945\n",
      "Iteration 11, loss = 5.52913404\n",
      "Iteration 12, loss = 1.15070808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72984577\n",
      "Iteration 2, loss = 18.13972636\n",
      "Iteration 3, loss = 1.85336718\n",
      "Iteration 4, loss = 8.01391276\n",
      "Iteration 5, loss = 0.69665917\n",
      "Iteration 6, loss = 5.49418369\n",
      "Iteration 7, loss = 1.25781497\n",
      "Iteration 8, loss = 7.40502442\n",
      "Iteration 9, loss = 6.28883737\n",
      "Iteration 10, loss = 1.67652151\n",
      "Iteration 11, loss = 1.68118747\n",
      "Iteration 12, loss = 3.57989585\n",
      "Iteration 13, loss = 2.94479510\n",
      "Iteration 14, loss = 2.20146129\n",
      "Iteration 15, loss = 2.34196356\n",
      "Iteration 16, loss = 1.58076902\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72996375\n",
      "Iteration 2, loss = 18.02193592\n",
      "Iteration 3, loss = 1.22565683\n",
      "Iteration 4, loss = 8.33228303\n",
      "Iteration 5, loss = 0.81948267\n",
      "Iteration 6, loss = 0.87458220\n",
      "Iteration 7, loss = 4.26280450\n",
      "Iteration 8, loss = 0.69330451\n",
      "Iteration 9, loss = 2.42969254\n",
      "Iteration 10, loss = 4.10744524\n",
      "Iteration 11, loss = 1.06792647\n",
      "Iteration 12, loss = 8.30656729\n",
      "Iteration 13, loss = 7.45194852\n",
      "Iteration 14, loss = 0.71514235\n",
      "Iteration 15, loss = 6.04921863\n",
      "Iteration 16, loss = 5.88574017\n",
      "Iteration 17, loss = 1.32172458\n",
      "Iteration 18, loss = 6.11121160\n",
      "Iteration 19, loss = 7.92017492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70079682\n",
      "Iteration 2, loss = 1.08479344\n",
      "Iteration 3, loss = 0.82895222\n",
      "Iteration 4, loss = 0.71258999\n",
      "Iteration 5, loss = 0.69320444\n",
      "Iteration 6, loss = 0.69357821\n",
      "Iteration 7, loss = 0.69280168\n",
      "Iteration 8, loss = 0.69265341\n",
      "Iteration 9, loss = 0.69256855\n",
      "Iteration 10, loss = 0.69249846\n",
      "Iteration 11, loss = 0.69235260\n",
      "Iteration 12, loss = 0.69224489\n",
      "Iteration 13, loss = 0.69217756\n",
      "Iteration 14, loss = 0.69207949\n",
      "Iteration 15, loss = 0.69195305\n",
      "Iteration 16, loss = 0.69187441\n",
      "Iteration 17, loss = 0.69182179\n",
      "Iteration 18, loss = 0.69173776\n",
      "Iteration 19, loss = 0.69124520\n",
      "Iteration 20, loss = 0.69105644\n",
      "Iteration 21, loss = 0.69095146\n",
      "Iteration 22, loss = 0.69081791\n",
      "Iteration 23, loss = 0.69065157\n",
      "Iteration 24, loss = 0.69052070\n",
      "Iteration 25, loss = 0.69041166\n",
      "Iteration 26, loss = 0.69024640\n",
      "Iteration 27, loss = 0.69014490\n",
      "Iteration 28, loss = 0.69000092\n",
      "Iteration 29, loss = 0.68983241\n",
      "Iteration 30, loss = 0.68967528\n",
      "Iteration 31, loss = 0.68952168\n",
      "Iteration 32, loss = 0.68938614\n",
      "Iteration 33, loss = 0.68921773\n",
      "Iteration 34, loss = 0.68912341\n",
      "Iteration 35, loss = 0.68904109\n",
      "Iteration 36, loss = 0.68882835\n",
      "Iteration 37, loss = 0.68864680\n",
      "Iteration 38, loss = 0.68846886\n",
      "Iteration 39, loss = 0.68827307\n",
      "Iteration 40, loss = 0.68804808\n",
      "Iteration 41, loss = 0.68798817\n",
      "Iteration 42, loss = 0.68777128\n",
      "Iteration 43, loss = 0.68757986\n",
      "Iteration 44, loss = 0.68738496\n",
      "Iteration 45, loss = 0.68718830\n",
      "Iteration 46, loss = 0.68698145\n",
      "Iteration 47, loss = 0.68676157\n",
      "Iteration 48, loss = 0.68651938\n",
      "Iteration 49, loss = 0.68632581\n",
      "Iteration 50, loss = 0.68617796\n",
      "Iteration 51, loss = 0.68596312\n",
      "Iteration 52, loss = 0.68572019\n",
      "Iteration 53, loss = 0.68547548\n",
      "Iteration 54, loss = 0.68522613\n",
      "Iteration 55, loss = 0.68495802\n",
      "Iteration 56, loss = 0.68475332\n",
      "Iteration 57, loss = 0.68457219\n",
      "Iteration 58, loss = 0.68431555\n",
      "Iteration 59, loss = 0.68403411\n",
      "Iteration 60, loss = 0.68375020\n",
      "Iteration 61, loss = 0.68346082\n",
      "Iteration 62, loss = 0.68315940\n",
      "Iteration 63, loss = 0.68291250\n",
      "Iteration 64, loss = 0.68265793\n",
      "Iteration 65, loss = 0.68236609\n",
      "Iteration 66, loss = 0.68204650\n",
      "Iteration 67, loss = 0.68172158\n",
      "Iteration 68, loss = 0.68137149\n",
      "Iteration 69, loss = 0.68107299\n",
      "Iteration 70, loss = 0.68076050\n",
      "Iteration 71, loss = 0.68046506\n",
      "Iteration 72, loss = 0.68007817\n",
      "Iteration 73, loss = 0.67983787\n",
      "Iteration 74, loss = 0.67965016\n",
      "Iteration 75, loss = 0.67945226\n",
      "Iteration 76, loss = 0.67915641\n",
      "Iteration 77, loss = 0.67890438\n",
      "Iteration 78, loss = 0.67874972\n",
      "Iteration 79, loss = 0.67902707\n",
      "Iteration 80, loss = 0.67958108\n",
      "Iteration 81, loss = 0.68125129\n",
      "Iteration 82, loss = 0.68528655\n",
      "Iteration 83, loss = 0.70717336\n",
      "Iteration 84, loss = 0.73608598\n",
      "Iteration 85, loss = 0.71631798\n",
      "Iteration 86, loss = 0.68376211\n",
      "Iteration 87, loss = 0.68143339\n",
      "Iteration 88, loss = 0.68125295\n",
      "Iteration 89, loss = 0.68108930\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70389361\n",
      "Iteration 2, loss = 1.05073801\n",
      "Iteration 3, loss = 0.99086681\n",
      "Iteration 4, loss = 0.75431065\n",
      "Iteration 5, loss = 0.69565375\n",
      "Iteration 6, loss = 0.69348397\n",
      "Iteration 7, loss = 0.69595782\n",
      "Iteration 8, loss = 0.69674762\n",
      "Iteration 9, loss = 0.69466613\n",
      "Iteration 10, loss = 0.69330283\n",
      "Iteration 11, loss = 0.69359032\n",
      "Iteration 12, loss = 0.69354816\n",
      "Iteration 13, loss = 0.69329927\n",
      "Iteration 14, loss = 0.69320242\n",
      "Iteration 15, loss = 0.69319142\n",
      "Iteration 16, loss = 0.69314563\n",
      "Iteration 17, loss = 0.69312203\n",
      "Iteration 18, loss = 0.69311523\n",
      "Iteration 19, loss = 0.69307457\n",
      "Iteration 20, loss = 0.69303237\n",
      "Iteration 21, loss = 0.69300345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70167987\n",
      "Iteration 2, loss = 1.07027942\n",
      "Iteration 3, loss = 0.89959068\n",
      "Iteration 4, loss = 0.80862478\n",
      "Iteration 5, loss = 0.69638780\n",
      "Iteration 6, loss = 0.69373865\n",
      "Iteration 7, loss = 0.69548049\n",
      "Iteration 8, loss = 0.69567818\n",
      "Iteration 9, loss = 0.69419452\n",
      "Iteration 10, loss = 0.69348596\n",
      "Iteration 11, loss = 0.69359938\n",
      "Iteration 12, loss = 0.69354047\n",
      "Iteration 13, loss = 0.69336411\n",
      "Iteration 14, loss = 0.69329639\n",
      "Iteration 15, loss = 0.69325423\n",
      "Iteration 16, loss = 0.69320201\n",
      "Iteration 17, loss = 0.69315390\n",
      "Iteration 18, loss = 0.69310705\n",
      "Iteration 19, loss = 0.69305977\n",
      "Iteration 20, loss = 0.69301251\n",
      "Iteration 21, loss = 0.69296518\n",
      "Iteration 22, loss = 0.69291772\n",
      "Iteration 23, loss = 0.69287012\n",
      "Iteration 24, loss = 0.69282249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70079682\n",
      "Iteration 2, loss = 17.90414707\n",
      "Iteration 3, loss = 8.09445252\n",
      "Iteration 4, loss = 15.85688223\n",
      "Iteration 5, loss = 8.67492336\n",
      "Iteration 6, loss = 2.64312069\n",
      "Iteration 7, loss = 2.09615841\n",
      "Iteration 8, loss = 3.81469505\n",
      "Iteration 9, loss = 3.27378228\n",
      "Iteration 10, loss = 2.01569669\n",
      "Iteration 11, loss = 1.95274165\n",
      "Iteration 12, loss = 2.42543134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70389361\n",
      "Iteration 2, loss = 18.13972621\n",
      "Iteration 3, loss = 2.33265053\n",
      "Iteration 4, loss = 15.65744515\n",
      "Iteration 5, loss = 11.47665021\n",
      "Iteration 6, loss = 5.13194658\n",
      "Iteration 7, loss = 2.93563994\n",
      "Iteration 8, loss = 4.17123574\n",
      "Iteration 9, loss = 1.33679672\n",
      "Iteration 10, loss = 3.94332471\n",
      "Iteration 11, loss = 4.39020021\n",
      "Iteration 12, loss = 2.05477629\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70167987\n",
      "Iteration 2, loss = 18.02193609\n",
      "Iteration 3, loss = 9.25697674\n",
      "Iteration 4, loss = 15.86761632\n",
      "Iteration 5, loss = 8.72442237\n",
      "Iteration 6, loss = 3.32887097\n",
      "Iteration 7, loss = 2.94739718\n",
      "Iteration 8, loss = 1.82369364\n",
      "Iteration 9, loss = 1.25336983\n",
      "Iteration 10, loss = 2.87764838\n",
      "Iteration 11, loss = 2.36679497\n",
      "Iteration 12, loss = 1.41249079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70008233\n",
      "Iteration 2, loss = 0.75918073\n",
      "Iteration 3, loss = 0.70794726\n",
      "Iteration 4, loss = 0.69411755\n",
      "Iteration 5, loss = 0.69311076\n",
      "Iteration 6, loss = 0.69293593\n",
      "Iteration 7, loss = 0.69260181\n",
      "Iteration 8, loss = 0.69241420\n",
      "Iteration 9, loss = 0.69222513\n",
      "Iteration 10, loss = 0.69205112\n",
      "Iteration 11, loss = 0.69176538\n",
      "Iteration 12, loss = 0.69153576\n",
      "Iteration 13, loss = 0.69128314\n",
      "Iteration 14, loss = 0.69100806\n",
      "Iteration 15, loss = 0.69069384\n",
      "Iteration 16, loss = 0.69047341\n",
      "Iteration 17, loss = 0.69015683\n",
      "Iteration 18, loss = 0.68973897\n",
      "Iteration 19, loss = 0.68943538\n",
      "Iteration 20, loss = 0.68914070\n",
      "Iteration 21, loss = 0.68885186\n",
      "Iteration 22, loss = 0.68857443\n",
      "Iteration 23, loss = 0.68825102\n",
      "Iteration 24, loss = 0.68791580\n",
      "Iteration 25, loss = 0.68757201\n",
      "Iteration 26, loss = 0.68720183\n",
      "Iteration 27, loss = 0.68687268\n",
      "Iteration 28, loss = 0.68653054\n",
      "Iteration 29, loss = 0.68615513\n",
      "Iteration 30, loss = 0.68579260\n",
      "Iteration 31, loss = 0.68543783\n",
      "Iteration 32, loss = 0.68505241\n",
      "Iteration 33, loss = 0.68465668\n",
      "Iteration 34, loss = 0.68424652\n",
      "Iteration 35, loss = 0.68390091\n",
      "Iteration 36, loss = 0.68346168\n",
      "Iteration 37, loss = 0.68302505\n",
      "Iteration 38, loss = 0.68258524\n",
      "Iteration 39, loss = 0.68213843\n",
      "Iteration 40, loss = 0.68168083\n",
      "Iteration 41, loss = 0.68121177\n",
      "Iteration 42, loss = 0.68073070\n",
      "Iteration 43, loss = 0.68023703\n",
      "Iteration 44, loss = 0.67973022\n",
      "Iteration 45, loss = 0.67920960\n",
      "Iteration 46, loss = 0.67867451\n",
      "Iteration 47, loss = 0.67812427\n",
      "Iteration 48, loss = 0.67755819\n",
      "Iteration 49, loss = 0.67697549\n",
      "Iteration 50, loss = 0.67637537\n",
      "Iteration 51, loss = 0.67575699\n",
      "Iteration 52, loss = 0.67511949\n",
      "Iteration 53, loss = 0.67446193\n",
      "Iteration 54, loss = 0.67378335\n",
      "Iteration 55, loss = 0.67308273\n",
      "Iteration 56, loss = 0.67235901\n",
      "Iteration 57, loss = 0.67161107\n",
      "Iteration 58, loss = 0.67083776\n",
      "Iteration 59, loss = 0.67003776\n",
      "Iteration 60, loss = 0.66920985\n",
      "Iteration 61, loss = 0.66835263\n",
      "Iteration 62, loss = 0.66746467\n",
      "Iteration 63, loss = 0.66654454\n",
      "Iteration 64, loss = 0.66559076\n",
      "Iteration 65, loss = 0.66460222\n",
      "Iteration 66, loss = 0.66357981\n",
      "Iteration 67, loss = 0.66253431\n",
      "Iteration 68, loss = 0.66152539\n",
      "Iteration 69, loss = 0.66089210\n",
      "Iteration 70, loss = 0.66267941\n",
      "Iteration 71, loss = 0.68006886\n",
      "Iteration 72, loss = 0.79073556\n",
      "Iteration 73, loss = 0.98205698\n",
      "Iteration 74, loss = 0.69802594\n",
      "Iteration 75, loss = 0.70059003\n",
      "Iteration 76, loss = 0.68993350\n",
      "Iteration 77, loss = 0.69095041\n",
      "Iteration 78, loss = 0.69291705\n",
      "Iteration 79, loss = 0.69283422\n",
      "Iteration 80, loss = 0.69255108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69905928\n",
      "Iteration 2, loss = 0.76433732\n",
      "Iteration 3, loss = 0.70276918\n",
      "Iteration 4, loss = 0.69351278\n",
      "Iteration 5, loss = 0.69258234\n",
      "Iteration 6, loss = 0.69251577\n",
      "Iteration 7, loss = 0.69221660\n",
      "Iteration 8, loss = 0.69166007\n",
      "Iteration 9, loss = 0.69160418\n",
      "Iteration 10, loss = 0.69115804\n",
      "Iteration 11, loss = 0.69098382\n",
      "Iteration 12, loss = 0.69085922\n",
      "Iteration 13, loss = 0.69064316\n",
      "Iteration 14, loss = 0.69031601\n",
      "Iteration 15, loss = 0.69030558\n",
      "Iteration 16, loss = 0.68992396\n",
      "Iteration 17, loss = 0.68977361\n",
      "Iteration 18, loss = 0.68949462\n",
      "Iteration 19, loss = 0.68925748\n",
      "Iteration 20, loss = 0.68896767\n",
      "Iteration 21, loss = 0.68871390\n",
      "Iteration 22, loss = 0.68843841\n",
      "Iteration 23, loss = 0.68817365\n",
      "Iteration 24, loss = 0.68790171\n",
      "Iteration 25, loss = 0.68760887\n",
      "Iteration 26, loss = 0.68735625\n",
      "Iteration 27, loss = 0.68700992\n",
      "Iteration 28, loss = 0.68670983\n",
      "Iteration 29, loss = 0.68639892\n",
      "Iteration 30, loss = 0.68610118\n",
      "Iteration 31, loss = 0.68578672\n",
      "Iteration 32, loss = 0.68545662\n",
      "Iteration 33, loss = 0.68509471\n",
      "Iteration 34, loss = 0.68478316\n",
      "Iteration 35, loss = 0.68442310\n",
      "Iteration 36, loss = 0.68406520\n",
      "Iteration 37, loss = 0.68368330\n",
      "Iteration 38, loss = 0.68336135\n",
      "Iteration 39, loss = 0.68293638\n",
      "Iteration 40, loss = 0.68257565\n",
      "Iteration 41, loss = 0.68215844\n",
      "Iteration 42, loss = 0.68174496\n",
      "Iteration 43, loss = 0.68130587\n",
      "Iteration 44, loss = 0.68093476\n",
      "Iteration 45, loss = 0.68046860\n",
      "Iteration 46, loss = 0.68000024\n",
      "Iteration 47, loss = 0.67954345\n",
      "Iteration 48, loss = 0.67907025\n",
      "Iteration 49, loss = 0.67857711\n",
      "Iteration 50, loss = 0.67807000\n",
      "Iteration 51, loss = 0.67755654\n",
      "Iteration 52, loss = 0.67702104\n",
      "Iteration 53, loss = 0.67643691\n",
      "Iteration 54, loss = 0.67596657\n",
      "Iteration 55, loss = 0.67535179\n",
      "Iteration 56, loss = 0.67474982\n",
      "Iteration 57, loss = 0.67415636\n",
      "Iteration 58, loss = 0.67357045\n",
      "Iteration 59, loss = 0.67311695\n",
      "Iteration 60, loss = 0.67281413\n",
      "Iteration 61, loss = 0.67301143\n",
      "Iteration 62, loss = 0.67478808\n",
      "Iteration 63, loss = 0.68169742\n",
      "Iteration 64, loss = 0.70351551\n",
      "Iteration 65, loss = 0.76677776\n",
      "Iteration 66, loss = 0.84484414\n",
      "Iteration 67, loss = 0.75303687\n",
      "Iteration 68, loss = 0.70004254\n",
      "Iteration 69, loss = 0.68073920\n",
      "Iteration 70, loss = 0.68025857\n",
      "Iteration 71, loss = 0.67638619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69831745\n",
      "Iteration 2, loss = 0.76056982\n",
      "Iteration 3, loss = 0.70471459\n",
      "Iteration 4, loss = 0.69318814\n",
      "Iteration 5, loss = 0.69150694\n",
      "Iteration 6, loss = 0.69193365\n",
      "Iteration 7, loss = 0.69194718\n",
      "Iteration 8, loss = 0.69163519\n",
      "Iteration 9, loss = 0.69142758\n",
      "Iteration 10, loss = 0.69117389\n",
      "Iteration 11, loss = 0.69094905\n",
      "Iteration 12, loss = 0.69068896\n",
      "Iteration 13, loss = 0.69041620\n",
      "Iteration 14, loss = 0.69013682\n",
      "Iteration 15, loss = 0.68983703\n",
      "Iteration 16, loss = 0.68947476\n",
      "Iteration 17, loss = 0.68922146\n",
      "Iteration 18, loss = 0.68893361\n",
      "Iteration 19, loss = 0.68860238\n",
      "Iteration 20, loss = 0.68828520\n",
      "Iteration 21, loss = 0.68798351\n",
      "Iteration 22, loss = 0.68764420\n",
      "Iteration 23, loss = 0.68731043\n",
      "Iteration 24, loss = 0.68697436\n",
      "Iteration 25, loss = 0.68664584\n",
      "Iteration 26, loss = 0.68628655\n",
      "Iteration 27, loss = 0.68595847\n",
      "Iteration 28, loss = 0.68557811\n",
      "Iteration 29, loss = 0.68522299\n",
      "Iteration 30, loss = 0.68483481\n",
      "Iteration 31, loss = 0.68444743\n",
      "Iteration 32, loss = 0.68404687\n",
      "Iteration 33, loss = 0.68365229\n",
      "Iteration 34, loss = 0.68323508\n",
      "Iteration 35, loss = 0.68282251\n",
      "Iteration 36, loss = 0.68239764\n",
      "Iteration 37, loss = 0.68196324\n",
      "Iteration 38, loss = 0.68150104\n",
      "Iteration 39, loss = 0.68105887\n",
      "Iteration 40, loss = 0.68061533\n",
      "Iteration 41, loss = 0.68010124\n",
      "Iteration 42, loss = 0.67962475\n",
      "Iteration 43, loss = 0.67915352\n",
      "Iteration 44, loss = 0.67861381\n",
      "Iteration 45, loss = 0.67806258\n",
      "Iteration 46, loss = 0.67731632\n",
      "Iteration 47, loss = 0.67658065\n",
      "Iteration 48, loss = 0.67641476\n",
      "Iteration 49, loss = 0.67684765\n",
      "Iteration 50, loss = 0.67848486\n",
      "Iteration 51, loss = 0.68346316\n",
      "Iteration 52, loss = 0.69633053\n",
      "Iteration 53, loss = 0.73065700\n",
      "Iteration 54, loss = 0.78401999\n",
      "Iteration 55, loss = 0.76734577\n",
      "Iteration 56, loss = 0.70224972\n",
      "Iteration 57, loss = 0.68231390\n",
      "Iteration 58, loss = 0.68114020\n",
      "Iteration 59, loss = 0.68032318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70008233\n",
      "Iteration 2, loss = 9.86801422\n",
      "Iteration 3, loss = 17.90416623\n",
      "Iteration 4, loss = 11.41083872\n",
      "Iteration 5, loss = 2.27558886\n",
      "Iteration 6, loss = 11.16043786\n",
      "Iteration 7, loss = 8.12796356\n",
      "Iteration 8, loss = 1.51577749\n",
      "Iteration 9, loss = 5.83357830\n",
      "Iteration 10, loss = 6.49009443\n",
      "Iteration 11, loss = 4.45419108\n",
      "Iteration 12, loss = 1.14655840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69905928\n",
      "Iteration 2, loss = 11.06374417\n",
      "Iteration 3, loss = 11.41194502\n",
      "Iteration 4, loss = 6.86275722\n",
      "Iteration 5, loss = 1.85807548\n",
      "Iteration 6, loss = 0.78032142\n",
      "Iteration 7, loss = 3.47045867\n",
      "Iteration 8, loss = 2.11924804\n",
      "Iteration 9, loss = 1.65528855\n",
      "Iteration 10, loss = 1.78203699\n",
      "Iteration 11, loss = 0.84064281\n",
      "Iteration 12, loss = 1.25033311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69831745\n",
      "Iteration 2, loss = 16.51277654\n",
      "Iteration 3, loss = 16.11280054\n",
      "Iteration 4, loss = 10.68054424\n",
      "Iteration 5, loss = 0.69443284\n",
      "Iteration 6, loss = 1.99593891\n",
      "Iteration 7, loss = 4.87954454\n",
      "Iteration 8, loss = 1.35472042\n",
      "Iteration 9, loss = 6.07817632\n",
      "Iteration 10, loss = 2.72850535\n",
      "Iteration 11, loss = 7.39088208\n",
      "Iteration 12, loss = 7.26909281\n",
      "Iteration 13, loss = 2.14283286\n",
      "Iteration 14, loss = 6.34253328\n",
      "Iteration 15, loss = 8.57026852\n",
      "Iteration 16, loss = 6.24368881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73268849\n",
      "Iteration 2, loss = 1.25232067\n",
      "Iteration 3, loss = 18.13971774\n",
      "Iteration 4, loss = 12.49649063\n",
      "Iteration 5, loss = 2.88297949\n",
      "Iteration 6, loss = 0.73044147\n",
      "Iteration 7, loss = 1.13310178\n",
      "Iteration 8, loss = 0.72459043\n",
      "Iteration 9, loss = 0.72790407\n",
      "Iteration 10, loss = 0.70954447\n",
      "Iteration 11, loss = 0.69578557\n",
      "Iteration 12, loss = 0.70119756\n",
      "Iteration 13, loss = 0.69948783\n",
      "Iteration 14, loss = 0.69671216\n",
      "Iteration 15, loss = 0.69708940\n",
      "Iteration 16, loss = 0.69728696\n",
      "Iteration 17, loss = 0.69753018\n",
      "Iteration 18, loss = 0.69777126\n",
      "Iteration 19, loss = 0.69799397\n",
      "Iteration 20, loss = 0.69824240\n",
      "Iteration 21, loss = 0.69865240\n",
      "Iteration 22, loss = 0.70015363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72984577\n",
      "Iteration 2, loss = 1.10358747\n",
      "Iteration 3, loss = 17.90413766\n",
      "Iteration 4, loss = 18.13990208\n",
      "Iteration 5, loss = 1.09618937\n",
      "Iteration 6, loss = 5.73565637\n",
      "Iteration 7, loss = 1.14497310\n",
      "Iteration 8, loss = 18.14142575\n",
      "Iteration 9, loss = 0.72653490\n",
      "Iteration 10, loss = 0.72119594\n",
      "Iteration 11, loss = 0.71775681\n",
      "Iteration 12, loss = 0.71619892\n",
      "Iteration 13, loss = 0.71388465\n",
      "Iteration 14, loss = 0.71112617\n",
      "Iteration 15, loss = 0.70824123\n",
      "Iteration 16, loss = 0.70429453\n",
      "Iteration 17, loss = 0.69999804\n",
      "Iteration 18, loss = 0.69885659\n",
      "Iteration 19, loss = 0.70002034\n",
      "Iteration 20, loss = 0.70177687\n",
      "Iteration 21, loss = 0.70275933\n",
      "Iteration 22, loss = 0.70258199\n",
      "Iteration 23, loss = 0.70170016\n",
      "Iteration 24, loss = 0.70083325\n",
      "Iteration 25, loss = 0.70043997\n",
      "Iteration 26, loss = 0.70051872\n",
      "Iteration 27, loss = 0.70075328\n",
      "Iteration 28, loss = 0.70080547\n",
      "Iteration 29, loss = 0.70112990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72996375\n",
      "Iteration 2, loss = 1.26042525\n",
      "Iteration 3, loss = 18.02192708\n",
      "Iteration 4, loss = 9.79381762\n",
      "Iteration 5, loss = 2.54315103\n",
      "Iteration 6, loss = 0.69639674\n",
      "Iteration 7, loss = 0.73465134\n",
      "Iteration 8, loss = 0.69519600\n",
      "Iteration 9, loss = 0.69514355\n",
      "Iteration 10, loss = 0.69819228\n",
      "Iteration 11, loss = 0.69958380\n",
      "Iteration 12, loss = 0.70031645\n",
      "Iteration 13, loss = 0.70058007\n",
      "Iteration 14, loss = 0.70087148\n",
      "Iteration 15, loss = 0.70008357\n",
      "Iteration 16, loss = 0.69948522\n",
      "Iteration 17, loss = 0.69883213\n",
      "Iteration 18, loss = 0.69823946\n",
      "Iteration 19, loss = 0.69778905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73268849\n",
      "Iteration 2, loss = 17.90557847\n",
      "Iteration 3, loss = 17.90639242\n",
      "Iteration 4, loss = 1.81494034\n",
      "Iteration 5, loss = 2.41680476\n",
      "Iteration 6, loss = 17.37397777\n",
      "Iteration 7, loss = 17.91432210\n",
      "Iteration 8, loss = 18.15233942\n",
      "Iteration 9, loss = 1.17968203\n",
      "Iteration 10, loss = 0.71366287\n",
      "Iteration 11, loss = 0.71655060\n",
      "Iteration 12, loss = 0.71947575\n",
      "Iteration 13, loss = 0.72238625\n",
      "Iteration 14, loss = 0.72523860\n",
      "Iteration 15, loss = 0.72799087\n",
      "Iteration 16, loss = 0.73062713\n",
      "Iteration 17, loss = 0.73312887\n",
      "Iteration 18, loss = 0.73549424\n",
      "Iteration 19, loss = 0.73772462\n",
      "Iteration 20, loss = 0.73982731\n",
      "Iteration 21, loss = 0.74181403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72984577\n",
      "Iteration 2, loss = 18.14115830\n",
      "Iteration 3, loss = 18.14189820\n",
      "Iteration 4, loss = 2.69895270\n",
      "Iteration 5, loss = 0.69952491\n",
      "Iteration 6, loss = 0.70133089\n",
      "Iteration 7, loss = 17.91428172\n",
      "Iteration 8, loss = 1.30129127\n",
      "Iteration 9, loss = 0.72087538\n",
      "Iteration 10, loss = 0.72720982\n",
      "Iteration 11, loss = 0.73142641\n",
      "Iteration 12, loss = 0.73287791\n",
      "Iteration 13, loss = 0.73263534\n",
      "Iteration 14, loss = 0.73104390\n",
      "Iteration 15, loss = 0.72999519\n",
      "Iteration 16, loss = 0.73101934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72996375\n",
      "Iteration 2, loss = 18.02335755\n",
      "Iteration 3, loss = 18.02409758\n",
      "Iteration 4, loss = 2.66363940\n",
      "Iteration 5, loss = 2.35446846\n",
      "Iteration 6, loss = 17.19317913\n",
      "Iteration 7, loss = 15.94287946\n",
      "Iteration 8, loss = 18.03428918\n",
      "Iteration 9, loss = 4.08952318\n",
      "Iteration 10, loss = 0.71373459\n",
      "Iteration 11, loss = 0.71665750\n",
      "Iteration 12, loss = 0.71959007\n",
      "Iteration 13, loss = 0.72248522\n",
      "Iteration 14, loss = 0.72530027\n",
      "Iteration 15, loss = 0.72800346\n",
      "Iteration 16, loss = 0.73057071\n",
      "Iteration 17, loss = 0.73298840\n",
      "Iteration 18, loss = 0.73524828\n",
      "Iteration 19, loss = 0.73734870\n",
      "Iteration 20, loss = 0.73929481\n",
      "Iteration 21, loss = 0.74109842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70079682\n",
      "Iteration 2, loss = 3.63030406\n",
      "Iteration 3, loss = 18.13971641\n",
      "Iteration 4, loss = 17.90418334\n",
      "Iteration 5, loss = 0.77639890\n",
      "Iteration 6, loss = 0.76408400\n",
      "Iteration 7, loss = 0.72524360\n",
      "Iteration 8, loss = 0.69394256\n",
      "Iteration 9, loss = 0.69683144\n",
      "Iteration 10, loss = 0.69721684\n",
      "Iteration 11, loss = 0.69733923\n",
      "Iteration 12, loss = 0.69724966\n",
      "Iteration 13, loss = 0.69700245\n",
      "Iteration 14, loss = 0.69665964\n",
      "Iteration 15, loss = 0.69629019\n",
      "Iteration 16, loss = 0.69595691\n",
      "Iteration 17, loss = 0.69570437\n",
      "Iteration 18, loss = 0.69555496\n",
      "Iteration 19, loss = 0.69551131\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70389361\n",
      "Iteration 2, loss = 3.41715632\n",
      "Iteration 3, loss = 11.35161724\n",
      "Iteration 4, loss = 6.19968653\n",
      "Iteration 5, loss = 17.36171015\n",
      "Iteration 6, loss = 0.86466739\n",
      "Iteration 7, loss = 1.52307144\n",
      "Iteration 8, loss = 1.80282804\n",
      "Iteration 9, loss = 0.69438491\n",
      "Iteration 10, loss = 0.69424401\n",
      "Iteration 11, loss = 0.69405384\n",
      "Iteration 12, loss = 0.69407226\n",
      "Iteration 13, loss = 0.69407908\n",
      "Iteration 14, loss = 0.69408212\n",
      "Iteration 15, loss = 0.69408805\n",
      "Iteration 16, loss = 0.69410154\n",
      "Iteration 17, loss = 0.69412498\n",
      "Iteration 18, loss = 0.69415860\n",
      "Iteration 19, loss = 0.69420079\n",
      "Iteration 20, loss = 0.69424878\n",
      "Iteration 21, loss = 0.69429922\n",
      "Iteration 22, loss = 0.69434884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70167987\n",
      "Iteration 2, loss = 3.52018710\n",
      "Iteration 3, loss = 18.02192587\n",
      "Iteration 4, loss = 18.02196900\n",
      "Iteration 5, loss = 0.78468238\n",
      "Iteration 6, loss = 0.77140400\n",
      "Iteration 7, loss = 0.72338232\n",
      "Iteration 8, loss = 0.69493215\n",
      "Iteration 9, loss = 0.69743781\n",
      "Iteration 10, loss = 0.69778270\n",
      "Iteration 11, loss = 0.69782564\n",
      "Iteration 12, loss = 0.69763675\n",
      "Iteration 13, loss = 0.69727967\n",
      "Iteration 14, loss = 0.69683072\n",
      "Iteration 15, loss = 0.69637229\n",
      "Iteration 16, loss = 0.69597216\n",
      "Iteration 17, loss = 0.69567566\n",
      "Iteration 18, loss = 0.69550604\n",
      "Iteration 19, loss = 0.69546106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70079682\n",
      "Iteration 2, loss = 17.90565871\n",
      "Iteration 3, loss = 5.22087456\n",
      "Iteration 4, loss = 15.05931645\n",
      "Iteration 5, loss = 5.43197437\n",
      "Iteration 6, loss = 7.72122175\n",
      "Iteration 7, loss = 18.14991159\n",
      "Iteration 8, loss = 1.21893768\n",
      "Iteration 9, loss = 0.71456637\n",
      "Iteration 10, loss = 0.71625022\n",
      "Iteration 11, loss = 0.71491671\n",
      "Iteration 12, loss = 0.72022633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70389361\n",
      "Iteration 2, loss = 18.14121110\n",
      "Iteration 3, loss = 17.90623134\n",
      "Iteration 4, loss = 0.72871482\n",
      "Iteration 5, loss = 0.70697215\n",
      "Iteration 6, loss = 6.83154635\n",
      "Iteration 7, loss = 18.15021581\n",
      "Iteration 8, loss = 0.85186616\n",
      "Iteration 9, loss = 0.88217434\n",
      "Iteration 10, loss = 1.69094836\n",
      "Iteration 11, loss = 0.88247477\n",
      "Iteration 12, loss = 0.72417107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70167987\n",
      "Iteration 2, loss = 18.02343800\n",
      "Iteration 3, loss = 18.02416029\n",
      "Iteration 4, loss = 18.02576438\n",
      "Iteration 5, loss = 18.02772515\n",
      "Iteration 6, loss = 18.02991145\n",
      "Iteration 7, loss = 16.66286492\n",
      "Iteration 8, loss = 8.90800786\n",
      "Iteration 9, loss = 11.61672651\n",
      "Iteration 10, loss = 2.94460207\n",
      "Iteration 11, loss = 0.72479161\n",
      "Iteration 12, loss = 0.72961773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70008233\n",
      "Iteration 2, loss = 1.02846938\n",
      "Iteration 3, loss = 9.90517811\n",
      "Iteration 4, loss = 10.60532289\n",
      "Iteration 5, loss = 2.28807950\n",
      "Iteration 6, loss = 17.90504952\n",
      "Iteration 7, loss = 8.89561005\n",
      "Iteration 8, loss = 17.38205065\n",
      "Iteration 9, loss = 27.56992288\n",
      "Iteration 10, loss = 38.80420858\n",
      "Iteration 11, loss = 50.58845310\n",
      "Iteration 12, loss = 62.55034356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69905928\n",
      "Iteration 2, loss = 1.01221370\n",
      "Iteration 3, loss = 10.94963387\n",
      "Iteration 4, loss = 1.74475612\n",
      "Iteration 5, loss = 18.13975125\n",
      "Iteration 6, loss = 2.90477056\n",
      "Iteration 7, loss = 0.79561727\n",
      "Iteration 8, loss = 0.82323353\n",
      "Iteration 9, loss = 0.75031650\n",
      "Iteration 10, loss = 1.06915990\n",
      "Iteration 11, loss = 2.51589856\n",
      "Iteration 12, loss = 2.61268694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69831745\n",
      "Iteration 2, loss = 1.03670648\n",
      "Iteration 3, loss = 10.90973184\n",
      "Iteration 4, loss = 6.62018220\n",
      "Iteration 5, loss = 16.31416261\n",
      "Iteration 6, loss = 1.28464155\n",
      "Iteration 7, loss = 0.69355383\n",
      "Iteration 8, loss = 0.69503700\n",
      "Iteration 9, loss = 0.69716866\n",
      "Iteration 10, loss = 0.69747523\n",
      "Iteration 11, loss = 0.70014904\n",
      "Iteration 12, loss = 0.70130862\n",
      "Iteration 13, loss = 0.69905756\n",
      "Iteration 14, loss = 0.69602308\n",
      "Iteration 15, loss = 0.69477636\n",
      "Iteration 16, loss = 0.69563731\n",
      "Iteration 17, loss = 0.69619642\n",
      "Iteration 18, loss = 0.69564258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70008233\n",
      "Iteration 2, loss = 18.14137032\n",
      "Iteration 3, loss = 17.90731008\n",
      "Iteration 4, loss = 18.14550302\n",
      "Iteration 5, loss = 18.14798149\n",
      "Iteration 6, loss = 17.91512290\n",
      "Iteration 7, loss = 17.91813942\n",
      "Iteration 8, loss = 18.15722355\n",
      "Iteration 9, loss = 18.16075065\n",
      "Iteration 10, loss = 17.92869821\n",
      "Iteration 11, loss = 17.93199905\n",
      "Iteration 12, loss = 17.93507141\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69905928\n",
      "Iteration 2, loss = 17.90581683\n",
      "Iteration 3, loss = 18.14287245\n",
      "Iteration 4, loss = 17.90980300\n",
      "Iteration 5, loss = 11.82340174\n",
      "Iteration 6, loss = 18.15125983\n",
      "Iteration 7, loss = 18.15466541\n",
      "Iteration 8, loss = 17.92290585\n",
      "Iteration 9, loss = 17.92609837\n",
      "Iteration 10, loss = 18.16469271\n",
      "Iteration 11, loss = 18.16761515\n",
      "Iteration 12, loss = 8.05407736\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69831745\n",
      "Iteration 2, loss = 18.02359001\n",
      "Iteration 3, loss = 18.02498732\n",
      "Iteration 4, loss = 18.02689088\n",
      "Iteration 5, loss = 18.02877771\n",
      "Iteration 6, loss = 18.03066455\n",
      "Iteration 7, loss = 18.03282196\n",
      "Iteration 8, loss = 18.03480845\n",
      "Iteration 9, loss = 18.03693515\n",
      "Iteration 10, loss = 11.91789900\n",
      "Iteration 11, loss = 18.04121540\n",
      "Iteration 12, loss = 9.57406728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "36 fits failed out of a total of 810.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "36 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 753, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 496, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Reintjes\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.70027341 0.5        0.5        0.70893142 0.5        0.5\n",
      " 0.7263044  0.69577352 0.5        0.70027341 0.5        0.5\n",
      " 0.70893142 0.495671   0.5        0.7263044  0.43045113 0.5\n",
      " 0.70027341 0.504329   0.5        0.70893142 0.495671   0.5\n",
      " 0.7263044  0.504329   0.5        0.71758943 0.69144452 0.5\n",
      " 0.69144452 0.5        0.50877193 0.71747551 0.5        0.504329\n",
      " 0.71758943 0.49988608 0.5        0.69144452 0.5        0.5\n",
      " 0.71747551 0.51304397 0.504329   0.71758943        nan 0.5\n",
      " 0.69144452        nan 0.504329   0.71747551        nan 0.5\n",
      " 0.70898838 0.5        0.504329   0.71764639 0.504329   0.504329\n",
      " 0.71331738 0.6916154  0.5        0.70898838 0.5        0.495671\n",
      " 0.71764639 0.495671   0.504329   0.71331738 0.5        0.5\n",
      " 0.70898838        nan 0.504329   0.71764639        nan 0.5\n",
      " 0.71331738        nan 0.5        0.70015949 0.5        0.5\n",
      " 0.70898838 0.5        0.495671   0.66547049 0.5        0.5\n",
      " 0.70015949 0.54397357 0.5        0.70898838 0.495671   0.495671\n",
      " 0.66547049 0.5        0.5        0.70015949        nan 0.5\n",
      " 0.70898838        nan 0.495671   0.66547049        nan 0.5\n",
      " 0.70471634 0.5        0.5        0.70015949 0.5        0.5\n",
      " 0.70454545 0.5        0.495671   0.70471634 0.495671   0.5\n",
      " 0.70015949 0.4825131  0.5        0.70454545 0.5        0.5\n",
      " 0.70471634        nan 0.504329   0.70015949        nan 0.5\n",
      " 0.70454545        nan 0.5        0.71753247 0.67834359 0.5\n",
      " 0.7131465  0.72624744 0.5        0.69998861 0.69138756 0.5\n",
      " 0.71753247 0.5        0.504329   0.7131465  0.5        0.5\n",
      " 0.69998861 0.5        0.5        0.71753247 0.5        0.5\n",
      " 0.7131465  0.5        0.5        0.69998861 0.5        0.504329\n",
      " 0.70033037 0.51275917 0.5        0.495671   0.5        0.5\n",
      " 0.73923445 0.56966279 0.5        0.70033037 0.5        0.5\n",
      " 0.495671   0.495671   0.5        0.73923445 0.53053087 0.5\n",
      " 0.70033037 0.5        0.5        0.495671   0.495671   0.5\n",
      " 0.73923445 0.5        0.5        0.70465938 0.49156983 0.5745614\n",
      " 0.70893142 0.70027341 0.5        0.72197539 0.5        0.66125541\n",
      " 0.70465938 0.58327637 0.5        0.70893142 0.53058783 0.5\n",
      " 0.72197539 0.54380269 0.5        0.70465938 0.5        0.495671\n",
      " 0.70893142 0.5        0.5        0.72197539 0.5        0.5\n",
      " 0.69588745 0.53075871 0.70904534 0.66108453 0.54385965 0.5\n",
      " 0.68295739 0.5        0.5        0.69588745 0.504329   0.5\n",
      " 0.66108453 0.5        0.5        0.68295739 0.52232855 0.5\n",
      " 0.69588745 0.5        0.5        0.66108453 0.5        0.5\n",
      " 0.68295739 0.495671   0.5        0.71326042 0.66524265 0.52164502\n",
      " 0.71331738 0.58225108 0.504329   0.71331738 0.73046252 0.5\n",
      " 0.71326042 0.5258601  0.5        0.71331738 0.56106175 0.5\n",
      " 0.71331738 0.56128959 0.5        0.71326042 0.5        0.5\n",
      " 0.71331738 0.495671   0.5        0.71331738 0.504329   0.5       ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters to optimize accuracy: {'activation': 'relu', 'hidden_layer_sizes': (8, 6), 'learning_rate_init': 0.001, 'random_state': 5, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define which hyperparameters should be used\n",
    "param_grid = {\n",
    "    'random_state': [1,3,5], #Use different seeds\n",
    "    'hidden_layer_sizes': [ (10,),(8, 6), (10, 8),(20, 10),(100, 50, 20)],\n",
    "    'activation': ['identity','relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=MLPClassifier(verbose=True,max_iter=300),\n",
    "                           scoring='accuracy', # optimize the accuracy with the selection of the hyperparameters\n",
    "                           param_grid=param_grid,\n",
    "                           cv=3)  # cv is the amount of folds for cross validation\n",
    "# Perform Grid Search to identify the best hyper parameters\n",
    "grid_search.fit(Xtrain, Ytrain)\n",
    "\n",
    "print(\"Best Hyperparameters to optimize accuracy:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d576b81d-393f-4057-903e-63353df7d3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(8, 6), max_iter=300, random_state=5,\n",
       "              solver=&#x27;lbfgs&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(8, 6), max_iter=300, random_state=5,\n",
       "              solver=&#x27;lbfgs&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(8, 6), max_iter=300, random_state=5,\n",
       "              solver='lbfgs', verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_clf = MLPClassifier(verbose=True, **best_params,max_iter=300)\n",
    "best_clf.fit(Xtrain, Ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48aec57e-4aae-438e-8261-5d26a84617a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7714285714285715"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Make prediction on test dataset\n",
    "Ypred=best_clf.predict(Xtest)\n",
    "\n",
    "# Import accuracy score partial_fit(X, y, classes=None)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calcuate accuracy\n",
    "accuracy_score(Ytest,Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fcc256d-7673-4a29-918e-55b9bcec8aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1  [0.76470588 0.77777778] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate f1 score \n",
    "f1 = f1_score(Ytest, Ypred, average=None)\n",
    "print(\"F1 \",f1,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f45afe9-bd36-4c01-9db7-97ccde9abc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "classes = [\"MEL\", \"NV\"]\n",
    "#create and plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(best_clf,Xtest,Ytest,display_labels=classes,cmap=plt.cm.OrRd,normalize=None,)\n",
    "plt.title(\"Confusion Matrix ISIC MLP\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
